{"includeInGlobalSearch": true, "url":"https://docs.mongodb.com/ecosystem/","documents":[{"slug":"contents/","headings":[],"text":"MongoDB Ecosystem Documentation Contents\nThe following is the site map for the MongoDB Ecosystem Documentation:\n\n\nMongoDB Drivers\nC Driver\nC++ Driver\nC# and .NET Driver\nJava Driver\nNode.js Driver\nPerl Driver\nPHP Driver\nPHP Libraries, Frameworks and Tools\n\n\nPython Driver\nRuby Driver\nScala Driver\nCommunity Supported Drivers\nGo Driver\nErlang Driver\nCommunity Supported Drivers Reference\n\n\nDriver Syntax Table\nDriver Compatibility\n\n\nMongoDB Integration and Tools\nMongoDB Connector for Hadoop\nGetting Started with Hadoop\nHadoop and MongoDB Use Cases\nConfigure Red Hat Enterprise Linux Identity Management with MongoDB\nOperational Procedures using Red Hat Enterprise Linux Identity Management\nHTTP Interface\nMunin Configuration Examples\nWireshark Support for MongoDB Protocol\n\n\nPlatforms\nAmazon EC2\nEC2 Backup and Restore\nDigitalOcean\ndotCloud\nJoyent Cloud\nModulus\nRackspace Cloud\nRed Hat OpenShift\nVMware Cloud Foundry\nMicrosoft Azure\nWindows Quick Links and Reference Center\n\n\nUse Cases\nStoring Log Data\nPre-Aggregated Reports (MMAPv1)\nHierarchical Aggregation\nProduct Catalog\nInventory Management\nCategory Hierarchy\nMetadata and Asset Management\nStoring Comments\n\n\n\n\n\n\n                \n    \n      MongoDB Drivers\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"MongoDB Ecosystem Documentation Contents","preview":"The following is the site map for the MongoDB Ecosystem Documentation: © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.  \n  \n    MongoDB Ecosystem","tags":""},{"slug":"drivers/c/","headings":[],"text":"MongoDB Drivers > \n          C MongoDB Driver \n      \n    \n  \n                \n                  \nC MongoDB Driver\n\nOn this page\n\nDownload and build\nMongoDB Compatibility\nLanguage Compatibility\n\n\nThe MongoDB C Driver is the officially supported driver for\nMongoDB. It is written in C89 compatible C.\nSee the following:\n\nMongoDB C Driver Documentation\nSource Code\nREADME\nJIRA\n\n\nDownload and build¶\nThe C driver is hosted at GitHub C driver.\nYou can download the latest stable releases from\nGitHub.\nConsult the building docs for detailed instructions on building the driver.\n\n\nMongoDB Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB C driver for use with a specific version of\nMongoDB.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\nC Driver Version\nMongoDB 2.4\nMongoDB 2.6\nMongoDB 3.0\nMongoDB 3.2\n\n\n\nVersion 1.3\n✓\n✓\n✓\n✓\n\nVersion 1.1.0\n✓\n✓\n✓\n \n\n\n\nThe driver does not support older versions of MongoDB.\n\n\nLanguage Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB C driver for use with a specific version of\nC.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\nC Driver Versions\nC89\nC99\nC11\n\n\n\nAll Versions\n✓\n✓\n✓\n\n\n\n\n\n\n                \n    \n      ←  \n      MongoDB Drivers\n      Legacy C++ MongoDB Driver\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"C MongoDB Driver","preview":"On this page The MongoDB C Driver is the officially supported driver for\nMongoDB. It is written in C89 compatible C. See the following: The C driver is hosted at GitHub C driver.\nYou can download the","tags":""},{"slug":"drivers/community-supported-drivers-toc/","headings":[],"text":"MongoDB Drivers > \n          Community Supported Drivers \n      \n    \n  \n                \n                  \nCommunity Supported Drivers\n\n\nGo Driver\nErlang Driver\nCommunity Supported Drivers Reference\n\n\n\n\n                \n    \n      ←  \n      MongoDB Scala Drivers\n      Community: Go Driver (mgo)\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Community Supported Drivers","preview":"© MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.  \n  \n    Community Supported Drivers","tags":""},{"slug":"drivers/community-supported-drivers/","headings":[],"text":"MongoDB Drivers > \n          \n          Community Supported Drivers > \n          Community Supported Drivers Reference \n      \n    \n  \n                \n                  \nCommunity Supported Drivers Reference\n\nActionScript3\nhttp://code.google.com/p/jmcnet-full-mongo-flex-driver/\nhttp://www.mongoas3.com\n\n\nC\nlibmongo-client\n\n\nClojure\nSee the Java Language Center\n\n\nColdFusion\ncfmongodb\nBlog post: Part 1\n| Part 2\n| Part 3\nhttp://github.com/virtix/cfmongodb/tree/0.9\nhttp://mongocfc.riaforge.org/\n\n\nD\nVibe.D native MongoDB driver\nPort of the legacy MongoDB 0.4 C Driver to D\n\n\nDart\nhttp://pub.dartlang.org/packages/mongo_dart\n\n\nDelphi\nmongo-delphi-driver - Full\nfeatured Delphi interface to MongoDB built on top of the\nlegacy MongoDB C driver\npebongo - Early stage Delphi\ndriver for MongoDB\nTMongoWire - Maps\nall the VarTypes of OleVariant to the BSON types, implements\nIPersistStream for (de)serialization, and uses TTcpClient for\nnetworking\nAlcinoe TALMongoClient - A Delphi\ncomponent suite that includes support for MongoDB\nSynopse mORMot framework -\nProvides access to MongoDB deployments for Synopse applications.\n\n\nElixir\nElixir Driver - MongoDB\ndriver in Elixir, a functional language built on top of the Erlang VM.\nEcto adapter - MongoDB adapter\nfor Ecto, a database wrapper and language integrated query for Elixir.\n\n\nEntity\nentity driver for mongodb on Google\nCode, included within the standard Entity Library\n\n\nErlang\nErlang Driver - Formerly the officially\nmaintained MongoDB Erlang driver, now maintained by community member Comtihon.\n\n\nFactor\nFactor\n\n\nFantom\nafMongo - a MongoDB driver written in pure Fantom.\nafMorphia - a Fantom to MongoDB object mapping library.\nFantoMongo\n\n\nF#\nF#\n\n\nGo\nmgo\n\n\nGroovy\ngmongo\nAlso see the Java Language Center\nBlog Post: Groovy on Grails in the land of MongoDB\nGrails Bates: Grails business audit trails plugin\n\n\nHaskell\nmongoDB, MongoDB driver for Haskell.\n\n\nJavaScript\nNarwhal\n\n\nLabVIEW\nmongo-labview-driver\n\n\nLisp\nhttps://github.com/fons/cl-mongo\n\n\nLua\nLuaMongo on Google Code\nLuaMongo fork on Github\n\n\nMathematica\nMongoDBLink\n\n\nMatLab\nmongo-matlab-driver\n\n\nNode.js\nObjective C\nNuMongoDB\nObjCMongoDB\n\n\nOCaml\nMongo.ml\n\n\nOpa\nOpa Standard Library MongoDB Driver\n\n\nPerl\nMango - A non-blocking\npure-Perl driver based on Mojolicious\n\n\nPHP Libraries, Frameworks, and Tools\nPowerShell\nmosh - MongoDB Powershell provider\nMdbc - MongoDB Cmdlets for PowerShell\n\n\nProlog\nProlongo\n\n\nPython\nMongoEngine\nMongoKit\nMongoKat\nMonary\nDjango-mongonaut\n\n\nR\nmongolite - High-level,\nhigh-performance MongoDB client based on libmongoc and jsonlite\nrmongodb - R interface to MongoDB\nbuilt on top of the legacy MongoDB C driver\nRMongo - R client to\ninterface with MongoDB\n\n\nRuby\nMongoMapper\n\n\nScala\nSee the Java Language Center\n\n\nSwift\nMongoKitten, - MongoDB driver for Swift\nPerfect-MongoDB, - A Server Side Swift\nwrapper of libmongoc C driver.\n\n\nRacket (PLT Scheme)\nmongodb.plt\n\n\nSmalltalk\nSqueaksource MongoTalk\n\n\n\n\n\n                \n    \n      ←  \n      Community: Erlang Driver\n      Driver Syntax Table\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Community Supported Drivers Reference","preview":"© MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.  \n  \n    Community Supported Drivers Reference","tags":""},{"slug":"drivers/cpp/","headings":[],"text":"MongoDB Drivers > \n          Legacy C++ MongoDB Driver \n      \n    \n  \n                \n                  \nLegacy C++ MongoDB Driver\n\nOn this page\n\nCompatibility\nAdditional Resource\n\n\nSee the following resources for the offically supported MongoDB C++\ndriver:\n\nDriver Repository\nDriver Wiki (Documentation)\nDownload and Compile\nTutorial\nAPI Documentation\n\n\nCompatibility¶\n\nMongoDB Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB C++ driver for use with a specific version of\nMongoDB.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\nC++ Driver Version\nMongoDB 2.4\nMongoDB 2.6\nMongoDB 3.0\nMongoDB 3.2\nMongoDB 3.4\n\n\n\nC++11 3.1\n✓\n✓\n✓\n✓\n✓\n\nC++11 3.0\n✓\n✓\n✓\n✓\n \n\nlegacy-1.1.0+\n✓\n✓\n✓\n✓\n \n\nlegacy-1.0.0+\n✓\n✓\n✓\n \n \n\n\n\nThe driver does not support older versions of MongoDB.\n\n\nLanguage Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB C++ driver for use with a specific version of\nC++.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\nC++ Driver Version\nC++03\nC++11\nC++14\n\n\n\nmongocxx 3.1.x\n \n✓\n✓\n\nmongocxx 3.0.x\n \n✓\n✓\n\nlegacy-1.0.0+\n✓\n✓\n✓\n\n\n\n\n\n\nAdditional Resource¶\n\nIntroducing the New C++ Driver\nGetting Started with MongoDB (C++ Edition)\n\n\n\n\n                \n    \n      ←  \n      C MongoDB Driver\n      C# and .NET MongoDB Driver\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Legacy C++ MongoDB Driver","preview":"On this page See the following resources for the offically supported MongoDB C++\ndriver: The following compatibility table specifies the recommended\nversion(s) of the MongoDB C++ driver for use with a","tags":""},{"slug":"drivers/csharp/","headings":[],"text":"MongoDB Drivers > \n          C# and .NET MongoDB Driver \n      \n    \n  \n                \n                  \nC# and .NET MongoDB Driver\n\nOn this page\n\nDriver Features\nDownload/Upgrade\nCompatibility\nPresentations\nCommunity Articles\nAdditional Resource\n\n\nThe official MongoDB C#/.NET Driver provides asynchronous interaction\nwith MongoDB. For the official MongoDB C#/.NET reference, see:\n\nMongoDB C#/.NET Documentation\nMongoDB C# API Documentation.\n\n\nDriver Features¶\n\nMongoDB Driver\nAn updated .NET driver offering a full asynchronous stack. For\ndocumentation on the update .NET driver, see MongoDB C#/.NET Driver\ndocumentation.\nBSON Library\nA standalone BSON library with a serialization infrastructure that\nyou can use to build high-performance serializers. For documentation\non the BSON library, see BSON Reference.\nCore Library\nA new core library upon which MongoDB .NET Driver is built. Users\ncan use the new core library to build alternative or experimental\nhigh-level APIs. For documentation on the core library, see Driver\nCore\n\n\n\nDownload/Upgrade¶\nThe recommended way to download the driver is to use nuget.\nFor more information, see Getting Started.\nIf upgrading from an earlier version of the C#/.NET driver, see also\nWhat’s New.\n\n\nCompatibility¶\n\nMongoDB Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB C#/.NET driver for use with a specific version of\nMongoDB.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\nC#/.NET Driver Version\nMongoDB 2.4\nMongoDB 2.6\nMongoDB 3.0\nMongoDB 3.2\nMongoDB 3.4\n\n\n\nVersion 2.3\n✓\n✓\n✓\n✓\n✓\n\nVersion 2.2\n✓\n✓\n✓\n✓\n \n\nVersion 2.0\n✓\n✓\n✓\n \n \n\nVersion 1.11\n✓\n✓\n✓\n✓\n \n\nVersion 1.10\n✓\n✓\n✓\n \n \n\n\n\nFor additional driver versions, see C#/.NET Driver MongoDB Compatibility Reference.\nThe driver does not support older versions of MongoDB.\n\n\n.NET Language Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB C#/.NET driver for use with a specific version of\n.NET.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\nDriver Version\n.NET 3.5\n.NET 4.0\n.NET 4.5\n.NET Core\n\n\n\nVersion 2.3\n \n \n✓\n✓\n\nVersion 2.2\n \n \n✓\n \n\nVersion 2.0\n \n \n✓\n \n\nVersion 1.11\n✓\n✓\n✓\n \n\nVersion 1.10\n✓\n✓\n✓\n \n\n\n\nFor additional driver versions, see .NET Driver Language Compatibility Reference.\n\n\nVisual Studio Support¶\nThe current version of the C# Driver has been built and tested using\n\nVisual Studio 2015\n\n\n\n\nPresentations¶\n\nLINQ support in C#/.NET driver (slide deck)\n(Aug 2012)\nWhat’s new in the .NET driver (slide deck) (Jul\n2012)\nC# Development (conference videos)\n\n\n\nCommunity Articles¶\n\nHow to MongoDB with C#\nA MongoDB Tutorial using C# and ASP.NET MVC\nExperimenting with MongoDB from C#\nIntroduction to MongoDB for .NET\nImplementing a Blog Using ASP.NET MVC and MongoDB\nIntro Article using a Post and Comments Example\nUsing the Official .NET driver from PowerShell\nTutorial MongoDB con ASP.NET MVC - Ejemplo Práctico\nA Quick MongoDB Repository\n\n\n\nAdditional Resource¶\n\nM101N: MongoDB for .NET Developers Free Online Course\nGetting Started with MongoDB (C#/.NET Edition)\n\n\n\n\n                \n    \n      ←  \n      Legacy C++ MongoDB Driver\n      Java MongoDB Driver\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"C# and .NET MongoDB Driver","preview":"On this page The official MongoDB C#/.NET Driver provides asynchronous interaction\nwith MongoDB. For the official MongoDB C#/.NET reference, see: The recommended way to download the driver is to use n","tags":""},{"slug":"drivers/driver-compatibility-reference/","headings":[],"text":"MongoDB Drivers > \n          Driver Compatibility \n      \n    \n  \n                \n                  \nDriver Compatibility\n\nOn this page\n\nC Driver Compatibility\nC++ Driver Compatibility\nC#/.Net Driver Compatibility\nJava Driver Compatibility\nNode.js Driver Compatibility\nPerl Driver Compatibility\nPHP Driver Compatibility\nPython Driver Compatibility\nMotor Driver Compatibility\nRuby Driver Compatibility\nScala Driver Compatibility\nCasbah Compatibility\n\n\n\nNote\nThe compatibility matrices show which versions of the driver are\nrecommended for use with a given major release. Older driver versions\nmay be able to connect to a version of MongoDB which is not\nrecommended for that driver version; however, not all\nserver features will be available.\n\n\nC Driver Compatibility¶\n\nMongoDB Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB C driver for use with a specific version of\nMongoDB.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\nC Driver Version\nMongoDB 2.4\nMongoDB 2.6\nMongoDB 3.0\nMongoDB 3.2\nMongoDB 3.4\n\n\n\nVersion 1.5\n✓\n✓\n✓\n✓\n✓\n\nVersion 1.3\n✓\n✓\n✓\n✓\n \n\nVersion 1.1.0\n✓\n✓\n✓\n \n \n\nVersion 1.0.0\n✓\n✓\n✓\n \n \n\n\n\nThe driver does not support older versions of MongoDB.\n\n\nLanguage Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB C driver for use with a specific version of\nC.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\nC Driver Versions\nC89\nC99\nC11\n\n\n\nAll Versions\n✓\n✓\n✓\n\n\n\n\n\n\nC++ Driver Compatibility¶\n\nMongoDB Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB C++ driver for use with a specific version of\nMongoDB.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\nC++ Driver Version\nMongoDB 2.4\nMongoDB 2.6\nMongoDB 3.0\nMongoDB 3.2\nMongoDB 3.4\n\n\n\nmongocxx 3.1.x\n✓\n✓\n✓\n✓\n✓\n\nmongocxx-3.0.x\n✓\n✓\n✓\n✓\n \n\nlegacy-1.1.x\n✓\n✓\n✓\n✓\n \n\nlegacy-1.0.x\n✓\n✓\n✓\n \n \n\n\n\nThe driver does not support older versions of MongoDB.\n\n\nLanguage Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB C++ driver for use with a specific version of\nC++.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\nC++ Driver Version\nC++03\nC++11\nC++14\n\n\n\nmongocxx-3.x\n \n✓\n✓\n\nlegacy-1.x\n✓\n✓\n✓\n\n\n\n\n\n\nC#/.Net Driver Compatibility¶\n\nMongoDB Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB C#/.NET driver for use with a specific version of\nMongoDB.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\nC#/.NET Driver Version\nMongoDB 2.4\nMongoDB 2.6\nMongoDB 3.0\nMongoDB 3.2\nMongoDB 3.4\n\n\n\nVersion 2.3\n✓\n✓\n✓\n✓\n✓\n\nVersion 2.2\n✓\n✓\n✓\n✓\n \n\nVersion 2.0\n✓\n✓\n✓\n \n \n\nVersion 1.11\n✓\n✓\n✓\n✓\n \n\nVersion 1.10\n✓\n✓\n✓\n \n \n\n\n\nThe driver does not support older versions of MongoDB.\n\n\nLanguage Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB C#/.NET driver for use with a specific version of\n.NET.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\nDriver Version\n.NET 3.5\n.NET 4.0\n.NET 4.5\n.NET Core\n\n\n\nVersion 2.3\n \n \n✓\n✓\n\nVersion 2.2\n \n \n✓\n \n\nVersion 2.0\n \n \n✓\n \n\nVersion 1.11\n✓\n✓\n✓\n \n\nVersion 1.10\n✓\n✓\n✓\n \n\n\n\n\n\n\nJava Driver Compatibility¶\n\nMongoDB Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB Java driver for use with a specific version of\nMongoDB.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\nJava Driver Version\nMongoDB 2.4\nMongoDB 2.6\nMongoDB 3.0\nMongoDB 3.2\nMongoDB 3.4\n\n\n\nVersion 3.4\n✓\n✓\n✓\n✓\n✓\n\nVersion 3.3\n✓\n✓\n✓\n✓\n \n\nVersion 3.2\n✓\n✓\n✓\n✓\n \n\nVersion 3.1\n✓\n✓\n✓\n \n \n\nVersion 3.0\n✓\n✓\n✓\n \n \n\nVersion 2.14\n✓\n✓\n✓\n✓  [*]\n \n\nVersion 2.13\n✓\n✓\n✓\n \n \n\nVersion 2.12\n✓\n✓\n \n \n \n\nVersion 2.11\n✓\n \n \n \n \n\n\n\nThe driver does not support older versions of MongoDB.\n\n\n\n[*]The 2.14 driver does not support all MongoDB 3.2 features (e.g.,\nread concern); however, if you are currently on a version 2.x driver\nand would like to run against MongoDB 3.2 but cannot upgrade to driver\nversion 3.2, use the 2.14 driver.\n\n\n\n\nLanguage Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB Java driver for use with a specific version of\nJava.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\nJava Driver Version\nJava 5\nJava 6\nJava 7\nJava 8\n\n\n\nVersion 3.4\n \n✓\n✓\n✓\n\nVersion 3.3\n \n✓\n✓\n✓\n\nVersion 3.2\n \n✓\n✓\n✓\n\nVersion 3.1\n \n✓\n✓\n✓\n\nVersion 3.0\n \n✓\n✓\n✓\n\nVersions >= 2.7\n✓\n✓\n✓\n✓\n\n\n\n\n\n\nNode.js Driver Compatibility¶\n\nMongoDB Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB Node.js driver for use with a specific version of\nMongoDB.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\nNode.js Driver\nMongoDB 2.4\nMongoDB 2.6\nMongoDB 3.0\nMongoDB 3.2\nMongoDB 3.4\n\n\n\n>=2.2.12\n✓\n✓\n✓\n✓\n✓\n\n>=2.0.14\n✓\n✓\n✓\n \n \n\n>=1.4.29\n✓\n✓\n✓\n \n \n\n1.4.X\n✓\n✓\n \n \n \n\n1.3.X\n✓\n \n \n \n \n\n1.2.X\n✓\n \n \n \n \n\n\n\nThe driver does not support older versions of MongoDB.\n\n\nLanguage Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB Node.js driver for use with a specific version of\nNode.js.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\nNode.js Driver\nNode.js v0.8.X\nNode.js v0.10.X\nNode.js v0.12.X\nNode.js v4.X.X\nNode.js v6.X.X\n\n\n\n2.2.X\n✓\n✓\n✓\n✓\n✓\n\n2.1.X\n✓\n✓\n✓\n✓\n✓\n\n2.0.X\n✓\n✓\n✓\n✓\n✓\n\n>=1.4.18\n✓\n✓\n✓\n \n \n\n1.4.X\n✓\n✓\n \n \n \n\n1.3.X\n✓\n✓\n \n \n \n\n1.2.X\n✓\n✓\n \n \n \n\n\n\n\n\n\nPerl Driver Compatibility¶\n\nMongoDB Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB Perl driver for use with a specific version of\nMongoDB.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\nPerl Driver\nMongoDB 2.4\nMongoDB 2.6\nMongoDB 3.0\nMongoDB 3.2\nMongoDB 3.4\n\n\n\n1.8.x\n✓\n✓\n✓\n✓\n✓\n\n1.6.x\n✓\n✓\n✓\n✓\n✓\n\n1.4.x\n✓\n✓\n✓\n✓\n \n\n1.2.x\n✓\n✓\n✓\n✓\n \n\n1.0.x\n✓\n✓\n✓\n \n \n\n\n\nThe driver does not support older versions of MongoDB.\n\n\nLanguage Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB Perl driver for use with a specific version of\nPerl.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerl Driver Version\n5.10\n5.12\n5.14\n5.16\n5.18\n5.20\n5.22\n5.24\n\n\n\n1.8.x\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n\n1.6.x\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n\n1.4.x\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n\n1.2.x\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n\n1.0.x\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n\n\n\n\n\n\nPHP Driver Compatibility¶\n\nMongoDB Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB PHP driver for use with a specific version of\nMongoDB.\nThe first column lists the driver version(s).\nIn the table below, mongodb and\nmongo refer to the new and legacy\nMongoDB PHP driver, respectively.\nPHPLIB refers to the\nuserland library.\n\n\n\n\n\n\n\n\n\n\nPHP Driver\nMongoDB 2.4\nMongoDB 2.6\nMongoDB 3.0\nMongoDB 3.2\nMongoDB 3.4\n\n\n\nPHPLIB 1.1 + mongodb-1.2\n✓\n✓\n✓\n✓\n✓\n\nPHPLIB 1.0 + mongodb-1.1\n✓\n✓\n✓\n✓\n \n\nmongodb-1.1\n✓\n✓\n✓\n✓\n \n\nmongodb-1.0\n✓\n✓\n✓\n \n \n\nmongo-1.6\n✓\n✓\n✓\n \n \n\nmongo-1.5\n✓\n✓\n \n \n \n\nmongo-1.4\n✓\n✓\n \n \n \n\nmongo-1.3\n✓\n \n \n \n \n\n\n\nThe driver does not support older versions of MongoDB.\n\n\nLanguage Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB PHP driver for use with a specific version of\nPHP.\nThe first column lists the driver version(s).\nIn the table below, mongodb and\nmongo refer to the new and legacy\nMongoDB PHP driver, respectively.\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHP Driver\nPHP 5.3\nPHP 5.4\nPHP 5.5\nPHP 5.6\nPHP 7.0\nPHP 7.1\nHHVM 3.12\nHHVM 3.15\n\n\n\nmongodb-1.2\n \n✓\n✓\n✓\n✓\n✓\n✓\n✓\n\nmongodb-1.1\n \n✓\n✓\n✓\n✓\n✓\n✓\n \n\nmongodb-1.0\n \n✓\n✓\n✓\n \n \n✓\n \n\nmongo-1.6\n✓\n✓\n✓\n✓\n \n \n \n \n\nmongo-1.5\n✓\n✓\n✓\n✓\n \n \n \n \n\nmongo-1.4\n✓\n✓\n✓\n \n \n \n \n \n\nmongo-1.3\n✓\n✓\n✓\n \n \n \n \n \n\n\n\n\n\n\nPython Driver Compatibility¶\n\nMongoDB Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB Python driver for use with a specific version of\nMongoDB.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\nPython Driver\nMongoDB 2.4\nMongoDB 2.6\nMongoDB 3.0\nMongoDB 3.2\nMongoDB 3.4\n\n\n\n3.4\n✓\n✓\n✓\n✓\n✓\n\n3.3\n✓\n✓\n✓\n✓\n \n\n3.2\n✓\n✓\n✓\n✓\n \n\n3.1\n✓\n✓\n✓\n \n \n\n3.0\n✓\n✓\n✓\n \n \n\n2.9\n✓\n✓\n✓\n \n \n\n2.8\n✓\n✓\n✓\n \n \n\n2.7\n✓\n✓\n \n \n \n\n2.6\n✓\n \n \n \n \n\n2.5\n✓\n \n \n \n \n\n\n\nThe driver does not support older versions of MongoDB.\n\n\nLanguage Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB Python driver for use with a specific version of\nPython.\nThe first column lists the driver version(s).\n\nPython 2 Compatibility¶\n\n\n\n\n\n\n\n\n\nPython Driver\nPython 2.4\nPython 2.5, Jython 2.5\nPython 2.6\nPython 2.7, PyPy\n\n\n\n3.4\n \n \n✓\n✓\n\n3.3\n \n \n✓\n✓\n\n3.2\n \n \n✓\n✓\n\n3.1\n \n \n✓\n✓\n\n3.0\n \n \n✓\n✓\n\n2.9\n✓\n✓\n✓\n✓\n\n2.8\n✓\n✓\n✓\n✓\n\n2.7\n✓\n✓\n✓\n✓\n\n2.6\n✓\n✓\n✓\n✓\n\n2.5\n✓\n✓\n✓\n✓\n\n2.4\n✓\n✓\n✓\n✓\n\n2.3\n✓\n✓\n✓\n✓\n\n2.2\n✓\n✓\n✓\n✓\n\n\n\n\n\nPython 3 Compatibility¶\n\n\n\n\n\n\n\n\n\n\n\nPython Driver\nPython 3.1\nPython 3.2\nPyPy3\nPython 3.3\nPython 3.4\nPython 3.5\n\n\n\n3.4\n \n \n✓\n✓\n✓\n✓\n\n3.3\n \n \n✓\n✓\n✓\n✓\n\n3.2\n \n✓\n✓\n✓\n✓\n✓\n\n3.1\n \n✓\n✓\n✓\n✓\n✓\n\n3.0\n \n✓\n✓\n✓\n✓\n \n\n2.9\n✓\n✓\n✓\n✓\n✓\n✓\n\n2.8\n✓\n✓\n✓\n✓\n✓\n \n\n2.7\n✓\n✓\n✓\n✓\n✓\n \n\n2.6\n✓\n✓\n✓\n✓\n✓\n \n\n2.5\n✓\n✓\n✓\n✓\n \n \n\n2.4\n✓\n✓\n✓\n✓\n \n \n\n2.3\n✓\n✓\n✓\n \n \n \n\n2.2\n✓\n✓\n✓\n \n \n \n\n\n\n\nNote\n\nJython 2.5 is a Python 2.5-compatible alternative interpreter.\nPyPy is a Python 2.7 and 3.2-compatible alternative interpreter.\n\n\n\n\n\n\nMotor Driver Compatibility¶\n\nMongoDB Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the Motor driver for use with a specific version of\nPython.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\nMotor Driver\nMongoDB 2.4\nMongoDB 2.6\nMongoDB 3.0\nMongoDB 3.2\nMongoDB 3.4\n\n\n\n1.0\n✓\n✓\n✓\n✓\n✓\n\n0.7\n✓\n✓\n✓\n✓\n \n\n0.6\n✓\n✓\n✓\n \n \n\n0.5\n✓\n✓\n✓\n \n \n\n0.4\n✓\n✓\n✓\n \n \n\n0.3\n✓\n✓\n \n \n \n\n0.2\n✓\n✓\n \n \n \n\n0.1\n✓\n \n \n \n \n\n\n\n\nMotor 1.0 wraps PyMongo 3.3+\nMotor 0.7 wraps PyMongo >=2.9.4 and <3.0\nMotor 0.6 wraps PyMongo 2.8\nMotor 0.5 wraps PyMongo 2.8\nMotor 0.4 wraps PyMongo 2.8\nMotor 0.3 wraps PyMongo 2.7.1\nMotor 0.2 wraps PyMongo 2.7.0\nMotor 0.1 wraps PyMongo 2.5.0\n\nThe driver does not support older versions of MongoDB.\n\n\nLanguage Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the Motor driver for use with a specific version of\nPython.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\n\nMotor Driver\nPython 2.5\nPython 2.6\nPython 2.7\nPython 3.3\nPython 3.4\nPython 3.5\n\n\n\n1.0\n \n✓\n✓\n✓\n✓\n✓\n\n0.7\n \n✓\n✓\n✓\n✓\n✓\n\n0.6\n \n✓\n✓\n✓\n✓\n✓\n\n0.5\n \n✓\n✓\n✓\n✓\n✓\n\n0.4\n \n✓\n✓\n✓\n✓\n \n\n0.3\n \n✓\n✓\n✓\n✓\n \n\n0.2\n \n✓\n✓\n✓\n \n \n\n0.1\n✓\n✓\n✓\n✓\n \n \n\n\n\n\nMotor 1.0 wraps PyMongo 3.3+\nMotor 0.7 wraps PyMongo >=2.9.4 and <3.0\nMotor 0.6 wraps PyMongo 2.8\nMotor 0.5 wraps PyMongo 2.8\nMotor 0.4 wraps PyMongo 2.8\nMotor 0.3 wraps PyMongo 2.7.1\nMotor 0.2 wraps PyMongo 2.7.0\nMotor 0.1 wraps PyMongo 2.5.0\n\n\nNote\n\nMotor version 0.5 and earlier requires Tornado, and supports the\nsame version of Python as its supported Tornado versions do.\nFor asyncio support, Motor requires Python 3.4+, or\nPython 3.3 with the asyncio package from PyPI.\nPyPy is not supported as it runs Motor code slowly.\nMotor does not support Jython or Windows.\n\n\n\n\n\nRuby Driver Compatibility¶\n\nMongoDB Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB Ruby driver for use with a specific version of\nMongoDB.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\nRuby Driver\nMongoDB 2.4\nMongoDB 2.6\nMongoDB 3.0\nMongoDB 3.2\nMongoDB 3.4\n\n\n\n2.4\n✓\n✓\n✓\n✓\n✓\n\n2.3\n✓\n✓\n✓\n✓\n \n\n2.2\n✓\n✓\n✓\n✓\n \n\n2.1\n✓\n✓\n✓\n \n \n\n2.0\n✓\n✓\n✓\n \n \n\n1.12\n✓\n✓\n✓\n \n \n\n1.11\n✓\n✓\n \n \n \n\n1.10\n✓\n✓\n \n \n \n\n1.9\n✓\n \n \n \n \n\n1.8\n✓\n \n \n \n \n\n\n\nThe driver does not support older versions of MongoDB.\n\n\nLanguage Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB Ruby driver for use with a specific version of\nRuby.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\n\n\nRuby Driver\nRuby 1.8.7\nRuby 1.9\nRuby 2.0\nRuby 2.1\nRuby 2.2\nRuby 2.3\nJRuby\n\n\n\n2.4\n \n✓\n✓\n✓\n✓\n✓\n✓\n\n2.3\n \n✓\n✓\n✓\n✓\n✓\n✓\n\n2.2\n \n✓\n✓\n✓\n✓\n✓\n✓\n\n2.1\n \n✓\n✓\n✓\n✓\n \n✓\n\n2.0\n \n✓\n✓\n✓\n \n \n✓\n\n1.9 - 1.12\n✓\n✓\n✓\n✓\n \n \n✓\n\n1.8\n✓\n✓\n✓\n \n \n \n✓\n\n1.7\n✓\n✓\n \n \n \n \n✓\n\n1.6\n✓\n✓\n \n \n \n \n✓\n\n\n\n\n\n\nScala Driver Compatibility¶\n\nMongoDB Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the Mongo Scala driver for use with a specific version of\nMongoDB.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\nScala Driver\nMongoDB 2.4\nMongoDB 2.6\nMongoDB 3.0\nMongoDB 3.2\nMongoDB 3.4\n\n\n\n2.1\n✓\n✓\n✓\n✓\n✓\n\n2.0\n✓\n✓\n✓\n✓\n✓\n\n1.2\n✓\n✓\n✓\n✓\n✓\n\n1.1\n✓\n✓\n✓\n✓\n \n\n1.0\n✓\n✓\n✓\n \n \n\n\n\nThe driver does not support older versions of MongoDB.\n\n\nLanguage Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the Mongo Scala driver for use with a specific version of\nScala.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\nScala Driver\nScala 2.11\nScala 2.12\n\n\n\n2.1\n✓\n✓\n\n2.0\n✓\n✓\n\n1.1\n✓\n✓\n\n1.1\n✓\n \n\n1.0\n✓\n \n\n\n\n\n\n\nCasbah Compatibility¶\n\nMongoDB Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the Casbah for use with a specific version of\nMongoDB.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\nCasbah\nMongoDB 2.4\nMongoDB 2.6\nMongoDB 3.0\nMongoDB 3.2\n\n\n\n3.1\n✓\n✓\n✓\n✓\n\n3.0\n✓\n✓\n✓\n \n\n2.8\n✓\n✓\n✓\n \n\n2.7\n✓\n✓\n \n \n\n2.6\n✓\n \n \n \n\n\n\nThe driver does not support older versions of MongoDB.\n\n\nLanguage Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the Casbah for use with a specific version of\nScala.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\nCasbah\nScala 2.8\nScala 2.9\nScala 2.10\nScala 2.11\n\n\n\n3.1\n \n \n✓\n✓\n\n3.0\n \n \n✓\n✓\n\n2.8\n \n✓\n✓\n✓\n\n2.7\n \n✓\n✓\n✓\n\n2.6\n \n✓\n✓\n \n\n2.5\n \n✓\n✓\n \n\n2.4\n✓\n✓\n \n \n\n\n\n\n\n\n\n                \n    \n      ←  \n      Driver Syntax Table\n      MongoDB Integration and Tools\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Driver Compatibility","preview":"On this page Note The compatibility matrices show which versions of the driver are\nrecommended for use with a given major release. Older driver versions\nmay be able to connect to a version of MongoDB","tags":""},{"slug":"drivers/erlang/","headings":[],"text":"MongoDB Drivers > \n          \n          Community Supported Drivers > \n          Community: Erlang Driver \n      \n    \n  \n                \n                  \nCommunity: Erlang Driver\n\nImportant\nErlang is no longer an officially supported MongoDB Driver.\n\n\nOn this page\n\nThird Party Frameworks and Libs\n\n\n\nDriver Download\nAPI Docs\n\n\nThird Party Frameworks and Libs¶\n\nMongrel: A record/document\nmapper that maps Erlang records to MongoDB documents\nErlmongo\nAPI Documentation for Mongrel\nemongo\n\n\n\n\n                \n    \n      ←  \n      Community: Go Driver (mgo)\n      Community Supported Drivers Reference\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Community: Erlang Driver","preview":"Important Erlang is no longer an officially supported MongoDB Driver. On this page © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.  \n  \n    Commu","tags":""},{"slug":"drivers/go/","headings":[],"text":"MongoDB Drivers > \n          \n          Community Supported Drivers > \n          Community: Go Driver (mgo) \n      \n    \n  \n                \n                  \nCommunity: Go Driver (mgo)\n\nOn this page\n\nCompatibility\nResources\n\n\nmgo is a Go driver for MongoDB.\n\nImportant\nmgo is not an officially supported MongoDB Driver at\nthis time.\n\n\nCompatibility¶\n\nMongoDB Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the Go driver for use with a specific version of\nMongoDB.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\nGo Driver Version\nMongoDB 2.4\nMongoDB 2.6\nMongoDB 3.0\n\n\n\nv2\n✓\n✓\n✓\n\n\n\nThe driver does not support older versions of MongoDB.\n\n\nLanguage Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the Go driver for use with a specific version of\nGo.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\nGo Driver Version\ngo1.1\ngo1.2\ngo1.3\ngo1.4\n\n\n\nv2\n✓\n✓\n✓\n✓\n\n\n\n\n\n\nResources¶\n\nAPI Documentation for mgo\nAPI Documentation for mgo/bson\nmgo User List\n\n\n\n\n                \n    \n      ←  \n      Community Supported Drivers\n      Community: Erlang Driver\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Community: Go Driver (mgo)","preview":"On this page mgo is a Go driver for MongoDB. Important mgo is not an officially supported MongoDB Driver at\nthis time. The following compatibility table specifies the recommended\nversion(s) of the Go","tags":""},{"slug":"drivers/java/","headings":[],"text":"MongoDB Drivers > \n          Java MongoDB Driver \n      \n    \n  \n                \n                  \nJava MongoDB Driver\n\nOn this page\n\nDriver Features\nDownload/Upgrade\nJava Driver Compatibility\nThird Party Frameworks and Libraries\nAdditional Resources\n\n\nStarting with the 3.0 release, the official MongoDB Java Driver\nprovides both synchronous and asynchronous interaction with MongoDB.\nFor the official MongoDB Java Driver reference, see:\n\nMongoDB Java Driver Documentation\nMongoDB Java Driver API Documentation\n\n\nDriver Features¶\n\nMongoDB Driver\nAn updated Java driver that includes the legacy API as well as a new\ngeneric MongoCollection interface that complies with a new\ncross-driver CRUD specification. For documentation on the Java\nDriver, including the Getting Started guide, see Java Driver\nDocumentation.\nMongoDB Async Driver\nA new asynchronous API that can leverage either Netty or Java 7’s\nAsynchronousSocketChannel for fast and non-blocking IO. For\ndocumentation on the Async Java Driver,including the Getting Started\nguide, see Async Java Driver Documentation.\nBSON Library\nA standalone BSON library with a new Codec infrastructure that you\ncan use to build high-performance encoders and decoders without\nrequiring an intermediate Map instance. For documentation on the\nBSON Library, see BSON Library.\nCore Driver\nA new core library upon which both the MongoDB Driver and Async\nDriver are both built. Users can use the new core library to build\nalternative or experimental high-level APIs.\n\n\n\nDownload/Upgrade¶\nThe recommended way to incorporate the driver into your project is with\na dependency management system. For more information, see\nMongoDB Java Driver.\nIf upgrading from an earlier version of the Java driver, see also\nWhat’s New.\n\n\nJava Driver Compatibility¶\n\nMongoDB Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB Java driver for use with a specific version of\nMongoDB.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\nJava Driver Version\nMongoDB 2.4\nMongoDB 2.6\nMongoDB 3.0\nMongoDB 3.2\nMongoDB 3.4\n\n\n\nVersion 3.4\n✓\n✓\n✓\n✓\n✓\n\nVersion 3.3\n✓\n✓\n✓\n✓\n \n\nVersion 3.2\n✓\n✓\n✓\n✓\n \n\nVersion 2.14\n✓\n✓\n✓\n✓ [*]\n \n\n\n\nFor additional driver versions, see Java Driver MongoDB Compatibility Reference.\nThe driver does not support older versions of MongoDB.\n\n\n\n[*]The 2.14 driver does not support all MongoDB 3.2 features (e.g.,\nread concern); however, if you are currently on a version 2.x driver\nand would like to run against MongoDB 3.2 but cannot upgrade to driver\nversion 3.2, use the 2.14 driver.\n\n\n\n\nLanguage Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB Java driver for use with a specific version of\nJava.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\nJava Driver Version\nJava 5\nJava 6\nJava 7\nJava 8\n\n\n\nVersion 3.x\n \n✓\n✓\n✓\n\nVersions 2.x\n✓\n✓\n✓\n✓\n\n\n\nFor additional driver versions, see Java Driver Language Compatibility Reference.\n\n\n\nThird Party Frameworks and Libraries¶\n\nPOJO Mappers¶\n\nMorphia. Type-Safe Wrapper with DAO/Datastore abstractions.\nSpring MongoDB. Provides Spring users with a familiar data access\nfeatures including rich POJO mapping.\nMorphium. Feature-rich POJO Mapper including features like\ndeclarative caching, cluster awareness, validation, partial updates\nsupports aggregation framework.\nMungbean (w/clojure support).\nDataNucleus JPA/JDO. JPA/JDO wrapper\nlib-mongomapper. JavaBean Mapper (No annotations).\nMongoJack. Uses jackson (annotations) to map to/from POJOs and has\na simple wrapper around DBCollection to simply this.\nKundera. JPA compliant ORM. Works with multiple datastores.\nMongoFS. Enhanced file\nstorage library with support for file compression, encryption, and\nZip file expansion. Can be used on top of a GridFS-compatible bucket.\nJongo. Query in Java as in mongo shell (using strings),\nunmarshall results into Java objects (using Jackson)\nMongoLink. Object Document Mapper (ODM.) Uses a plain java DSL for\nmapping declaration.\nHibernate OGM. Provides Java\nPersistence support for MongoDB.\nMorphix. Lightweight,\neasy-to-use POJO mapper, with object caching and lifecycle methods.\n\n\n\nCode Generation¶\n\nSculptor. MongoDB-based DSL -> Java (code generator)\nGuicyData. DSL -> Java generator with Guice integration.\n\n\n\nMisc¶\n\nmongo-queue-java.\nJava message queue using MongoDB as a backend.\nmongo-java-logging. A Java logging handler.\nlog4mongo. A log4j appender\nAllanbank Asynchronous Java Driver\nJDBC Driver for MongoDB\n(Experimental, Type4) JDBC driver\nMetamodel data exploration and querying library\nMongodb Java REST server based on Jetty\n\n\n\nClojure¶\n\nmonger\nCongo Mongo\n\n\n\nGroovy¶\n\nGMongo, a Groovy wrapper to the MongoDB Java driver\nGMongo 0.5 Release Writeup\n\n\n\nJavaScript (Rhino)¶\n\nHorn of Mongo. A MongoDB\nshell built on the Rhino JavaScript Engine for Java.\nMongoDB-Rhino. A toolset\nto provide full integration between the Rhino JavaScript engine for\nthe JVM and MongoDB. Uses the MongoDB Java driver.\n\n\n\nHadoop¶\nMongoDB Connector for Hadoop\n\n\n\nAdditional Resources¶\n\nM101J: MongoDB for Java Developers Free Online Course\nPresentation: MongoDB + Java - Everything You Need to Know\nGetting Started  with MongoDB (Java Edition)\n\n\n\n\n                \n    \n      ←  \n      C# and .NET MongoDB Driver\n      Node.js MongoDB Driver\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Java MongoDB Driver","preview":"On this page Starting with the 3.0 release, the official MongoDB Java Driver\nprovides both synchronous and asynchronous interaction with MongoDB.\nFor the official MongoDB Java Driver reference, see: T","tags":""},{"slug":"drivers/node-js/","headings":[],"text":"MongoDB Drivers > \n          Node.js MongoDB Driver \n      \n    \n  \n                \n                  \nNode.js MongoDB Driver\n\nOn this page\n\nNode.js Driver\nObject Mappers\nOther Notable Projects\nTutorials\n3rd Party Drivers\nAdditional Resources\n\n\nYou use Node.js to write event-driven, scalable\nnetwork programs in server-side JavaScript. Node.js runs on Google’s V8.\nThis section describes the available tools and suggested practices for\nusing Node.js with MongoDB.\nFor the official MongoDB Node.js Driver reference, see:\n\nNode.js Driver Manual.\n\nThe following are the README and source code:\n\nNode.js README\nNode.js source code\n\n\nNode.js Driver¶\nThe MongoDB Node.js driver is the officially supported node.js driver for MongoDB. In\nSpring 2012, MongoDB officially adopted the popular\nNode MongoDB Native Project\nand sponsored the maintainer, Christian Kvalheim, to continue its\ndevelopment. It is written in pure JavaScript and provides a native\nasynchronous Node.js interface to MongoDB. The driver is optimized for\nsimplicity. It can be used on its own, but it also serves as the basis\nof several object mapping libraries, such as Mongoose.\n\nInstall or Upgrade¶\nThe easiest way to install is to use npm:\nnpm install mongodb\n\n\n\n\nMongoDB Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB Node.js driver for use with a specific version of\nMongoDB.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\nNode.js Driver\nMongoDB 2.4\nMongoDB 2.6\nMongoDB 3.0\nMongoDB 3.2\nMongoDB 3.4\n\n\n\n>=2.2.12\n✓\n✓\n✓\n✓\n✓\n\n>=2.0.14\n✓\n✓\n✓\n \n \n\n\n\nFor additional driver versions, see Node.js Driver MongoDB Compatibility Reference.\nThe driver does not support older versions of MongoDB.\n\n\nLanguage Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB Node.js driver for use with a specific version of\nNode.js.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\nNode.js Driver\nNode.js v0.8.X\nNode.js v0.10.X\nNode.js v0.12.X\nNode.js v4.X.X\nNode.js v6.X.X\n\n\n\n2.2.X\n✓\n✓\n✓\n✓\n✓\n\n2.1.X\n✓\n✓\n✓\n✓\n✓\n\n2.0.X\n✓\n✓\n✓\n✓\n✓\n\n\n\nFor additional driver versions, see Node.js Driver Language Compatibility Reference.\n\n\n\nObject Mappers¶\nBecause MongoDB is so easy to use, the basic Node.js driver can be the\nbest solution for many applications. However, if you need validations,\nassociations, and other high-level data modeling functions, then an\nObject Document Mapper may be helpful.\n\nMongoose¶\nMongoose is an ODM for\nNode.js. It has a thriving open source community and includes advanced\nschema-based features such as async validation, casting, object\nlife-cycle management, pseudo-joins, and rich query builder support.\nInstall it easily with npm:\nnpm install mongoose\n\n\nThe following are related resources:\n\nMongoose MongoDB ODM\nMongoose plugins\nQuickstart Tutorial\nSource Code\nGoogle Group\nBug reports\nIRC: #mongoosejs on freenode\n\n\n\n\nOther Notable Projects¶\n\nMongoskin: Layer for node-mongodb-native.\ncrudlet-mongodb: Streamable interface for mongodb.\nMongolia: Lightweight MongoDB ORM/driver wrapper.\nMongojs: Somewhat mimics the MongoDB shell api.\nMongoSmash: Uses Harmony’s Object.observe().\nCamo: A class-based ES6 ODM for MongoDB.\n\nEach of these projects build on top of the native Node.js driver, and so\nsome knowledge of that is useful, especially if you work with a custom\nMongoDB configuration.\n\n\nTutorials¶\n\nMongoskin Tutorial\nMongoose Tutorial\nCamo Tutorial\n\n\n\n3rd Party Drivers¶\nA few 3rd party drivers exist. While not officially supported, these\ndrivers take a different approach that may be valuable given your\nneeds.\n\nnode-mongodb: An\nasynchronous Node interface to MongoDB, written in C.\nMongolian DeadBeef:\nA Node.js driver that attempts to approximate the MongoDB shell.\n\n\n\nAdditional Resources¶\n\nBuilding Your First Application With MongoDB: Creating a REST API Using the MEAN Stack (Part 1)\nPresentation: MongoDB and the MEAN Stack\nMongoDB and the MEAN Stack: Optimizing an Example Application\nMongoDB University 101JS Node.JS Course\nGetting Started with MongoDB (Node.JS Edition)\n\n\n\n\n                \n    \n      ←  \n      Java MongoDB Driver\n      MongoDB Perl Driver\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Node.js MongoDB Driver","preview":"On this page You use Node.js to write event-driven, scalable\nnetwork programs in server-side JavaScript. Node.js runs on Google’s V8.\nThis section describes the available tools and suggested practices","tags":""},{"slug":"drivers/perl/","headings":[],"text":"MongoDB Drivers > \n          MongoDB Perl Driver \n      \n    \n  \n                \n                  \nMongoDB Perl Driver\n\nOn this page\n\nMongoDB Perl Driver Compatibility\nThird-party Perl Drivers\nObject-Document Mappers\nOther Perl Tools\n\n\nThe official Perl driver is available through CPAN as the package\nMongoDB.\n\nInstallation\nTutorial\n\n\nMongoDB Perl Driver Compatibility¶\n\nMongoDB Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB Perl driver for use with a specific version of\nMongoDB.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\nPerl Driver\nMongoDB 2.4\nMongoDB 2.6\nMongoDB 3.0\nMongoDB 3.2\nMongoDB 3.4\n\n\n\n1.8.x\n✓\n✓\n✓\n✓\n✓\n\n1.6.x\n✓\n✓\n✓\n✓\n✓\n\n1.4.x\n✓\n✓\n✓\n✓\n \n\n1.2.x\n✓\n✓\n✓\n✓\n \n\n1.0.x\n✓\n✓\n✓\n \n \n\n\n\nThe driver does not support older versions of MongoDB.\n\n\nLanguage Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB Perl driver for use with a specific version of\nPerl.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerl Driver Version\n5.10\n5.12\n5.14\n5.16\n5.18\n5.20\n5.22\n5.24\n\n\n\n1.8.x\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n\n1.6.x\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n\n1.4.x\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n\n1.2.x\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n\n1.0.x\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n\n\n\nFor additional driver versions, see Perl Driver Language Compatibility Reference.\n\n\n\nThird-party Perl Drivers¶\n\nMango¶\nMango is a pure-Perl, non-blocking,\nasynchronous driver for MongoDB, designed to work with the Mojolicious web framework.\nCreated by Sebastian Riedel.\n\n\n\nObject-Document Mappers¶\n\nMeerkat¶\nMeerkat lets you manage MongoDB documents\nas Moose objects. Your objects represent projections of the document state maintained in the database.\nMeerkat is designed for atomic operations that keep client-side objects in sync with\nthe database. Created by David Golden.\n\n\nMongoDB::Simple¶\nMongoDB::Simple is\na basic object-to-document mapping system with few dependencies. Created\nby Ian Kent.\n\n\nMongoose¶\nMongoose is an attempt to\nbring together the full power of Moose with MongoDB. It provides a full suite\nof object-to-document mapping facilities. Created by Rodrigo\nde Oliveira Gonzalez.\n\n\nMongoDBI¶\nMongoDBI is an\nObject-Document-Mapper (ODM) for MongoDB. It allows you to create\nMoose-based classes to interact with MongoDB databases.\nMongoDBI allows you to easily model classes while leveraging\nthe power of MongoDB’s schema-less and expeditious document-based design,\ndynamic queries, and atomic modifier operations. Created by Al Newkirk.\n\n\nMongoDBx::Class¶\nMongoDBx::Class is an\nORM for MongoDB databases. MongoDBx::Class takes advantage of the fact\nthat Perl’s MongoDB driver is Moose-based to extend and tweak the\ndriver’s behavior, instead of wrapping it. This means MongoDBx::Class\ndoes not define its own syntax, so you simply use it exactly as you\nwould the MongoDB driver directly. Created by Ido Perlmuter.\n\n\n\nOther Perl Tools¶\n\nBSON¶\nBSON is a pure-Perl BSON\nimplementation. Created by Stefan G.\n\n\nEntities::Backend::MongoDB¶\nEntities::Backend::MongoDB\nis a backend for the Entities user management and authorization system. It\nstores all entities and relations between them in a MongoDB database. Created by\nIdo Perlmuter.\n\n\nMojoX::Session::Store::MongoDB¶\nMojoX::Session::Store::MongoDB\nis a store for MojoX::Session\nthat stores a session in a MongoDB database. Created by Ask Bjørn Hansen.\n\n\nMongoDB::Admin¶\nMongoDB::Admin is a\ncollection of MongoDB administrative functions. Created by David Burley.\n\n\nMongoDBx::Queue¶\nMongoDBx::Queue is\na basic message queue system backed by MongoDB. Created by David Golden.\n\n\nMongoX¶\nMongoX is light wrapper for\nMongoDB, providing some nice DSL sugar. Created by Pan Fan.\n\n\nMooseX::Role::MongoDB¶\nMooseX::Role::MongoDB\nis a Moose role which provides connection management and automatic\nfork handling for the MongoDB driver. Created by David Golden.\n\n\nOOP Perl CMS¶\nOOP Perl CMS is\nbased on Khurt Williams’ Object Oriented Perl methodology and can be\nused as a basic CMS framework or as a basis for your own CMS system. It\nuses Apache & mod_perl with MongoDB backend. Created by Waitman Gobble.\n\n\n\n\n                \n    \n      ←  \n      Node.js MongoDB Driver\n      PHP MongoDB Driver\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"MongoDB Perl Driver","preview":"On this page The official Perl driver is available through CPAN as the package\nMongoDB. The following compatibility table specifies the recommended\nversion(s) of the MongoDB Perl driver for use with a","tags":""},{"slug":"drivers/php-libraries/","headings":[],"text":"MongoDB Drivers > \n          \n          PHP MongoDB Driver > \n          PHP Libraries, Frameworks and Tools \n      \n    \n  \n                \n                  \nPHP Libraries, Frameworks and Tools\n\nOn this page\n\nLibraries for the mongodb Extension\nStand-alone Libraries\nFramework Integrations\n\n\nLibraries for the mongo Extension\nStand-alone Libraries\nFramework Integrations\nMiscellaneous Projects\n\n\n\n\n\nLibraries for the mongodb Extension¶\n\nStand-alone Libraries¶\n\nMongo Queue PHP is\na PHP message queue, which uses MongoDB as a backend.\nMongo PHP Adapter is a\nuserland library designed to act as an adapter between applications relying on\nthe legacy mongo extension and the new mongodb extension. It provides\nthe API of the legacy driver on top of the new driver and library, which\nallows for compatibility with PHP 7.\nMongodm is a MongoDB ORM that includes\nsupport for references, embedded documents, and multilevel inheritance.\nYadm is a MongoDB ODM written for the\nmongodb extension. It is schema-less and supports fast object hydration\nand persistence, which makes it well-suited for modeling aggregation results.\n\n\n\nFramework Integrations¶\n\nDrupal\nMongoDB integration for Drupal.\nThis is a collection of several modules which allow sites to store different\ntypes of Drupal data in MongoDB. Support for the mongodb extension\nexists for Drupal 8.\n\n\nLaravel\nLaravel MongoDB: An\nEloquent model and Query builder with support for MongoDB, using the\noriginal Laravel API. This library extends the original Laravel classes, so\nit uses exactly the same methods.\nlaravel-mongo:  Convenience\nlibrary for working with MongoDB documents in Laravel and the Lumen\nmicro-framework. This library is built on top of the official MongoDB PHP\nlibrary and includes an abstract model, a helper class for handling bulk\nwrites, database wrappers for Laravel and Lumen, and some necessary helper\nfunctions.\n\n\nWebiny\nEntity Component is an ODM\nlayer for MongoDB. Entity classes created using this component will serve as\nthe main building blocks for modules, forms, tables, etc.\nMongo Component is\nan abstraction layer for MongoDB.\n\n\nYii2\nMongoDB Extension for Yii 2 provides\nMongoDB integration for Yii framework 2.0.\n\n\n\n\n\n\nLibraries for the mongo Extension¶\n\nStand-alone Libraries¶\n\nDoctrine MongoDB ODM is a library\nthat provides a PHP object mapping functionality for MongoDB. An underlying\nMongoDB abstraction layer exists as\na separate project. A Symfony bundle and Zend Framework\nmodule for the ODM are\nalso available. Although it is written for the legacy mongo extension, it\nis tested to work with the mongodb extension using Mongo PHP Adapter.\nMongator ODM is an easy, powerful,\nand ultrafast ODM for PHP and MongoDB. It is a fork of the\nMandango ODM.\nMongoFilesystem\nimplements a hierarchical file system using MongoDB as a storage engine. The\nlibrary uses GridFS for storing the files and a standard collection for the\nfolder information. There is an object-oriented representation of the folders\nand files in the filesystem and rich API for performing operations. The\nlibrary also implements renderers for JSON, HTML, and XML.\nMongofill is a pure PHP\nimplementation of the mongo extension, which means that it can be used\nwith HHVM. A separate mongofill-hhvm package provides a libbson extension for HHVM, which allows for\nmore performant BSON encoding and decoding.\nMongoQueue is a PHP queue that\nallows for moving tasks and jobs into an asynchronous process for completion\nin the background. The queue is managed by MongoDB.\nMongoRecord is a PHP MongoDB ORM\nlayer built on top of the mongo PECL extension.\nPHPMongo ODM is an ODM with support\nfor validation, relations, events, document versioning, and database\nmigrations. Although it is written for the legacy mongo extension, it is\ntested to work with the mongodb extension using Mongo PHP Adapter.\nYamop is yet another MongoDB ODM for\nPHP. It works like the standard MongoDB PHP extension interface but returns\nobjects instead of arrays (as ODM). A Laravel component is also available.\n\n\n\nFramework Integrations¶\n\nDrupal\nMongoDB integration for Drupal.\nThis is a collection of several modules which allow sites to store different\ntypes of Drupal data in MongoDB. Support for the mongo extension exists\nfor Drupal 6, 7, and 8.\n\n\nKohana\nMangoDB: Mango is an ORM and\nActiveRecord-like library that takes full advantage of MongoDB’s features.\nMongoDB PHP ODM is a\nsimple but powerful set of wrappers for using MongoDB in PHP. It is designed\nfor use with Kohana 3 but should integrate easily with any PHP\napplication.\n\n\nPhalcon\nObject Document Mapper\noffers a CRUD functionality, events, validations, and other services for\nMongoDB.\n\n\nYii 1.x\nMongoYii is ActiveRecord ORM for\nYii framework 1.x that supports MongoDB.\nYii MongoDB Driver is a\nMongoDB extension for Yii framework 1.x.\n\n\n\n\n\nMiscellaneous Projects¶\n\nPeclMongoPhpDoc provides\nskeleton classes for the mongo extension, which may be used to support\nautocomplete and inline documentation for IDEs.\n\n\n\n\n\n                \n    \n      ←  \n      PHP MongoDB Driver\n      Python MongoDB Drivers\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"PHP Libraries, Frameworks and Tools","preview":"On this page © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.  \n  \n    PHP Libraries, Frameworks and Tools","tags":""},{"slug":"drivers/php/","headings":[],"text":"MongoDB Drivers > \n          PHP MongoDB Driver \n      \n    \n  \n                \n                  \nPHP MongoDB Driver\n\nOn this page\n\nDrivers\nCompatibility\nSee Also\n\n\nFor the official MongoDB PHP Driver reference, see:\n\nMongoDB PHP Library Documentation\nMongoDB PHP and HHVM Extension Documentation\n\n\nDrivers¶\nThe currently maintained driver is the mongodb extension available from PECL. This\ndriver can be used stand-alone, although it is very bare-bones. You should\nconsider using the driver with the complimentary PHP library, which implements a more\nfull-featured API on top of the bare-bones driver. Further information on this\narchitecture may be found in the PHP.net documentation.\nThe mongo extension available from PECL\nis an older, legacy driver for PHP 5.x. The mongo extension is no longer\nmaintained and new projects are advised to use the mongodb extension and\nPHP library. A community-developed Mongo PHP Adapter project implements the legacy\nmongo extension’s API using the new mongodb extension and PHP library,\nwhich may be useful for those wishing to migrate existing applications.\n\n\nCompatibility¶\n\nMongoDB Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB PHP driver for use with a specific version of\nMongoDB.\nThe first column lists the driver version(s).\nIn the table below, mongodb and\nmongo refer to the new and legacy\nMongoDB PHP driver, respectively.\nPHPLIB refers to the\nuserland library.\n\n\n\n\n\n\n\n\n\n\nPHP Driver\nMongoDB 2.4\nMongoDB 2.6\nMongoDB 3.0\nMongoDB 3.2\nMongoDB 3.4\n\n\n\nPHPLIB 1.1 + mongodb-1.2\n✓\n✓\n✓\n✓\n✓\n\nPHPLIB 1.0 + mongodb-1.1\n✓\n✓\n✓\n✓\n \n\nmongodb-1.1\n✓\n✓\n✓\n✓\n \n\nmongodb-1.0\n✓\n✓\n✓\n \n \n\nmongo-1.6\n✓\n✓\n✓\n \n \n\nmongo-1.5\n✓\n✓\n \n \n \n\n\n\nFor additional driver versions, see PHP Driver MongoDB Compatibility Reference.\nThe driver does not support older versions of MongoDB.\n\n\nLanguage Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB PHP driver for use with a specific version of\nPHP.\nThe first column lists the driver version(s).\nIn the table below, mongodb and\nmongo refer to the new and legacy\nMongoDB PHP driver, respectively.\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHP Driver\nPHP 5.3\nPHP 5.4\nPHP 5.5\nPHP 5.6\nPHP 7.0\nPHP 7.1\nHHVM 3.12\nHHVM 3.15\n\n\n\nmongodb-1.2\n \n✓\n✓\n✓\n✓\n✓\n✓\n✓\n\nmongodb-1.1\n \n✓\n✓\n✓\n✓\n✓\n✓\n \n\nmongodb-1.0\n \n✓\n✓\n✓\n \n \n✓\n \n\nmongo-1.6\n✓\n✓\n✓\n✓\n \n \n \n \n\nmongo-1.5\n✓\n✓\n✓\n✓\n \n \n \n \n\n\n\nFor additional driver versions, see PHP Driver Language Compatibility Reference.\n\n\n\nSee Also¶\n\nPHP Libraries, Frameworks and Tools\n\n\n\nPHP Libraries, Frameworks and Tools\n\n\n\n\n\n                \n    \n      ←  \n      MongoDB Perl Driver\n      PHP Libraries, Frameworks and Tools\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"PHP MongoDB Driver","preview":"On this page For the official MongoDB PHP Driver reference, see: The currently maintained driver is the mongodb extension available from PECL. This\ndriver can be used stand-alone, although it is very","tags":""},{"slug":"drivers/python/","headings":[],"text":"MongoDB Drivers > \n          Python MongoDB Drivers \n      \n    \n  \n                \n                  \nPython MongoDB Drivers\n\nOn this page\n\nPython Driver\nAsync Driver\nPython Tools\nAlternative drivers\nTutorials\nPresentations\n\n\nThis is an overview of the available tools for using Python with\nMongoDB. Those wishing to skip to more detailed discussion should check\nout the Python Driver Tutorial.\n\nPython Driver¶\nPyMongo is the recommended way to work with MongoDB from Python.\n\nInstallation\nTutorial\nAPI Documentation\nChangelog\nSource Code\n\n\nPython Driver Compatibility¶\n\nMongoDB Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB Python driver for use with a specific version of\nMongoDB.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\nPython Driver\nMongoDB 2.4\nMongoDB 2.6\nMongoDB 3.0\nMongoDB 3.2\nMongoDB 3.4\n\n\n\n3.4\n✓\n✓\n✓\n✓\n✓\n\n3.3\n✓\n✓\n✓\n✓\n \n\n3.2\n✓\n✓\n✓\n✓\n \n\n2.9\n✓\n✓\n✓\n \n \n\n\n\nFor additional driver versions, see Python Driver MongoDB Compatibility Reference.\nThe driver does not support older versions of MongoDB.\n\n\nLanguage Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the MongoDB Python driver for use with a specific version of\nPython.\nThe first column lists the driver version(s).\n\nPython 2 Compatibility¶\n\n\n\n\n\n\n\n\n\nPython Driver\nPython 2.4\nPython 2.5, Jython 2.5\nPython 2.6\nPython 2.7, PyPy\n\n\n\n3.4\n \n \n✓\n✓\n\n3.3\n \n \n✓\n✓\n\n3.2\n \n \n✓\n✓\n\n2.9\n✓\n✓\n✓\n✓\n\n\n\n\n\nPython 3 Compatibility¶\n\n\n\n\n\n\n\n\n\n\n\nPython Driver\nPython 3.1\nPython 3.2\nPyPy3\nPython 3.3\nPython 3.4\nPython 3.5\n\n\n\n3.4\n \n \n✓\n✓\n✓\n✓\n\n3.3\n \n \n✓\n✓\n✓\n✓\n\n3.2\n \n✓\n✓\n✓\n✓\n✓\n\n2.9\n✓\n✓\n✓\n✓\n✓\n✓\n\n\n\nFor additional driver versions, see Python Driver Language Compatibility Reference.\n\nNote\n\nJython 2.5 is a Python 2.5-compatible alternative interpreter.\nPyPy is a Python 2.7 and 3.2-compatible alternative interpreter.\n\n\n\n\n\n\n\nAsync Driver¶\nMotor is the recommended asynchronous Python driver for MongoDB.\nIt is compatible with Tornado and\nasyncio.\n\nDocumentation\nTutorial\nChangelog\nSource Code\n\n\nArticles About Motor¶\n\nPorting From PyMongo To Motor\nRefactoring Tornado Coroutines\nAll Motor articles on A. Jesse Jiryu Davis’s blog\n\n\n\nMotor Compatibility¶\n\nMongoDB Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the Motor driver for use with a specific version of\nPython.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\nMotor Driver\nMongoDB 2.4\nMongoDB 2.6\nMongoDB 3.0\nMongoDB 3.2\nMongoDB 3.4\n\n\n\n1.0\n✓\n✓\n✓\n✓\n✓\n\n0.7\n✓\n✓\n✓\n✓\n \n\n0.4\n✓\n✓\n✓\n \n \n\n0.3\n✓\n✓\n \n \n \n\n\n\n\nMotor 1.0 wraps PyMongo 3.3+\nMotor 0.7 wraps PyMongo >=2.9.4 and <3.0\nMotor 0.4 wraps PyMongo 2.8\nMotor 0.3 wraps PyMongo 2.7.1\n\nFor additional driver versions, see Motor Driver MongoDB Compatibility Reference.\nThe driver does not support older versions of MongoDB.\n\n\nLanguage Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the Motor driver for use with a specific version of\nPython.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\n\n\nMotor Driver\nPython 2.6\nPython 2.7\nPython 3.1\nPython 3.2\nPython 3.3\nPython 3.4\nPython 3.5\n\n\n\n1.0\n✓\n✓\n \n \n✓\n✓\n✓\n\n0.7\n✓\n✓\n \n \n✓\n✓\n✓\n\n0.4\n✓\n✓\n \n \n✓\n✓\n \n\n0.3\n✓\n✓\n \n \n✓\n✓\n \n\n\n\n\nMotor 1.0 wraps PyMongo 3.3+\nMotor 0.7 wraps PyMongo >=2.9.4 and <3.0\nMotor 0.4 wraps PyMongo 2.8\nMotor 0.3 wraps PyMongo 2.7.1\n\nFor additional driver versions, see Motor Driver Language Compatibility Reference.\n\nNote\n\nMotor requires Tornado, and supports the same version of Python\nas its supported Tornado versions do.\nPyPy is not supported as it runs Motor code slowly.\nMotor does not support Jython or Windows.\n\n\n\n\n\n\nPython Tools¶\n\nORM Like Layers¶\nBecause MongoDB is so easy to use the basic Python driver is often the\nbest solution for many applications. However, if you need data\nvalidation, associations and other high-level data modeling\nfunctionality then ORM like layers may be desired.\n\nORM like layers\n\n\n\nFramework Tools¶\nSeveral tools and adapters for integration with various Python\nframeworks and libraries also exist.\n\nFramework Tools\n\n\n\n\nAlternative drivers¶\n\nAlternative driver list\n\n\n\nTutorials¶\n\nPython Driver Tutorial\nGetting Started with PyMODM\nGetting Started with MongoDB (Python Edition)\n\n\n\nPresentations¶\n\nMongoDB & Python: Workshop materials from PyCon 2012\nPyCon Poster: PyCon 2012\nRealtime Analytics using MongoDB, Python, Gevent, and ZeroMQ: Rick Copeland’s presentation from Mongo Seattle (December 2011)\nMongoDB with Python, Pylons, and Pyramid: Niall O’Higgins’ presentation from MongoSF (May 2011)\nPython Development with MongoDB: Bernie Hackett’s presentation from MongoSF (May 2011)\nBuilding a Social Graph with MongoDB at Eventbrite:\nBrian Zambrano’s presentation from MongoSV (December 2010)\nMore Python-related presentations\n\n\n\n\n                \n    \n      ←  \n      PHP Libraries, Frameworks and Tools\n      MongoDB Scala Drivers\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Python MongoDB Drivers","preview":"On this page This is an overview of the available tools for using Python with\nMongoDB. Those wishing to skip to more detailed discussion should check\nout the Python Driver Tutorial. PyMongo is the rec","tags":""},{"slug":"drivers/ruby/","headings":[],"text":"Ruby MongoDB Driver\n\nOn this page\n\nRuby Driver\n\n\n\nRuby Driver Moved\nFor Ruby Driver documentation, see\nhttps://docs.mongodb.com/ruby-driver/master/.\n\n\nRuby Driver¶\nThe MongoDB Ruby driver is the officially supported Ruby driver for\nMongoDB. It’s written in pure Ruby and is optimized for simplicity.\n\n\n\n                \n    \n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Ruby MongoDB Driver","preview":"On this page Ruby Driver Moved For Ruby Driver documentation, see\nhttps://docs.mongodb.com/ruby-driver/master/. The MongoDB Ruby driver is the officially supported Ruby driver for\nMongoDB. It’s writte","tags":""},{"slug":"drivers/scala/","headings":[],"text":"MongoDB Drivers > \n          MongoDB Scala Drivers \n      \n    \n  \n                \n                  \nMongoDB Scala Drivers\n\nOn this page\n\nMongo Scala Driver\nCasbah\nCommunity\n\n\nThis is an overview of the available tools for using Scala with\nMongoDB. Those wishing to skip to more detailed discussion should check\nout the Mongo Scala Driver Tutorial.\n\nMongo Scala Driver¶\nThe Mongo Scala Driver is the officially supported Scala driver for MongoDB.\nIt’s a modern idiomatic Scala driver with asynchronous and non-blocking IO.\n\nDocumentation Hub\nAPI ScalaDoc\nGetting Started Guide\nJava Driver\n\n\nCompatibility¶\n\nMongoDB Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the Mongo Scala driver for use with a specific version of\nMongoDB.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\n\nScala Driver\nMongoDB 2.4\nMongoDB 2.6\nMongoDB 3.0\nMongoDB 3.2\nMongoDB 3.4\n\n\n\n2.1\n✓\n✓\n✓\n✓\n✓\n\n2.0\n✓\n✓\n✓\n✓\n✓\n\n1.2\n✓\n✓\n✓\n✓\n✓\n\n1.1\n✓\n✓\n✓\n✓\n \n\n1.0\n✓\n✓\n✓\n \n \n\n\n\nFor additional driver versions, see Mongo Scala Driver Compatibility Reference.\nThe driver does not support older versions of MongoDB.\n\n\nLanguage Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the Mongo Scala driver for use with a specific version of\nScala.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\nScala Driver\nScala 2.11\nScala 2.12\n\n\n\n2.1\n✓\n✓\n\n2.0\n✓\n✓\n\n1.1\n✓\n✓\n\n1.1\n✓\n \n\n1.0\n✓\n \n\n\n\nFor additional driver versions, see Mongo Scala Driver Language Compatibility Reference.\n\n\n\n\nCasbah¶\nCasbah is the legacy Scala driver for MongoDB.\nIt provides wrappers and extensions to the\nJava Driver\nmeant to allow a more Scala-friendly interface to MongoDB.\nIt supports serialization/deserialization of common Scala types (including\ncollections and regex), Scala collection versions of DBObject and\nDBList and a fluid query DSL.\n\nAPI documentation\nMailing List\nJava Driver\n\n\nCompatibility¶\n\nMongoDB Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the Casbah for use with a specific version of\nMongoDB.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\nCasbah\nMongoDB 2.6\nMongoDB 3.0\nMongoDB 3.2\nMongoDB 3.4\n\n\n\n3.1\n✓\n✓\n✓\n✓\n\n3.0\n✓\n✓\n✓\n \n\n2.8\n✓\n✓\n✓\n \n\n\n\nFor additional driver versions, see Casbah Compatibility Reference.\nThe driver does not support older versions of MongoDB.\n\n\nLanguage Compatibility¶\nThe following compatibility table specifies the recommended\nversion(s) of the Casbah for use with a specific version of\nScala.\nThe first column lists the driver version(s).\n\n\n\n\n\n\n\n\n\nCasbah\nScala 2.9.3\nScala 2.10\nScala 2.11\nScala 2.12\n\n\n\n3.1\n \n✓\n✓\n✓\n\n3.0\n \n✓\n✓\n \n\n2.8\n✓\n✓\n✓\n \n\n\n\nFor additional driver versions, see Casbah Language Compatibility Reference.\n\n\n\n\nCommunity¶\n\nReactive-Mongo a reactive driver that\nallows you to design very scalable applications unleashing MongoDB\ncapabilities like streaming infinite live collections and files for\nmodern Realtime Web applications.\nRepo\nBlog post\n\n\nTepkin a reactive driver built on top of Akka.io and Akka Streams that\nhas a powerful DSL to easily create MongoDB records. It allows you to design very scalable applications by\nasynchronously inserting and retrieving (infinite) streams of data with non-blocking backpressure directly into\nrespectively out of MongoDB through Akka streams.\nRepo\nCode samples\n\n\nRogue: A Type-Safe Scala DSL\n- Foursquare’s DSL for querying MongoDB alongside Lift-MongoDB-Record.\n- Tutorial/Intro\n- Source/Downloads\nfrontlets lightweight\ntyped wrappers around Scala maps, with strong mongo support and JSON\nintegration. Supports type-safe queries in spirit of the original\nmongo collection interface, object graph traversal, immutable\nobjects among other features.\nSubset2 MongoDB document\nparser combinators and builders for use with the Java driver.\nHammersmith is a\nScala-based, asynchronous Netty driver for MongoDB with type-class\nbased fast custom object encoding.\nSalat is a simple serialization\nlibrary for case classes. Leverages MongoDB’s DBObject\n(which uses BSON underneath) as its target format. Salat is focused\non fostering a DWIM and intuitive usage pattern for the end-user’s\nbenefit, without sacrificing run time performance.\nBlue Eyes is a lightweight\nframework for building REST APIs with strong MongoDB integration\nincluding a DSL and Mock MongoDB for testing.\nLift Web Framework supports MongoDB through\nLift-MongoDB\nand object mapping via the\nRecord back-end\nimplementation.\nMssd (MongoDB Synchronous Scala Driver) is a synchronous Scala\ndriver which wraps the plain old java driver with a more elegant\nscala friendly API. Mssd focuses on simplicity of use with not too\nmuch magic involved.\nMongolia is a type-safe Scala\nDSL, based on the v2.+ Java driver. It enforces type-safety by implicitly\nconverting non-BSON values to BSON, fully configurable. It also bypasses\nthe Java drivers’ UUID bug by storing UUIDs as type 4 Binary.\n\n\n\n\n                \n    \n      ←  \n      Python MongoDB Drivers\n      Community Supported Drivers\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"MongoDB Scala Drivers","preview":"On this page This is an overview of the available tools for using Scala with\nMongoDB. Those wishing to skip to more detailed discussion should check\nout the Mongo Scala Driver Tutorial. The Mongo Scal","tags":""},{"slug":"drivers/syntax-table/","headings":[],"text":"MongoDB Drivers > \n          Driver Syntax Table \n      \n    \n  \n                \n                  \nDriver Syntax Table\nThe documentation generally gives examples in JavaScript, use this chart to\nconvert those examples to any language.\n\n\n\n\n\n\n\n\n\n\n\n\nJavaScript\nPython\nPHP\nRuby\nJava\nC++\nC#\nPerl\n\n\n\n[ ]\n\n\n\n[]\n\n\n\narray()\n\n\n\n[]\n\n\n\nBasicDBList\n\n\n\nBSONObj\n\n\nor\nbson::bo\n\n\n\nBsonArray\n\n\n\n[ ]\n\n\n\n\n{}\n\n\n\n{}\n\n\n\nnew stdClass\n\n\n\n{}\n\n\n\nBasicDBObject\n\n\n\nBSONObj\n\n\n\nBsonDocument\n\n\n\n{}\n\n\n\n\n{ x : 1 }\n\n\n\n{\"x\": 1}\n\n\n\narray('x' => 1)\n\n\n\n{'x' => 1}\n\n\n\nBasicDBObjectBuilder.start().add(\"x\", 1).get()\n\n\n\nBSONObjBuilder().append(\"x\", 1).obj();\nBSON( \"x\" << 1 )\n\n\n\nnew BsonDocument(\"x\", 1)\n\n\n\n{ x : 1 }\n\n\n\n\nconnect(\"www.example.net\")\n\n\n\nMongoClient(\"www.example.net\")\n\n\n\nnew MongoClient(\"mongodb://www.example.net\")\n\n\n\nMongoClient.new(\"www.example.net\")\n\n\n\nnew MongoClient(\"www.example.net\")\n\n\n\nmongo::DBClientConnection conn;\nconn.connect(\"www.example.net\");\n\n\n\nMongoClient(\"mongodb://www.example.net\");\n\n\n\nMongoDB::Client->new(host => 'www.example.net')\n\n\n\n\ncursor.next()\n\n\n\ncursor.next()\n\n\n\n$cursor->getNext()\n\n\n\ncursor.next\n\n\n\ncursor.next()\n\n\n\ncursor.next()\n\n\n\nforeach (var document in cursor)\n\n\n\n$cursor->next()\n\n\n\n\ncursor.hasNext()\n\n\n\nDoes not exist\n$cursor->hasNext()\n\n\n\ncursor.has_next?\n\n\n\ncursor.hasNext()\n\n\n\ncursor.more()\n\n\n\n \n$cursor->has_next()\n\n\n\n\ncollection.findOne()\n\n\n\ncollection.find_one()\n\n\n\n$collection->findOne()\n\n\n\ncollection.find_one\n\n\n\ncollection.findOne()\n\n\n\nconnection.findOne(namespace, query)\n\n\n\ncollection.FindOne()\n\n\n\n$collection->find_one()\n\n\n\n\n\n\n\n\n                \n    \n      ←  \n      Community Supported Drivers Reference\n      Driver Compatibility\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Driver Syntax Table","preview":"The documentation generally gives examples in JavaScript, use this chart to\nconvert those examples to any language. or © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered tradem","tags":""},{"slug":"drivers/","headings":[],"text":"MongoDB Drivers\n\nOn this page\n\nDrivers\nCommunity Supported Drivers\nReference\n\n\n\nDrivers¶\n\nNote\nFor information on next generation MongoDB drivers, see the\nfollowing blog posts:\n\nServer Discovery and Monitoring Blog Post\nServer Selection Blog Post\nConsistent CRUD API Blog Post\n\n\n\n\n\n\n\n\n\n\n\n\nDocumentation\nReleases\nSource\nAPI\nJIRA\nOnline Course\n\n\n\nC\nReleases\nSource\nAPI\nJIRA\n \n\nC++11\nReleases\nSource\nAPI\nJIRA\n \n\nC#\nReleases\nSource\nAPI\nJIRA\nCourse\n\nJava\nReleases\nSource\nAPI\nJIRA\nCourse\n\nNode.js\nReleases\nSource\nAPI\nJIRA\nCourse\n\nPerl\nReleases\nSource\nAPI\nJIRA\n \n\nPHP\nReleases\nSource\nAPI\nJIRA\n \n\nPython\nReleases\nSource\nAPI\nJIRA\nCourse\n\nMotor\nReleases\nSource\nAPI\nJIRA\n \n\nRuby\nReleases\nSource\nAPI\nJIRA\n \n\nScala\nReleases\nSource\nAPI\nJIRA\n \n\n\n\nFor information on MongoDB licensing, see MongoDB Licensing.\n\n\nC Driver\nC++ Driver\nC# and .NET Driver\nJava Driver\nNode.js Driver\nPerl Driver\nPHP Driver\nPHP Libraries, Frameworks and Tools\n\n\nPython Driver\nRuby Driver\nScala Driver\n\n\n\n\nCommunity Supported Drivers¶\n\n\n\n\n\n\n\n\n\n\nDocumentation\nReleases\nSource\nAPI\nJIRA\nOnline Course\n\n\n\nGo (mgo)\nReleases\nSource\nAPI\nJIRA\n \n\nErlang\nReleases\nSource\nAPI\nJIRA\n \n\n\n\n\nCommunity Supported Drivers Reference\n\n\n\nCommunity Supported Drivers\nGo Driver\nErlang Driver\nCommunity Supported Drivers Reference\n\n\n\n\n\n\nReference¶\n\n\nDriver Syntax Table\nDriver Compatibility\n\n\n\n\n\n                \n    \n      ←  \n      MongoDB Ecosystem Documentation Contents\n      C MongoDB Driver\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"MongoDB Drivers","preview":"On this page Note For information on next generation MongoDB drivers, see the\nfollowing blog posts: For information on MongoDB licensing, see MongoDB Licensing. © MongoDB, Inc 2008-2017. MongoDB, Mong","tags":""},{"slug":"genindex/","headings":[],"text":"Index\n\n\n \n\n\n\n    \n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Index","preview":"© MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.  \n  \n    Index","tags":""},{"slug":"index/","headings":[],"text":"MongoDB Drivers and Ecosystem\nThe MongoDB Ecosystem contains documentation for the drivers,\nframeworks, tools, and platform services that work with MongoDB. For\nthe main MongoDB documentation, see the MongoDB Manual.\n\nAnnouncing new online course\nAdobe Experience Manager and MongoDB\n\n\nMongoDB Drivers¶\n\n\n\n\n\n\n\n\n\n\nDocumentation\nReleases\nSource\nAPI\nJIRA\nOnline Course\n\n\n\nC\nReleases\nSource\nAPI\nJIRA\n \n\nC++11\nReleases\nSource\nAPI\nJIRA\n \n\nC#\nReleases\nSource\nAPI\nJIRA\nCourse\n\nJava\nReleases\nSource\nAPI\nJIRA\nCourse\n\nNode.js\nReleases\nSource\nAPI\nJIRA\nCourse\n\nPerl\nReleases\nSource\nAPI\nJIRA\n \n\nPHP\nReleases\nSource\nAPI\nJIRA\n \n\nPython\nReleases\nSource\nAPI\nJIRA\nCourse\n\nMotor\nReleases\nSource\nAPI\nJIRA\n \n\nRuby\nReleases\nSource\nAPI\nJIRA\n \n\nScala\nReleases\nSource\nAPI\nJIRA\n \n\n\n\nFor information on MongoDB licensing, see MongoDB Licensing.\n\n\nTools and Integration Frameworks¶\n\n\nMongoDB Connector for Hadoop\nHTTP Interface\nMunin Configuration Examples\nWireshark Support for MongoDB Protocol\n\n\n\n\nPlatforms and Services¶\n\n\nAmazon EC2\nRed Hat Enterprise Linux\ndotCloud\nJoyent Cloud\nRackspace Cloud\nRed Hat OpenShift\nVMware Cloud Foundry\nMicrosoft Azure\nWindows Quick Links and Reference Center\n\n\n\n\nUse Cases¶\n\n\nStoring Log Data\nPre-Aggregated Reports (MMAPv1)\nHierarchical Aggregation\nProduct Catalog\nInventory Management\nCategory Hierarchy\nMetadata and Asset Management\nStoring Comments\n\n\n\n\n\n                \n    \n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"MongoDB Drivers and Ecosystem","preview":"The MongoDB Ecosystem contains documentation for the drivers,\nframeworks, tools, and platform services that work with MongoDB. For\nthe main MongoDB documentation, see the MongoDB Manual. Announcing ne","tags":""},{"slug":"platforms/amazon-ec2/","headings":[],"text":"Platforms > \n          Amazon EC2 \n      \n    \n  \n                \n                  \nAmazon EC2\n\nOn this page\n\nMongoDB Atlas and EC2\nMongoDB Cloud Manager and EC2\nStorage Considerations\nManually Deploy MongoDB on EC2\nBackup, Restore, Verify\nDeployment Notes\nAdditional Resources\n\n\nTo deploy MongoDB on Amazon EC2, you\ncan either:\n\nSet up a new instance manually, or\nUse MongoDB Atlas or MongoDB Cloud Manager.\n\n\nMongoDB Atlas and EC2¶\nMongoDB Atlas is a hosted\ndatabase as a service from the team that engineers the database. Atlas\ndeploys MongoDB on AWS EC2 instances. The MongoDB Atlas GUI allows you:\n\nSet up and manage your clusters\nScale up or out with automated sharding\nMonitor your MongoDB deployments running in MongoDB Atlas\nConfigure security controls\nManage automated backups and failure recovery\n\n\n\n\nMongoDB Cloud Manager and EC2¶\nMongoDB Cloud Manager is a\nmanagement platform for MongoDB, delivered as a cloud service. MongoDB\nCloud Manager allows you to:\n\nMonitor your deployment and set custom alerts\nAutomate database setup and maintenance\nVisualize and optimize query performance\nManage automated backups and recovery\n\n\nStart a free trial of Cloud Manager to\nautomatically provision your EC2 instances.\n\nSee also\nCloud Manager documentation Provision machines on EC2.\n\n\n\nStorage Considerations¶\nEC2 instances can be configured with either ephemeral storage or\npersistent storage using the Elastic Block Store (EBS). Ephemeral\nstorage is lost when instances are terminated, so it is generally not\nrecommended unless you understand the data loss implications.\nFor almost all deployments EBS will be the better choice. For\nproduction systems we recommend using\n\nEBS-optimized EC2 instances\nProvisioned IOPS (PIOPS) EBS volumes\n\nStorage configuration needs vary among deployments, but for best\nperformance we recommend separate volumes for data files, the\njournal, and the log. Each has different write behavior, and\nplacing them on separate volumes reduces I/O contention.\n\nNote\nUsing different storage devices will affect your ability to create\nsnapshot-style backups of your data, since the files will be on\ndifferent devices and volumes.\n\nUsing RAID levels such as RAID0, RAID1, or RAID10 can provide\nvolume-level redundancy or capacity. Different storage configurations\nwill have different cost implications, especially when combined with\nPIOPS EBS volumes.\n\n\nManually Deploy MongoDB on EC2¶\nThe following steps can be used to deploy MongoDB on EC2 yourself. The\ninstances will be configured with the following characteristics:\n\nAmazon Linux\nMongoDB installed via yum\nIndividual PIOPS EBS volumes for data (1000 IOPS), journal (250 IOPS),\nand log (100 IOPS)\nUpdated read-ahead values for each block device\nUpdate ulimit settings\n\nBefore continuing be sure to have the following:\n\nInstall EC2 command line tools\nGenerate an EC2 key pair for connecting to the instance via SSH\nCreate a security group that allows SSH connections\n\nCreate the instance using the key pair and security group previously\ncreated and also include the --ebs-optimized flag and\nspecify individual PIOPS EBS volumes (/dev/xvdf for data,\n/dev/xvdg for journal, /dev/xvdh for log). Refer to the\ndocumentation for ec2-run-instances\nfor more information on devices and parameters.:\nec2-run-instances ami-05355a6c -t m1.large -g [SECURITY-GROUP] -k [KEY-PAIR] -b \"/dev/xvdf=:200:false:io1:1000\" -b \"/dev/xvdg=:25:false:io1:250\" -b \"/dev/xvdh=:10:false:io1:100\" --ebs-optimized true\n\n\nYou can use the returned instance-id to ascertain the IP Address or\nDNS information for the instance:\nec2-describe-instances [INSTANCE-ID]\n\n\nNow SSH into the instance:\nssh -i /path/to/keypair.pem ec2-user@ec2-1-2-3-4.amazonaws.com\n\n\nAfter login, update installed packages, add the MongoDB yum repo, and install MongoDB:\necho \"[mongodb-org-3.2]\nname=MongoDB Repository\nbaseurl=https://repo.mongodb.org/yum/amazon/2013.03/mongodb-org/3.2/x86_64/\ngpgcheck=1\nenabled=1\ngpgkey=https://www.mongodb.org/static/pgp/server-3.2.asc\" |\n  sudo tee -a /etc/yum.repos.d/mongodb-org-3.2.repo\n\n\nsudo yum -y update && sudo yum install -y mongodb-org-server \\\n    mongodb-org-shell mongodb-org-tools\n\n\nNext, create/configure the mount points, mount each volume, set ownership (MongoDB runs under the mongod user/group), and set the /journal link:\nsudo mkdir /data /log /journal\n\nsudo mkfs.ext4 /dev/xvdf\nsudo mkfs.ext4 /dev/xvdg\nsudo mkfs.ext4 /dev/xvdh\n\necho '/dev/xvdf /data ext4 defaults,auto,noatime,noexec 0 0\n/dev/xvdg /journal ext4 defaults,auto,noatime,noexec 0 0\n/dev/xvdh /log ext4 defaults,auto,noatime,noexec 0 0' | sudo tee -a /etc/fstab\n\nsudo mount /data\nsudo mount /journal\nsudo mount /log\n\nsudo chown mongod:mongod /data /journal /log\n\nsudo ln -s /journal /data/journal\n\n\nNow configure the following MongoDB parameters by editing the configuration file /etc/mongod.conf so that it contains the following:\ndbpath = /data\nlogpath = /log/mongod.log\n\n\nIf you don’t want MongoDB to start at boot, you can issue the following\ncommand:\nsudo chkconfig mongod off\n\n\nBy default Amazon Linux uses ulimit settings that are not\nappropriate for MongoDB. To setup ulimit to match the\ndocumented ulimit settings, run the\nfollowing command:\necho '* soft nofile 64000\n* hard nofile 64000\n* soft nproc 64000\n* hard nproc 64000' | sudo tee /etc/security/limits.d/90-mongodb.conf\n\n\nAdditionally, default read ahead settings on EC2 are not optimized for\nMongoDB. As noted in the read-ahead settings from Production\nNotes, you should adjust the settings\nto read approximately 32 blocks (or 16 KB) of data. The\nfollowing command will set the readahead appropriately (repeat for\nadditional volumes):\nsudo blockdev --setra 32 /dev/xvdf\n\n\nTo make this change persistent across system boot, issue the following\ncommand:\necho 'ACTION==\"add|change\", KERNEL==\"xvdf\", ATTR{bdi/read_ahead_kb}=\"16\"' | sudo tee -a /etc/udev/rules.d/85-ebs.rules\n\n\nOnce again, repeat the above command for all required volumes (note:\nthe device we created was named /dev/xvdf but the name used by\nthe system is xvdf).\nTo start mongod, issue the following command:\nsudo service mongod start\n\n\nThen connect to the MongoDB instance using the mongo shell:\nmongo\n\n\nTo have MongoDB startup automatically at boot issue the following command:\nsudo chkconfig mongod on\n\n\nFor production deployments consider using Replica Sets or Sharding.\n\n\nBackup, Restore, Verify¶\nDepending upon the configuration of your EC2 instances, there are a\nnumber of ways to conduct regular backups of your data. For specific\ninstructions on backing up, restoring and verifying refer to\nEC2 Backup and Restore.\n\n\nDeployment Notes¶\n\nInstance Types¶\nMongoDB works on most EC2 types including Linux and Windows.  We\nrecommend you use a 64 bit instance as this is\nrequired for all MongoDB databases of significant size.\nAdditionally, we find that the larger instances tend to be on the\nfreshest ec2 hardware.\n\nRunning MongoDB¶\nBefore starting a mongod instance, decide where to put your\ndata files.  Run df -h to see a list of available\nvolumes. On some images, /mnt will be the locally-attached\nstorage volume. Alternatively, you may want to use Elastic Block Store which will have a different mount\npoint.\nMounting the filesystem with the mount -o noatime option can\nimprove disk performance. For example, the following is an example\nof an /etc/fstab stanza with this option:\n/dev/mapper/my_vol /var/lib/mongodb xfs noatime,noexec 0 0\n\n\nCreate the mongodb data file directory in the desired location and then\nstart the mongod server:\nmkdir /mnt/db\n./mongod --fork --logpath ~/mongod.log --dbpath /mnt/db/\n\n\n\n\nOperating System¶\nOccasionally, due to the shared and virtualized nature of EC2, an\ninstance can experience intermittent I/O problems and low responsiveness\ncompared to other similar instances. Terminating the instance and\nbringing up a new one can in some cases result in better performance.\nSome people have reported problems with ubuntu 10.04 on ec2.\nPlease read\nUbuntu issue 614853\nand\nLinux Kernel issue 16991 for further\ninformation.\n\n\n\nNetworking¶\n\nPort Management¶\nBy default the database will now be listening on port 27017. The web\nadministrative UI will be on port 28017.\n\n\nKeepalive¶\nChange the default TCP keepalive time to 300 seconds. See our\ntroubleshooting page for details.\n\n\n\nSecure Instances¶\nRestrict access to your instances by using the Security Groups feature\nwithin AWS. A Security Group is a set of firewall rules for incoming\npackets that can apply to TCP, UDP or ICMP.\nA common approach is to create a MongoDB security group that contains\nthe nodes of your cluster (replica set members or sharded cluster\nmembers), followed by the creation of a separate security group for your\napp servers or clients.\nCreate a rule in your MongoDB security group with the “source” field set\nto the Security Group name containing your app servers and the port set\nto 27017 (or whatever port you use for your MongoDB). This will ensure\nthat only your app servers have permission to connect to your MongoDB\ninstances.\nRemember that Security Groups only control ingress traffic.\n\n\nCommunication Across Regions¶\nEvery EC2 instance will have a private IP address that can be used to\ncommunicate within the EC2 network. It is also possible to assign a\npublic “elastic” IP to communicate with the servers from another\nnetwork. If using different EC2 regions, servers can only communicate\nvia public IPs.\nTo set up a cluster of servers that spans multiple regions, it is\nrecommended to name the server hostname to the “public dns name”\nprovided by EC2. This will ensure that servers from a different network\nuse the public IP, while the local servers use the private IP, thereby\nsaving costs. This is required since EC2 security groups are local to a\nregion.\nTo control communications between instances in different regions (for\nexample, if you have two members of a replica set in one region and a\nthird member in another), it is possible to use a built-in firewall\n(such as IPtables on Linux) to restrict allowed traffic to certain\n(elastic) IP addresses or ports.\nFor example one solution is following, on each server:\n\nset the hostname of the server\n\nsudo hostname server1\n\n\n\ninstall “bind”, it will serve as local resolver\nadd a zone for your domain, say “myorg.com”, and add the CNAMEs for all your servers\n\nserver1          IN     CNAME   ec2-50-19-237-42.compute-1.amazonaws.com.\nserver2          IN     CNAME   ec2-50-18-157-87.us-west-1.compute.amazonaws.com.\n\n\n\nrestart bind and modify /etc/resolv.conf to use the local bind\n\nsearch myorg.conf\nnameserver 127.0.0.1\n\n\nThen:\n\nverify that you can properly resolve server1, server2, … using a\ntool like dig.\nwhen running mongod, db.serverStatus() should show the\ncorrect hostname, e.g. “server1:27017”.\nyou can then set up replica sets or shards using the simple\nhostname. For example connect to server1 and run\nrs.initiate(), then rs.add('server2:27017').\n\n\n\n\nAdditional Resources¶\n\nVirtualizing MongoDB on Amazon EC2 and GCE: Part 1\nVirtualizing MongoDB on Amazon EC2 and GCE: Part 2\n\n\n\n\n                \n    \n      ←  \n      Platforms\n      EC2 Backup and Restore\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Amazon EC2","preview":"On this page To deploy MongoDB on Amazon EC2, you\ncan either: MongoDB Atlas is a hosted\ndatabase as a service from the team that engineers the database. Atlas\ndeploys MongoDB on AWS EC2 instances. The","tags":""},{"slug":"platforms/digitalocean/","headings":[],"text":"Platforms > \n          DigitalOcean \n      \n    \n  \n                \n                  \nDigitalOcean\n\nOn this page\n\nLaunching a MongoDB Instance\nInstance Details\nAdditional Information\n\n\nDeploying a MongoDB instance on DigitalOcean\nis simple with the MongoDB One-Click application. It allows you to spin up a\nserver with MongoDB pre-installed so you can get your application off the\nground quickly.\n\nLaunching a MongoDB Instance¶\nYou can launch a new MongoDB instance by selecting MongoDB from the\nApplications tab on the Droplet create page:\n\nOnce you have created your server, you can connect to it via the web-based\nconsole in the DigitalOcean control panel or via SSH:\nssh root@your.ip.address\n\n\n\nUsing the API¶\nOne-Click Application images are also available via DigitalOcean’s RESTful\nAPI. Use the slug\n(\"mongodb\") rather than an ID when specifying the image to ensure you are\nusing the most up to date version.\nThe following example will launch a launch a 2 GB server with private\nnetworking enabled, running MongoDB in the NYC3 region. Remember to\nreplace the API access token with one of your own obtained in the\nDigtialOcean control panel.\ncurl -X POST -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer b7d03a6947b217efb6f3ec3bd3504582' \\\n    -d '{\"name\":\"shard-01.example.com\", \"region\":\"nyc3\",\"size\":\"2gb\", \"image\":\"mongodb\", \"private_networking\":true}' \\\n    \"https://api.digitalocean.com/v2/droplets\"\n\n\n\n\n\nInstance Details¶\nThe environment on your new server will consist of:\n\nUbuntu 14.04 LTS\nMongoDB 3.x installed via Apt from the upstream repository\n\nMongoDB will be available at port 27017 and is bound to the localhost by\ndefault. Its data is stored in /var/lib/mongodb/. You can view or\nmodify its configuration details in /etc/mongod.conf.\nTo connect to the test database with the MongoDB shell, simply run:\nmongo\n\n\nThe daemon will be running by default and can be managed using the native\nservice commands, i.e.:\nsudo service mongodb restart\n\n\n\n\nAdditional Information¶\nThe One-Click application simply provides you with MongoDB as a pre-installed\nbase. It’s up to you how you want to use it. Whether you are building out a\nsharded cluster or simply want to connect it to an app on the same host, the\nDigitalOcean community has a number of tutorials which should point you in the\nright direction:\n\nHow To Implement Replication Sets in MongoDB\nHow To Create a Sharded Cluster in MongoDB\nHow To Connect Node.js to a MongoDB Database\n\nA MEAN Stack One-Click application image\nis also available. It provides Node.js, Express, and AngularJS pre-installed\nalong side of MongoDB.\n\n\n\n                \n    \n      ←  \n      EC2 Backup and Restore\n      dotCloud\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"DigitalOcean","preview":"On this page Deploying a MongoDB instance on DigitalOcean\nis simple with the MongoDB One-Click application. It allows you to spin up a\nserver with MongoDB pre-installed so you can get your application","tags":""},{"slug":"platforms/dotcloud/","headings":[],"text":"Platforms > \n          dotCloud \n      \n    \n  \n                \n                  \ndotCloud\n\nOn this page\n\nIf You Don’t Have a dotCloud Account Yet\nWith a dotCloud Account\nScaling\nAdvanced Use\nMore Docs\nReady-to-use Apps\nGetting help\n\n\nMongoDB can run on the dotCloud platform. It supports\nreplica sets, and has alpha support for sharding.\nThe whole point of dotCloud is to run your apps and your databases in\nthe same place, to optimize for latency and reliability. However, you\ncan also deploy MongoDB on dotCloud and use it to power an app running\nanywhere else.\n\nIf You Don’t Have a dotCloud Account Yet¶\nWell, what are you waiting for? :-)\nGo ahead and create one (it’s free!) and\ninstall the CLI:\nsudo easy_install pip; sudo pip install dotcloud\n\n\nIf you need help to get the CLI running, check the\ndotCloud install docs and don’t\nhesitate to ask for help.\n\n\nWith a dotCloud Account¶\nThe following snippet will deploy MongoDB on dotCloud for you in no\ntime:\nmkdir mongodb-on-dotcloud\ncat >mongodb-on-dotcloud/dotcloud.yml <<EOF\ndb:\n  type: mongodb\nEOF\ndotcloud push mongorocks mongodb-on-dotcloud\ndotcloud info mongorocks.db\n\n\nThe last command will show you the host, port, and credentials to be\nused to connect to your database.\n\n\nScaling¶\nAssuming you followed the instructions of the previous section, if you\nwant to get a replica sets of 3 servers:\ndotcloud scale mongorocks db=3\n\n\n\n\nAdvanced Use¶\nIf you want to have a closer look at your MongoDB server, nothing beats\nSSH access:\ndotcloud ssh mongorocks.db\n\n\n\n\nMore Docs¶\n\ndotCloud documentation for the MongoDB service\ngeneric introduction to dotCloud (in\ncase you want to run not only MongoDB, but also Node.js, Python,\nRuby, Perl, Java, RabbitMQ, Redis, MySQL, PostgreSQL, CouchDB, Riak,\nErlang, or something else, on dotCloud)\n\n\n\nReady-to-use Apps¶\nAll you need to do to run them is a git clone and a dotcloud push:\n\nDjango setup using MongoDB to store objects\nMongoDB + Node.js sample app\n\n\n\nGetting help¶\ndotCloud has a Q&A site,\nand the dotCloud team can be reached through the FreeNode IRC network\non hash tag “dotcloud”.\n\n\n\n                \n    \n      ←  \n      DigitalOcean\n      Joyent Cloud\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"dotCloud","preview":"On this page MongoDB can run on the dotCloud platform. It supports\nreplica sets, and has alpha support for sharding. The whole point of dotCloud is to run your apps and your databases in\nthe same plac","tags":""},{"slug":"platforms/modulus/","headings":[],"text":"Platforms > \n          Modulus \n      \n    \n  \n                \n                  \nModulus\n\nOn this page\n\nModulus Account Setup\nCreate a Database\nMore Documentation\nHelp\n\n\nModulus is a premier Node.js and MongoDB hosting platform that\nprovides a complete technology stack for application developers. This\nincludes custom SSL, WebSockets, statistics, and more.\nMongoDB is available as a first-class solution on the Modulus\nplatform. The platform provides easy managment\ntools to create databases, create and update users, and observe\ndatabase performance in realtime.\nModulus supports replica sets on dedicated plans. This includes the\nablity for Meteor users to have access to the oplog. To find out more\nabout our dedicated plans, contact sales.\nBy providing a place to run both Node.js applications and MongoDB\ndatabases Modulus gives you a single place to handle all your platform\nneeds. This means faster connections between your applications and\ndatabases, and fewer headaches managing them. This doesn’t mean that\nyou have to run your application on Modulus; you can choose any\napplication platform you’d like.\nIf you need a dedicated environment, Modulus provides the answer with\nCurvature. You can get everything mentioned\nabove with the flexibility to choose where you run your application.\nIt could be on-premises, in a dedicated cloud, or a hybrid of the two.\n\nModulus Account Setup¶\nIf you don’t have an account yet, go ahead and create one.\nNext, install the CLI (requires Node.js & npm).\nnpm install -g modulus\n\n\nIf you have any questions check the documentation first, and if that\ndoesn’t provide you with an answer contact Modulus.\nNow make sure you’re logged in.\nmodulus login\n\n\nOnce logged in you’re ready to go.\n\n\nCreate a Database¶\nIt’s very simple to create a database on Modulus.\nmodulus mongo create \"Database Name\"\nWelcome to Modulus\nYou are logged in as spiderman\nCreating MongoDB database Database Name\n[√] New MongoDB database Database Name created.\nDatabase available at: noveda.modulusmongo.net:27017/xxxxxxx\n\n\nOnce you’ve created a database, you’ll need to create a user.\nWelcome to Modulus\nYou are logged in as spiderman\n[?] Are you sure you want to use database Database Name? (yes)\nSelected MongoDB database Database Name.\n[?] Enter username: johnny\n[?] Enter password: five\n[?] Read only permissions? (yes) no\n[✓] New MongoDB database user johnny created.\n\n\nThe full MongoDB URI corresponding to the above steps is:\nmongodb://johnny:five@noveda.modulusmongo.net:27017/xxxxxxx\n\n\nDatabases can also be created through the web interface. Navigate to\nyour User dashboard and select the Databases tab on the left.\nTo create a new database click the “Create Database” button.\n\nOn this dialog enter your desired label for the database. This name is\nsimply a convenience label and is not the actual name of the database\nwithin MongoDB. Actual database names are created automatically by\nModulus.\nYou can also create a database user using the web interface. You must\ncreate a user in order to access your database.\nOnce complete, the popup will change with information about your\nnewly-created database.\n\nThe MongoDB URI can be used to connect to your database from your\napplication.  The Mongo Console is the command used to connect to your\ninstance using the mongo shell. This information can also\nbe retrieved at any time from the database dashboard.\n\n\nMore Documentation¶\n\nMongoDB-specific documentation\nModulus documentation\n\n\n\nHelp¶\nThere are many ways you can get help with Modulus. We always recommend\njoining the IRC channel as a first step. You can also contact the\nModulus team via email and Twitter.\n\n#modulus on irc.freenode.net IRC\nEmail\n@OnModulus on Twitter\n\n\n\n\n                \n    \n      ←  \n      Joyent Cloud\n      Rackspace Cloud\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Modulus","preview":"On this page Modulus is a premier Node.js and MongoDB hosting platform that\nprovides a complete technology stack for application developers. This\nincludes custom SSL, WebSockets, statistics, and more.","tags":""},{"slug":"platforms/rackspace-cloud/","headings":[],"text":"Platforms > \n          Rackspace Cloud \n      \n    \n  \n                \n                  \nRackspace Cloud\n\nOn this page\n\nDeployment Planning\nDeploy a Single Node\nReplica Sets\nBackups\n\n\nThis guide is intended to provide instructions on getting started with\nMongoDB using Rackspace Cloud Servers.\nFirst we’ll step through deployment planning (instance sizes, topology,\nsecurity) and then we’ll set up a single MongoDB node. We’ll use the\nsame steps to create a multi-node replica set and cover the steps needed\nto backup your database.\n\nDeployment Planning¶\n\nInstance Sizes¶\nRackspace Cloud offers instances with RAM ranging from 256 MB up to 64\nGB. When considering instances for initial development purposes, those\nwith 256 or 512 MB are an appropriate starting point. As development\nprogresses towards production-level deployments we recommend moving to\nhigher memory instances. The top-end Cloud instances are appropriate for\npre-production and some production deployments. If you need to grow your\ndeployment beyond the Cloud instances, Rackspace also offers managed\ndedicated servers that can be scaled to increase database performance\nand throughput.\nWhen planning your deployment, it’s important to account for your\nresource needs for today and into the future. If you plan on growing\nyour production systems into Rackspace’s dedicated services then it may\nbe useful to consider using RackConnect. This is a service that bridges\nRackspace’s public cloud infrastructure with their private dedicated\nservers. It makes migrating from Cloud to dedicated much easier and is\nalso useful when creating a hybrid deployment solution combining\nresources from both. For more information refer to\nRackspace’s RackConnect site.\n\n\nTopology¶\nThe following are example deployment scenarios for MongoDB built on\nRackspace servers.\nThe single node example is the simplest; a single MongoDB instance can\nbe deployed in Rackspace Cloud or Managed hosting. This deployment is\nuseful for development and early app testing.\n\nFor production-level deployments, we’ll move to a three node Replica Set\nrunning in Rackspace Cloud.\n\nDepending on the size of your database deployment or if your app\nrequires greater levels of database performance, another option is a\nhybrid deployment. In this case, a Replica Set which spans Rackspace\nCloud and Managed hosting can be deployed. Cloud and Managed hosting are\n“connected” via RackConnect, providing a faster interconnect between the\ntwo hosting services.\n\n\n\nSecurity¶\nWhen deploying onto Rackspace Cloud, you’ll need to secure access to\nyour instances so that MongoDB is only available to known safe systems.\nTo secure your instances we’ll use the iptables command-line tool to\ncontrol access to the system with a firewall. Once we’ve completed\nsecuring our instance, it will be using the following firewall\nconfiguration:\nport 22 (SSH) will accept any incoming TCP connections\nAll other ports will reject incoming TCP connections\nLater on when we deploy multiple instances we’ll open up port 27017 (for\nmongod) to other specific IP addresses in order to facilitate a working\nreplica set. The ports required for sharding (27018 and 27019) aren’t\nrequired immediately since we’ll be setting up a single node and replica\nset however just remember to open them up later if necessary.\n\n\nMonitoring¶\nMonitoring is a critical component of all database servers. MongoDB\nincludes several tools to help gather statistics about data sizes,\nindex sizes and analyze queries.\nConsider the MongoDB Cloud Manager, a cloud-hosted deployment, monitoring, and backup\nsolution, that provides custom dashboards and reporting for your MongoDB\ninstances. For more information, refer to the MongoDB Cloud Manager documentation.\n\n\n\nDeploy a Single Node¶\n\nCreate an Instance¶\nLog in to the Rackspace Cloud control panel and navigate to\nHosting > Cloud Servers > Add Server. In this example we used Ubuntu 11.10\n(Oneiric Ocelot) as the OS image for our instance. Once selected, enter\na server name, select a server size and continue. Be sure to record your\nroot password or you’ll have to reset it later. Once the instance is\navailable, connect to it via SSH to secure it. By default, you’ll be\nconnecting to the server as root. If you’d like to create additional\nusers/groups to use with your instance, consult the documentation for\nthe chosen OS. For our configuration, we used the admin group and added\nan additional user, per the\nUbuntu 11.10 server guide.\nFirst we added a new user (admin) to the admin group:\nroot# adduser admin --ingroup admin\n\n\nAfter stepping through the prompts to set a password (and other account\ninformation), the admin user will be created and added to the admin\ngroup.\nAt this point, log out of the server as root and login as admin.\n\n\nSecure Your Instance¶\nAs discussed above in Security, we’ll use a\nsimple firewall setup to secure access to the instances running MongoDB.\nThe built-in ufw command provides a simple mechanism to configure the\nsystem firewall. First off, enable the firewall and add a rule to allow\nSSH access:\nsudo ufw enable\nsudo ufw allow 22\n\n\nNow the only port that will allow connections is port 22, attempts to\nconnect on any other port will be dropped. With this firewall setup, the\ninstance is secure and we can proceed with configuring MongoDB. For more\ninformation, refer to the\nRackspace Introduction to IPTables\nand the\nUbuntu firewall guide.\n\n\nInstall MongoDB¶\nUsing the instructions from the MongoDB documentation on\nUbuntu and Debian packages\nwe added an additional apt-get source and installed DB.\nFirst add the MongoDB GPG key to apt-get to create a “trusted” source:\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv 7F0CEB10\n\n\nNext, add the following line to /etc/apt/sources.list:\nsudo nano /etc/apt/sources.list\n...\ndeb http://downloads-distro.mongodb.org/repo/ubuntu-upstart dist 10gen\n...\n\n\nNow update apt-get to pick up the new packages:\nsudo apt-get update\n\n\nFinally, install MongoDB:\nsudo apt-get install mongodb-10gen\n\n\nAt this point MongoDB will be installed and started, which we can\nconfirm via the service utility:\nsudo service mongodb status\nmongodb start/running, process 2308\n\n\nYou can also check the status by using tail to examine the MongoDB log:\nsudo tail -f /var/log/mongodb/mongodb.log\n\n\n\n\nConfigure MongoDB¶\nBy default, MongoDB is set to use /var/lib/mongodb as the data path. If\nyou’d like to amend that, or update the log path, shutdown the MongoDB\ninstance and update /etc/mongodb.conf with any changes.\nIf you change the MongoDB dbpath or logpath, be sure to set the proper\nownership, etc. For example, to change MongoDB to use /data as the data\npath, use the following steps.\nFirst, shutdown the MongoDB service:\nsudo service mongodb stop\n\n\nNext, update the MongoDB configuration file:\nsudo nano /etc/mongodb.conf\n...\ndbpath=/data\n...\n\n\nNow create the /data directory and update it’s ownership:\nsudo mkdir /data\nsudo chown mongodb:mongodb /data\n\n\nRestart MongoDB with\nsudo service mongodb start\n\n\n\n\n\nReplica Sets¶\nMongoDB Replica Sets are a form of asynchronous\ndata replication between primary/secondary instances of MongoDB, adding\nautomatic failover and automatic recovery of secondary nodes. In\naddition, Replica Sets can also provide the ability to distribute the\nread workload of your data.\nTo create a Replica Set, first create three instances of MongoDB. Next\ninstall and configure MongoDB on each instance. (See\n/tutorial/deploy-replica-set.) Just before starting MongoDB,\nedit the /etc/mongodb.conf configuration file, adding the replSet\nparameter:\nsudo nano /etc/mongodb.conf\n...\nreplSet=replicaSetName\n...\n\n\nBefore proceeding, note that our current security setup has blocked\naccess to all incoming ports (other than port 22 for SSH) therefore the\nMongoDB instances won’t be accessible. We’ll need to add rule to each\ninstance that allows access to port 27017 (the standard mongod port)\nfrom the other instances in our replica set.\nCollect the IP addresses of the instances in your Replica Set and\nexecute the following ufw commands on each instance:\nsudo ufw allow proto tcp from [IP Address 1] to any port 27017\nsudo ufw allow proto tcp from [IP Address 2] to any port 27017\nsudo ufw allow proto tcp from [IP Address 3] to any port 27017\n\n\nThen start MongoDB with\nsudo service mongodb start\n\n\nOnce MongoDB has started and is running on each instance we’ll connect\nto the desired primary node, initialize the Replica Set and add the\nsecondary nodes. First connect to the desired primary node and launch\nthe mongo shell:\nmongo\nMongoDB shell version: 2.0.4\nconnecting to: test\n\n\nNow initialize the Replica Set:\nrs.initiate()\n{\n   \"info2\" : \"no configuration explicitly specified -- making one\",\n   \"me\" : \"67.207.133.237:27017\",\n   \"info\" : \"Config now saved locally. Should come online in about a minute.\",\n   \"ok\" : 1\n}\n\n\nNext add the other nodes to the Replica Set. If you’re executing this on\nthe first configured instance, add the other 2 instances here:\nrs.add(\"[IP Address 2]\");\n{ \"ok\" : 1 }\nrs.add(\"[IP Address 3]\");\n{ \"ok\" : 1 }\n\n\nFinally, you can check the status of the Replica Set from the\nmongo shell with the following:\nrs.status()\n\n\nIn the output of the rs.status() command, you should see three entries,\none for each instance. Each one should provide information about it’s\ncurrent status within the Replica Set. For more information, see\nMongoDB Replica Sets.\n\n\nBackups¶\nTo facilitate backups there are two main options: Cloud Servers images\nor MongoDB built-in backup tools.\n\nCloud Servers Images¶\nCloud Servers images are snapshots of an entire instance which are saved\nin Rackspace Cloud Files. Images can be created from any running\ninstance and the process can be completed via the Rackspace Cloud\ncontrol panel. When creating backups from instances in a Replica Set, be\nsure to image a SECONDARY instance to avoid any interruptions for your\napplications or data. To image a SECONDARY, first lock it against\ndatabase writes from within the Replica Set, create the image and then\nunlock it to rejoin the Replica Set.\nTo lock the SECONDARY database, connect with the mongo shell and use the\nfollowing command:\nSECONDARY> db.fsyncLock()\n{\n   \"info\" : \"now locked against writes, use db.fsyncUnlock() to unlock\",\n   \"seeAlso\" : \"http://docs.mongodb.com/manual/reference/command/fsync/\",\n   \"ok\" : 1\n}\n\n\nWhile the database is locked it will not accept any incoming writes from\nthe PRIMARY instance. Once locked, go to the Rackspace Cloud control\npanel and create a “New On-Demand Image”. Once the image has been\ncreated and is complete, go back to the mongo shell and issue the\n“unlock” command:\nSECONDARY> db.fsyncUnlock()\n{ \"ok\" : 1, \"info\" : \"unlock completed\" }\n\n\nOnce unlocked, any data that was written to the PRIMARY while the\nSECONDARY was locked will asynchronously replicate to the newly unlocked\nSECONDARY.\n\n\nBuilt-in Tools¶\nMongoDB ships with several built-in command line tools to help with\ndatabase administration. One of those tools mongodump can be used to\ncreate file-based backups of the running database. Mongodump can be used\non a live database instance however for the sake of data consistency, it\nis wise to use the --oplog option when calling mongodump. This allows\nfor point-in-time backups that catch any data that was written while\nmongodump was working. For more information about live data backups,\nsee the mongodump documentation.\n\n\n\n\n                \n    \n      ←  \n      Modulus\n      Red Hat OpenShift\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Rackspace Cloud","preview":"On this page This guide is intended to provide instructions on getting started with\nMongoDB using Rackspace Cloud Servers. First we’ll step through deployment planning (instance sizes, topology,\nsecur","tags":""},{"slug":"platforms/red-hat-enterprise-linux/","headings":[],"text":"Red Hat Enterprise Linux\nTo install MongoDB with Red Hat Enterprise Linux, see the\nInstall MongoDB on Red Hat Enterprise Linux\ntutorial.\n\nSee also\nRed Hat OpenShift\n\n\nIdentity Management¶\nThe Identity Management (IdM) solution in Red Hat Enterprise Linux\nintegrates Kerberos authentication, directory services, certificate\nmanagement, DNS, and NTP into a single service. The following\ntutorials describe how to set up and configure MongoDB enterprise to\ntake advantage of these features when managing MongoDB deployments.\n\n\nConfigure Red Hat Enterprise Linux Identity Management with MongoDB\nOperational Procedures using Red Hat Enterprise Linux Identity Management\n\n\n\n\n\n                \n    \n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Red Hat Enterprise Linux","preview":"To install MongoDB with Red Hat Enterprise Linux, see the\nInstall MongoDB on Red Hat Enterprise Linux\ntutorial. See also Red Hat OpenShift The Identity Management (IdM) solution in Red Hat Enterprise","tags":""},{"slug":"platforms/red-hat-openshift/","headings":[],"text":"Platforms > \n          Red Hat OpenShift \n      \n    \n  \n                \n                  \nRed Hat OpenShift\nOpenShift is a Platform as a Service (PaaS) offering from RedHat, which\nprovides support for rapid deployment and automatic scalability support\nfor web applications developed with Java EE, Node.js, Python, PHP, Perl\nand Ruby. OpenShift Express is a free shared-cloud solution for\ndeploying your applications. With Express, you simply push your app\nusing the command line tools to OpenShift and the platform takes care\nof the hosting infrastructure.\nIn this guide, we’ll take a simple Java web application and deploy it\nonto OpenShift Express. We’ll walk through the steps required to setup\nand administer MongoDB as well as backing up your data.\n\nCloning the App¶\nWe’ll be deploying an existing Java web application onto OpenShift. The\napp is a simple REST-style checkin mechanism with a single endpoint.\nThe app supports POST-ing new checkins (by supplying a comment and\nlocation) and GET-ing checkins near a given location (by supplying a\npair of coordinates). The app was built using Java SE 6 and Maven. Make sure to have\nthose components installed, along with Git for\nyour platform, before continuing.\nFirst, start by cloning the repository for the web application:\ngit clone https://github.com/crcsmnky/openshift-checkins.git\ncd openshift-checkins\n\n\nNext, make sure you can build the app as-is using Maven:\nmvn package\n\n\nIf that completes successfully, you’re ready to move on. To prepare our\napp for deployment, we’ll need to setup our OpenShift Express account.\n\n\nDeploying onto Express¶\nTo deploy apps onto Express you’ll need to create an OpenShift account\nfrom the OpenShift sign up page and install the\nOpenShift command line tools.\nFirst, follow the steps from the OpenShift Express getting started\nguide in the\nInstall the client tools section for your platform. Once the tools\nhave been installed we’ll create a domain name, app name and then push\nthe sample app to OpenShift Express (the URL for the app will be\nsomething like http://appname-domainname.rhcloud.com).\nFirst, create the domain name:\n$ rhc domain create -n your-domain -l your-openshift-login\n\n\nThis command will prompt you for your account password then for an SSH\npassphrase (it generates a public/private keypair to use for connecting\nto the OpenShift service). Next, create an entry for the app that we’ll\nbe deploying. Here you’ll need to supply the app name (example,\nexpresscheckin) and jbossas-7.0 as the app type:\n$ rhc app create -a expresscheckin -t jbossas-7\nPassword:\nCreating application: expresscheckin\nNow your new domain name is being propagated worldwide (this might take a minute)...\nWarning: Permanently added 'expresscheckin-your-domain.rhcloud.com,50.16.164.248' (RSA) to the\nlist of known hosts.\nConfirming application 'expresscheckin' is available:  Success!\n\nexpresscheckin published:  http://expresscheckin-your-domain.rhcloud.com/\ngit url:  ssh://429827960cbf4518b1785ed928db9be7@expresscheckin-your-\ndomain.rhcloud.com/~/git/expresscheckin.git/\nSuccessfully created application: expresscheckin\n\n\nAfter the app is created the command will also clone the app’s\nrepository locally. Before continuing, we need to set up MongoDB on\nOpenShift and also update our app’s MongoDB connection information:\n$ rhc app cartridge add -a expresscheckin -c mongodb-2.0\n\nRESULT:\n\nMongoDB 2.0 database added.  Please make note of these credentials:\n\n       Root User: admin\n   Root Password: 1wbLGYAmPgDM\n   Database Name: expresscheckin\n  Connection URL: mongodb://127.6.85.129:27017/\n\nYou can manage your new MongoDB by also embedding rockmongo-1.1\n\n\nNow that we’ve added support for MongoDB to our app (on OpenShift)\nwe’ll need to take the credentials returned and update our\nopenshift-checkins app configuration. Go back to the\nopenshift-checkins directory and edit CheckinServlet.java and\nadd/update the following lines in the init function using the\nMongoDB details provided by OpenShift (don’t forget to uncomment the\nif statement with the authentication statement):\n$ cd openshift-checkins\n$ nano src/main/java/CheckinServlet.java\n...\nconn = new Mongo(\"127.6.85.129\", 27017);\ndb = conn.getDB(\"expresscheckin\");\n\nif (!db.authenticate(\"admin\", \"1wbLGYAmPgDM\".toCharArray())) {\n  throw new MongoException(\"unable to authenticate\");\n}\n...\n\n\nThen build the openshift-checkins WAR file:\n$ mvn package\n\n\nNow, return to the cloned repository, remove the sample code generated\nfrom the expresscheckin app repo that was cloned to our system and\ncopy the checkins.war file into the deployments as\nROOT.war:\n$ cd ../expresscheckin\n$ rm -rf pom.xml src\n$ cp ../openshift-checkins/target/checkins.war deployments/ROOT.war\n\n\nAs part of the deployment process, we also need to flag our WAR file to\nbe deployed (expanded and copied to the right places):\n$ touch deployments/ROOT.war.dodeploy\n\n\nNow we can add it to the repository, commit and push:\n$ git add -A\n$ git commit -m \"initial deployment of expresscheckin app onto OpenShift Express\"\n$ git push origin\n\n\nAfter pushing the app, it will take a few minutes for the app to become\navailable at http://expresscheckin-your-domain.rhcloud.com. That’s it, you’ve\ndeployed a simple Java app that uses MongoDB to OpenShift Express.\nRefer to Testing the Deployment below for some notes on testing\nthe app’s functionality.\n\n\nTesting the Deployment¶\nOnce you’ve deployed your app onto OpenShift Express, the URL for the\napp will be something of the form\nhttp://expresscheckin-your-domain.rhcloud.com. We’ll now use that URL\nto conduct some tests on our deployed app.\nSince the app is a simple RESTish mechanism we can use curl to test\nit out. Let’s start by posting a new comment and location to the URL\n(ex. http://appurl.rhcloud.com/checkin):\n$ curl -X POST -d \"comment=hello&x=1&y=1\" http://appurl.rhcloud.com/checkin\n\n\nNow let’s see if we can find the comment we just posted at that\nlocation (x = 1, y = 1):\n$ curl \"http://appurl.rhcloud.com/checkin?x=1&y=1\"\n{ \"_id\" : { \"$oid\" : \"4f0e068c3004bfd40822840b\"} , \"comment\" : \"hello\" , \"location\" : [ 1.0 ,\n1.0]}\n\n\nIf these worked, then it looks like we’ve got a functional app deployed\nonto OpenShift.\n\n\nAdvanced Functionality¶\nOnce you’ve deployed your app, you’ll probably need to connect to your\nserver at some point so you can do things like reviewing app logs or\nquery data in MongoDB. The following steps will cover how you can do\nthat (and if available, set up some advanced functionality).\n\n\nApp Administration¶\nUsing the command line tools, we can connect to the server our app is\ndeployed on. Run the following command to get information about the\ncurrent running apps:\n$ rhc domain show\nPassword:\n\nUser Info\n=========\nNamespace: checkins\n  RHLogin: sandeep@clusterbeep.org\n\n\nApplication Info\n================\nexpresscheckin\n    Framework: jbossas-7\n     Creation: 2012-04-14T14:04:54-04:00\n         UUID: 485daa768043454d9cdcb9343018eb6e\n      Git URL: ssh://485daa768043454d9cdcb9343018eb6e@expresscheckin-\ncheckins.rhcloud.com/~/git/expresscheckin.git/\n   Public URL: http://expresscheckin-checkins.rhcloud.com/\n\n Embedded:\n      mongodb-2.0 - Connection URL: mongodb://127.6.85.129:27017/\n\n\nThe UDID above shows the user ID we can use to SSH into our server:\n$ ssh 429827960cbf4518b1785ed928db9be7@expresscheckin-your-domain.rhcloud.com\n\nWelcome to OpenShift shell\n\nThis shell will assist you in managing openshift applications.\n\n!!! IMPORTANT !!! IMPORTANT !!! IMPORTANT !!!\nShell access is quite powerful and it is possible for you to\naccidentally damage your application.  Proceed with care!\nIf worse comes to worse, destroy your application with rhc-ctl-app\nand recreate it\n!!! IMPORTANT !!! IMPORTANT !!! IMPORTANT !!!\n\ntype \"help\" for more info.\n\n\nNow at the prompt type help to see the available commands (in\naddition to normal shell commands):\n[openshift]$ help\nHelp menu: The following commands are available to help control your openshift\napplication and environment.\n\nctl_app         control your application (start, stop, restart, etc)\nctl_all         control application and deps like mysql in one command\ntail_all        tail all log files\nexport          list available environment variables\nrm              remove files / directories\nls              list files / directories\nps              list running applications\nkill            kill running applications\nmongo           interactive MongoDB shell\n\n\nTo connect to MongoDB, we’ll need to use the credentials provided to us\nfrom OpenShift above when we set it up initially. Once you have those\nin hand, here’s how you should connect to the database:\n[openshift]$ mongo expresscheckin -u admin -p 1wbLGYAmPgDM\nMongoDB shell version: 2.0.2-rc1\nconnecting to: 127.1.13.1:27017/expresscheckin\n>\n\n\nFrom here you can query your data as needed for your application.\nFinally, to backup your app’s data, follow the instructions found on\nthe MongoDB Backup docs.\nSpecifically you’ll need to use the mongodump command to do a live backup of your data.\nBacking up your app is a simple single step where we create a\nsnapshot of the entirety of the application including data, which\nwe can do locally on our development machine:\n$ rhc app snapshot save -a expresscheckin\nPassword:\nPulling down a snapshot to expresscheckin.tar.gz\n\nRunning extra dump: mongodb_dump.sh\nMongoDB already running\nStopping application...\nDone\nCreating and sending tar.gz\nRunning extra cleanup: mongodb_cleanup.sh\nStarting application...\nDone\n\n\n\n\nAdditional Information¶\nFor additional information, refer to the following:\n\nOpenShift Express User Guide or\nOpenShift Flex User Guide\n\n\n\n\n                \n    \n      ←  \n      Rackspace Cloud\n      VMware Cloud Foundry\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Red Hat OpenShift","preview":"OpenShift is a Platform as a Service (PaaS) offering from RedHat, which\nprovides support for rapid deployment and automatic scalability support\nfor web applications developed with Java EE, Node.js, Py","tags":""},{"slug":"platforms/vmware-cloud-foundry/","headings":[],"text":"Platforms > \n          VMware Cloud Foundry \n      \n    \n  \n                \n                  \nVMware Cloud Foundry\n\nOn this page\n\nStart a MongoDB Service\nJava\nNode.js\nRuby\n\n\nMongoDB is a supported service on VMware’s\nCloud Foundry.\nCloud Foundry is a platform-as-a-service solution. At this time,\nCloudFoundry.com (the public instance of Cloud Foundry operated by\nVMware) supports applications written in Spring Java, Rails, and Sinatra\nfor Ruby, Node.js, Scala and other JVM languages/frameworks, including\nGroovy and Grails. See <http://www.cloudfoundry.com/faq#language>.\n\nStart a MongoDB Service¶\nvmc create-service mongodb --name MyMongoDB\n\n\nOnce you create a MongoDB service, you can bind and use it inside of\nCloud Foundry applications.\n\n\nJava¶\n\nJava MongoDB Driver\n\n\n\nNode.js¶\n\nCloudFoundry Node.JS MongoDB Documentation\nBlog post: Getting started with VMware Cloud Foundry, MongoDB and Node.js\nNode.js MongoDB Driver\n\n\n\nRuby¶\n\nCloudFoundry Ruby MongoDB Documentation\nBlog post: Getting started with VMware Cloud Foundry, MongoDB and Rails\nMongoDB Ruby Language Center\nUsing GridFS with Ruby and Cloud Foundry\n\n\n\n\n                \n    \n      ←  \n      Red Hat OpenShift\n      Microsoft Azure\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"VMware Cloud Foundry","preview":"On this page MongoDB is a supported service on VMware’s\nCloud Foundry. Cloud Foundry is a platform-as-a-service solution. At this time,\nCloudFoundry.com (the public instance of Cloud Foundry operated","tags":""},{"slug":"platforms/windows-azure/","headings":[],"text":"Platforms > \n          Microsoft Azure \n      \n    \n  \n                \n                  \nMicrosoft Azure\n\nOn this page\n\nDeployment Recommendations\nPresentations\n\n\nMongoDB on Azure brings the power of the leading next-generation\ndatabase to Microsoft’s flexible, open, and scalable cloud.\nMicrosoft Azure is an open and\nflexible cloud platform that enables you to quickly build, deploy, and\nmanage applications across a global network of Microsoft-managed\ndatacenters. You can build applications using any language, tool, or\nframework. You can also integrate your public cloud applications with\nyour existing IT environment.\nTogether, MongoDB and Azure provide customers the tools to build\nlimitlessly scalable applications in the cloud.\nUsers interested in deploying MongoDB directly on Azure can do so\nusing Azure VM (Azure Infrastructure-as-a-Service) on Linux or\nWindows, or Azure Worker Roles (Azure Platform-as-a-Service) on\nWindows.\n\nDeployment Recommendations¶\nSee Azure Production Notes for\nrecommendations when deploying MongoDB on the Azure platform.\n\n\nPresentations¶\nUsers deploying MongoDB on Azure may be interested in the following\npresentations:\n\nMongoDB Paris 2012: MongoDB on Azure\nMongoNYC 2012: MongoDB on Windows Azure\nHands-On: Deploying MongoDB on Azure\n\n\n\n\n                \n    \n      ←  \n      VMware Cloud Foundry\n      Windows Quick Links and Reference Center\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Microsoft Azure","preview":"On this page MongoDB on Azure brings the power of the leading next-generation\ndatabase to Microsoft’s flexible, open, and scalable cloud. Microsoft Azure is an open and\nflexible cloud platform that en","tags":""},{"slug":"platforms/windows/","headings":[],"text":"Platforms > \n          Windows Quick Links and Reference Center \n      \n    \n  \n                \n                  \nWindows Quick Links and Reference Center\n\nOn this page\n\nRunning MongoDB on Windows\nRunning as a Service\nWriting Apps\nBuilding\nWindows Azure\nAppHarbor\n\n\n\nRunning MongoDB on Windows¶\nSee the Install on Windows\ntutorial for more information on how to install and run the database for\nthe first time.\nFor the supported versions of Windows, see the Recommended\nOperating Systems for Production Deployments and Other Supported\nOperating Systems tables on Install MongoDB\npage.\n\n\nRunning as a Service¶\nSee the Windows Service page.\n\n\nWriting Apps¶\nYou can write apps in almost any programming language, see the\nDrivers page.\nIn particular C#, .NET, PHP, C and C++ work just fine.\n\n\nBuilding¶\nWe recommend using the pre-built binaries, but if you prefer to build\nfrom source, see https://github.com/mongodb/mongo-cxx-driver/wiki.\n\n\nWindows Azure¶\nFor information on running Windows on Azure, see Microsoft Azure.\n\n\nAppHarbor¶\n\nInstructions for running MongoDB on AppHarbor\nSample  ASP.NET MVC app that uses MongoDB from AppHarbor\n\n\n\n\n                \n    \n      ←  \n      Microsoft Azure\n      Use Cases\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Windows Quick Links and Reference Center","preview":"On this page See the Install on Windows\ntutorial for more information on how to install and run the database for\nthe first time. For the supported versions of Windows, see the Recommended\nOperating Sy","tags":""},{"slug":"platforms/","headings":[],"text":"Platforms\n\nAmazon Web Services EC2¶\n\n\nAmazon EC2\nEC2 Backup and Restore\n\n\n\n\nDigitalOcean¶\n\n\nDigitalOcean\n\n\n\n\ndotCloud¶\n\n\ndotCloud\n\n\n\n\nJoyent Cloud¶\n\n\nJoyent Cloud\n\n\n\n\nModulus¶\n\n\nModulus\n\n\n\n\nRackspace Cloud¶\n\n\nRackspace Cloud\n\n\n\n\nRed Hat OpenShift¶\n\n\nRed Hat OpenShift\n\n\n\n\nVMware Cloud Foundry¶\n\n\nVMware Cloud Foundry\n\n\n\n\nWindows Azure¶\n\n\nMicrosoft Azure\n\n\n\n\nTraditional Platforms¶\n\n\nWindows Quick Links and Reference Center\n\n\n\n\n\n                \n    \n      ←  \n      Wireshark Support for MongoDB Protocol\n      Amazon EC2\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Platforms","preview":"© MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.  \n  \n    Platforms","tags":""},{"slug":"search/","headings":[],"text":"Search\n    \n    \n    \n      Please activate JavaScript to enable the search\n      functionality.\n    \n    \n    \n      From here you can view your search results. Enter your search\n      words into the search box and hit \"Enter\". Note that the search\n      function will automatically search for all of the words. Pages\n      containing fewer words won't appear in the result list.\n    \n  \n  \n    \n      \n    \n  \n\n    \n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Search","preview":"Please activate JavaScript to enable the search\n      functionality.\n     \n      From here you can view your search results. Enter your search\n      words into the search box and hit \"Enter\". Note tha","tags":""},{"slug":"tools/hadoop/","headings":[],"text":"MongoDB Integration and Tools > \n          MongoDB Connector for Hadoop \n      \n    \n  \n                \n                  \nMongoDB Connector for Hadoop\n\nOn this page\n\nInstallation\nPresentations\nBlog Posts\nAdditional Resources\n\n\nThe MongoDB Connector for Hadoop is a plugin for Hadoop that provides\nthe ability to use MongoDB as an input source and/or an output destination.\nThe source code is available on Github where you can find a more\ncomprehensive wiki.\nIf you have questions please email the mongodb-user Mailing List. For any issues please\nfile a ticket in Jira.\nThis guide also includes the following documentation:\n\nGetting Started with Hadoop\nHadoop and MongoDB Use Cases\n\n\nInstallation¶\n\nObtain the Hadoop connector.  You may either download the JARs from the\nMaven Central Repository or build it\nyourself.\nThe JARs are universal and will work with any version of Hadoop.\nObtain a JAR for the MongoDB Java Driver.\nMove these JARs onto each node of the Hadoop cluster.\nYou may provision each node so that the jars are somewhere on Hadoop’s CLASSPATH\n(e.g. $HADOOP_PREFIX/share/hadoop/common), or you can use the Hadoop\nDistributedCache\nto move the JARs onto pre-existing nodes.\n\nFor more complete install instructions, please see the installation instructions\nfor your platform on the Hadoop Connector wiki.\n\n\nPresentations¶\n\nMongoDB and Hadoop: Driving Business Insights\nby Sandeep Parikh, 2014\nWebinar: Using MongoDB + Hadoop Together\nby Buzz Moschetti, 2014\n\n\n\nBlog Posts¶\n\nMongoDB Hadoop Connector 1.4 Improvements\nMongoDB Hadoop Connector Announcement\n\n\n\nAdditional Resources¶\n\nHadoop and MongoDB Integration Overview\nUsing MongoDB with Hadoop & Spark: Part 1\nUsing MongoDB with Hadoop & Spark: Part 2\nUsing MongoDB with Hadoop & Spark: Part 3\n\n\n\n\n                \n    \n      ←  \n      MongoDB Integration and Tools\n      Getting Started with Hadoop\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"MongoDB Connector for Hadoop","preview":"On this page The MongoDB Connector for Hadoop is a plugin for Hadoop that provides\nthe ability to use MongoDB as an input source and/or an output destination. The source code is available on Github wh","tags":""},{"slug":"tools/http-interfaces/","headings":[],"text":"MongoDB Integration and Tools > \n          HTTP Interface \n      \n    \n  \n                \n                  \nHTTP Interface\n\nOn this page\n\nREST Interfaces\nHTTP Interfaces\nHTTP Status Interface\nSimple REST API\nReplica Set Admin UI\nSee Also\nAdditional Resources\n\n\n\nREST Interfaces¶\n\nEve (Python)¶\nEve is an open source Python REST API framework designed for human beings. It\nallows to effortlessly build and deploy highly customizable, fully featured\nRESTful Web Services powered by MongoDB.\nThe codebase is thoroughly tested under Python 2.6, 2.7, 3.3, 3.4 and PyPy.\nAll you need to bring your REST Web Service online is a MongoDB database, a\nconfiguration file and a launch script.  Overall, you will find that configuring\nand fine-tuning your API is a very simple process.\n\nEmphasis on REST\nFull range of CRUD operations\nCustomizable resource endpoints\nCustomizable, multiple item endpoints\nFiltering and Sorting\nPagination\nHATEOAS\nJSON and XML Rendering\nConditional Requests\nData Integrity and Concurrency Control\nBulk Inserts\nData Validation\nExtensible Data Validation\nResource-level Cache Control\nAPI Versioning\nDocument Versioning\nAuthentication\n… and (a lot) more.\n\n\n\nRESTHeart (Java)¶\nRESTHeart Java REST API server for\nMongoDB, built on top of Undertow non-blocking\nHTTP server.  License: GNU AFFERO GENERAL PUBLIC LICENSE Version 3.\n\nCRUD operations API on your data;\nData model operations API: create databases, collections, indexes\nand the data structure;\nSuper easy setup, with convention over configuration approach;\nPluggable security with User Management and ACL;\nHAL hypermedia type;\nSuper lightweight: pipeline architecture, ~6Mb footprint,\n~200Mb RAM peek usage, starts in milliseconds;\nHigh throughput: very small overhead on MongoDB performance;\nHorizontally scalable: fully stateless architecture supporting\nMongoDB replica sets and shards;\nBuilt on top of Undertow non-blocking HTTP server;\nEmbeds the excellent HAL browser\nby Mike Kelly (the author of the HAL specifications);\nSupports Cross-Origin Resource Sharing (CORS) so that your one-page web\napplication can deal with RESTHeart running on a different\ndomain. In other words, CORS is an evolution of JSONP;\nIdeal as AngularJS (or any other MVW Javascript framework) back-end.\n\n\n\nDrowsyDromedary (Ruby)¶\nDrowsyDromedary\nis a REST layer for MongoDB based on Ruby.\n\n\nCrest (Node.jS)¶\nCrest is a REST API Server for\nMongoDB built in node.js using the\nMongoDB Node Native driver.\n\n\nAMID¶\nAMID is a REST\ninterface for MongoDB. Written in Node.js, supports multi-threading\nand a modular architecture to perform custom search operation.\nAMID also provides an optional extjs GUI for queries: AMIDGUI.\n\n\nKule¶\nKule is a customizable REST\ninterface for MongoDB based on Python.\n\n\nDreamFactory¶\nDreamFactory is an open source backend\nwith a REST API for MongoDB. DreamFactory on GitHub.\n\n\n\nHTTP Interfaces¶\n\nSleepy Mongoose (Python)¶\nSleepy Mongoose\nis a full featured HTTP interface for MongoDB.\n\n\n\nHTTP Status Interface¶\n\nDeprecated since version 3.2: Starting in 3.2, MongoDB deprecates the HTTP interface.\n\nMongoDB provides a simple HTTP interface listing information of interest\nto administrators. If you enable the interface with the --rest\noption to mongod, you may access it via a port that is\n1000 more than the configured mongod port. The default\nport for the HTTP interface is 28017. To access the HTTP interface an\nadministrator may, for example, point a browser to\nhttp://localhost:28017 if mongod is running with the default port on the\nlocal machine.\n\nHere is a description of the informational elements of the HTTP\ninterface:\n\n\n\n\n\n\nelement\ndescription\n\n\n\ndb version\ndatabase version information\n\ngit hash\ndatabase version developer tag\n\nsys info\nmongod compilation environment\n\ndblocked\nindicates whether the primary mongod mutex is held\n\nuptime\ntime since this mongod instance was started\n\nassertions\nany software assertions that have been raised by this mongod instance\n\nreplInfo\ninformation about replication configuration\n\ncurrentOp\nmost recent client request\n\n# databases\nnumber of databases that have been accessed by this mongod instance\n\ncurclient\nlast database accessed by this mongod instance\n\nCursors\ndescribes outstanding client cursors\n\nmaster\nwhether this mongod instance has been designated a master\n\nslave\nwhether this mongod instance has been designated a slave\n\ninitialSyncCompleted\nwhether this slave or repl pair node has completed an initial\nclone of the mongod instance it is replicating\n\nDBTOP\nDisplays the total time the mongod instance has devoted to each\nlisted collection, as well as the percentage of available time\ndevoted to each listed collection recently and the number of\nreads, writes, and total calls made recently\n\ndt\nTiming information about the primary mongod mutex\n\n\n\n\nHTTP Status Interface Security¶\n\nWarning\nEnsure that the HTTP status interface, the REST API, and\nthe JSON API are all disabled in production environments to prevent\npotential data exposure and vulnerability to attackers.\nSee Security and MongoDB API Interfaces\n\nIf security is configured for a mongod instance,\nauthentication is required for a client to access the HTTP status interface\nfrom another machine.\n\nNote\nWhile MongoDB Enterprise does support Kerberos authentication,\nKerberos is not supported by the HTTP status interface in any\nversion of MongoDB.\n\nChanged in version 3.0.\n\nNeither the HTTP status interface nor the REST API support the\nSCRAM-SHA-1 challenge-response\nuser authentication mechanism introduced in version 3.0.\n\n\n\n\nSimple REST API¶\nThe mongod process includes a simple REST API as a convenience. With no support for\ninsert, update, or remove operations, it is generally used\nfor monitoring, alert scripts, and administrative tasks. For full REST\ncapabilities, consider using an external REST Interface such as\nSleepy.Mongoose.\nv1.4+: This API is disabled by default, as it could provide unauthenticated access to data.\nUse --rest on the command line to enable, but be aware of security implications.\nTo get the contents of a collection (note the trailing slash):\nhttp://127.0.0.1:28017/databaseName/collectionName/\n\n\nTo add a limit:\nhttp://127.0.0.1:28017/databaseName/collectionName/?limit=-10\n\n\nTo skip:\nhttp://127.0.0.1:28017/databaseName/collectionName/?skip=5\n\n\nTo query for {a : 1}:\nhttp://127.0.0.1:28017/databaseName/collectionName/?filter_a=1\n\n\nSeparate conditions with an &:\nhttp://127.0.0.1:28017/databaseName/collectionName/?filter_a=1&limit=-10\n\n\nSame as db.$cmd.findOne({listDatabase:1}) on the admin database\nin the shell:\nhttp://localhost:28017/admin/$cmd/?filter_listDatabases=1&limit=1\n\n\nTo count documents in a collection:\nhttp://host:port/db/$cmd/?filter_count=collection&limit=1\n\n\n\nJSON in the simple REST API¶\nThe simple REST API uses strict JSON (as opposed to the shell,\nwhich uses Dates, regular expressions, etc.). To display non-JSON types,\nthe web interface wraps them in objects and uses the key for the type.\nFor example:\n# ObjectIds just become strings\n\"_id\" : \"4a8acf6e7fbadc242de5b4f3\"\n\n# dates\n\"date\" : { \"$date\" : 1250609897802 }\n\n# regular expressions\n\"match\" : { \"$regex\" : \"foo\", \"$options\" : \"ig\" }\n\n\nThe code type has not been implemented yet and causes the DB to crash if\nyou try to display it in the browser.\nSee Mongo Extended JSON\nfor details.\n\n\n\nReplica Set Admin UI¶\nThe mongod process includes a simple administrative UI for checking the\nstatus of a replica set.\nTo use, first enable --rest from the mongod command line. The rest port\nis the db port plus 1000 (thus, the default is 28017). Be sure this port\nis secure before enabling this.\nThen you can navigate to http://<hostname>:28017/ in your web\nbrowser. Once there, click Replica Set Status\n(/_replSet) to move to the Replica\nSet Status page.\n\n\n\nSee Also¶\n\nMonitoring Database Systems\nMongoDB Cloud Manager for seamless automation, backup, and monitoring.\n\n\n\nAdditional Resources¶\nBuilding your first app with MongoDB: Creating a REST API using the MEAN stack\n\n\n\n                \n    \n      ←  \n      Operational Procedures using Red Hat Enterprise Linux Identity Management\n      Munin Configuration Examples\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"HTTP Interface","preview":"On this page Eve is an open source Python REST API framework designed for human beings. It\nallows to effortlessly build and deploy highly customizable, fully featured\nRESTful Web Services powered by M","tags":""},{"slug":"tools/munin/","headings":[],"text":"MongoDB Integration and Tools > \n          Munin Configuration Examples \n      \n    \n  \n                \n                  \nMunin Configuration Examples\n\nOn this page\n\nRequirements\nInstall\nConfiguration\nMongoDB Munin Plugin\nCheck your setup\nAdvanced charting\nSee Also\n\n\nMunin can use be used for monitoring\naspects of a running system. This page describes how to set up and use\nthe MongoDB plugin with Munin.\nMunin is made up of two components:\n\nThe agent and plugins that are installed on the system you want to\nmonitor\nThe server, which polls the agents and creates the basic web pages and\ngraphs to visualize the data\n\n\nRequirements¶\nThe Munin plugin only supports Python 2.x. You cannot use Munin with\nPython 3.x.\nFor the MongoDB Munin Plugin requirements, see\nhttps://github.com/comerford/mongo-munin.\n\n\nInstall¶\nYou can download from SourceForge,\nbut prebuilt packages are also available. For example on Ubuntu you can\ninstall the agent and server as follows:\n\nTo install the agent, issue following on each node you want to\nmonitor.\nshell> sudo apt-get install munin-node\n\n\n\nTo install the server on Ubuntu, you must have Apache2 . Issue the\nfollowing commands:\nshell> apt-get install apache2\nshell> apt-get install munin\n\n\n\n\n\n\nConfiguration¶\nYou must configure the agents and server with the IP address and port\nneeded to contact each other. The following examples use these\naddresses:\n\ndb1 : 10.202.210.175\ndb2 : 10.203.22.38\nmunin-server : 10.194.102.70\n\n\nAgent Configuration¶\nOn each node, add an entry as shown in the following example\nconfiguration:\nFor db1:\n/etc/munin/munin-node.conf\nhost_name db1-ec2-174-129-52-161.compute-1.amazonaws.com\nallow ^10\\.194\\.102\\.70$\n\n\nFor db2:\n/etc/munin/munin-node.conf\nhost_name db2-ec2-174-129-52-161.compute-1.amazonaws.com\nallow ^10\\.194\\.102\\.70$\n\n\nIn the configuration:\n\nhost_name is used by the server and can be whatever you like.\nallow is the IP address of the server, enabling the server to poll\nthe agent.\n\n\n\nServer Configuration¶\nFor each node that is being monitored add an entry, as shown in the\nfollowing example configuration:\n[db1-ec2-174-129-52-161.compute-1.amazonaws.com]\naddress 10.202.210.175\nuse_node_name no\n\n[db2-ec2-184-72-191-169.compute-1.amazonaws.com]\naddress 10.203.22.38\nuse_node_name no\n\n\nIn the configuration:\n\nThe name in between the [] must match the name set in the agents\nmunin-node.conf.\naddress is the IP address of the node where the agent is running.\nuse_node_name determines whether the name between [] (no) or\nthe name reported by the node (yes) is used to fetch node data.\n\n\n\n\nMongoDB Munin Plugin¶\nA plugin is available that\nprovide metrics for:\n\nB-Tree stats\nCurrent connections\nMemory usage\nDatabase operations (inserts, updates, queries etc.)\n\nThe plugin can be installed on each node where MongoDB. For\nrequirements and instructions to installing the MongoDB Munin Plugin,\nsee https://github.com/comerford/mongo-munin.\n\n\nCheck your setup¶\nAfter installing the plugin and making the configuration changes, force\nthe server to update the information to check that your setup is correct\nusing the following:\nshell> sudo -u munin /usr/share/munin/munin-update\n\n\nIf everything is set up correctly, you will get a chart like this:\n\n\n\n\n\nAdvanced charting¶\n\nAggregate Values Across All Nodes¶\nIf you are running a large MongoDB cluster, you may want to aggregate\nthe values (e.g. inserts per second) across all the nodes in the\ncluster. Munin provides a simple way to aggregate. The following example\ndefines a new segment called CLUSTER:\n/etc/munin/munin.conf\n[compute-1.amazonaws.com;CLUSTER]\nupdate no\n\n\nIn the example:\n\nupdate no means Munin can generate the chart based on existing\ndata. This tells Munin not to poll the agents for the data\n\n\n\nAggregate Inserts, Updates, and Deletes¶\nThe following example defines a chart to aggregate the inserts, updates,\nand deletes for the cluster:\ncluster_ops.graph_title Cluster Ops\ncluster_ops.graph_category mongodb\ncluster_ops.graph_total total\ncluster_ops.total.graph no\ncluster_ops.graph_order insert update delete\ncluster_ops.insert.label insert\ncluster_ops.insert.sum \\\n  db1-ec2-174-129-52-161.compute-1.amazonaws.com:mongo_ops.insert \\\n  db2-ec2-184-72-191-169.compute-1.amazonaws.com:mongo_ops.insert\ncluster_ops.update.label update\ncluster_ops.update.sum \\\n  db1-ec2-174-129-52-161.compute-1.amazonaws.com:mongo_ops.update \\\n  db2-ec2-184-72-191-169.compute-1.amazonaws.com:mongo_ops.update\ncluster_ops.delete.label delete\ncluster_ops.delete.sum \\\n  db1-ec2-174-129-52-161.compute-1.amazonaws.com:mongo_ops.delete \\\n  db2-ec2-184-72-191-169.compute-1.amazonaws.com:mongo_ops.delete\n\n\nIn the example:\n\ncluster_ops: name of this chart.\n\ncluster_ops.graph_category mongodb: puts this chart into the\nmongodb category. Allows you to collect similar charts on a single\npage.\n\ncluster_ops.graph_order insert update delete: indicates the order\nof the lines on the key for the chart.\n\ncluster_ops.insert: represents a single line on the chart, in this\ncase the insert.\n\ncluster_ops.insert.sum: indicates the values are summed.\n\ndb1-ec2-174-129-52-161.compute-1.amazonaws.com: indicates the node\nto aggregate.\n\nmongo_ops.insert: indicates the chart (mongo_ops) and the counter\n(insert) to aggregate.\nAnd this is what it looks like\n\n\n\n\n\n\nSee Also¶\n\nMonitoring Database Systems\nMongoDB Cloud Manager for seamless automation, backup, and monitoring.\n\n\n\n\n                \n    \n      ←  \n      HTTP Interface\n      Wireshark Support for MongoDB Protocol\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Munin Configuration Examples","preview":"On this page Munin can use be used for monitoring\naspects of a running system. This page describes how to set up and use\nthe MongoDB plugin with Munin. Munin is made up of two components: The Munin pl","tags":""},{"slug":"tools/wireshark/","headings":[],"text":"MongoDB Integration and Tools > \n          Wireshark Support for MongoDB Protocol \n      \n    \n  \n                \n                  \nWireshark Support for MongoDB Protocol\n\nOn this page\n\nMongoDB Port\nExample\n\n\nWireshark, an advanced interactive network\ntraffic sniffer, has full support\nfor the MongoDB Wire protocol.\nYou can visually inspect MongoDB traffic, do complex filters on specific\nvalues of MongoDB wire messages, and dig into individual documents both\nsent and received.\n\nMongoDB Port¶\nThe Mongo protocol definition included in Wireshark assumes\nthat the traffic occurs on the default MongoDB TCP port 27017. To\nverify or update the TCP port of the MongoDB server (or client), go to\nPreferences -> Protocols -> Mongo.\nTo verify the port using the tshark command line interface instead\nof the GUI, use the following:\ntshark -G currentprefs | grep mongo\n\n\n\nNote\nIf the port value preference does not match the port on which your\nMongoDB runs, no mongo wire protocol data will be captured and all\nfilter expressions will have an empty result.\n\n\n\nExample¶\n\n\n\n\n\n\n                \n    \n      ←  \n      Munin Configuration Examples\n      Platforms\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Wireshark Support for MongoDB Protocol","preview":"On this page Wireshark, an advanced interactive network\ntraffic sniffer, has full support\nfor the MongoDB Wire protocol. You can visually inspect MongoDB traffic, do complex filters on specific\nvalues","tags":""},{"slug":"tools/","headings":[],"text":"MongoDB Integration and Tools\n\nIntegration Frameworks¶\n\n\nMongoDB Connector for Hadoop\nGetting Started with Hadoop\nHadoop and MongoDB Use Cases\nConfigure Red Hat Enterprise Linux Identity Management with MongoDB\nOperational Procedures using Red Hat Enterprise Linux Identity Management\n\n\n\n\nAdministrative Tools¶\n\n\nHTTP Interface\nMunin Configuration Examples\nWireshark Support for MongoDB Protocol\n\n\n\nRelated\nFor seamless automation, backup, and monitoring, try MongoDB Cloud Manager: the\neasiest way to run MongoDB in the cloud. Try it free for 30 days.  See also MongoDB Cloud Manager documentation.\n\n\n\n\n                \n    \n      ←  \n      Driver Compatibility\n      MongoDB Connector for Hadoop\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"MongoDB Integration and Tools","preview":"Related For seamless automation, backup, and monitoring, try MongoDB Cloud Manager: the\neasiest way to run MongoDB in the cloud. Try it free for 30 days.  See also MongoDB Cloud Manager documentation.","tags":""},{"slug":"tutorial/authenticate-with-csharp-driver/","headings":[],"text":"Authenticate to MongoDB with the C# Driver\nThis page is a brief overview of authenticating to a MongoDB cluster\nwith the MongoDB C# Driver using version 1.8 and above.\n\nInternal Authentication¶\nInternal authentication refers to accounts stored inside MongoDB.\nCurrently, the only way to authenticate against internal accounts\nis to use the MongoDB Challenge Response mechanism, or MONGODB-CR.\nThis is the default mechanism.  To authenticate as the user “user1”\nwith a password of “password1”, defined in the “test” database:\nvar credential = MongoCredential.CreateMongoCRCredential(\"test\", \"user1\", \"password1\");\n\nvar settings = new MongoClientSettings\n{\n    Credentials = new[] { credential }\n};\n\nvar mongoClient = new MongoClient(settings);\n\n\nAlternatively, when you only need a single user, it is possible to\nspecify this in the connection string.\nvar connectionString = \"mongodb://user1:password1@localhost/test\";\n\nvar mongoClient = new MongoClient(connectionString);\n\n\n\nNote\nIf you do not specify a database in the the connection string, the\ndefault database is the “admin” database.\n\nIn some cases you may need to authenticate as multiple users in\ndifferent databases. For example, imagine a map/reduce job that reads\nfrom the database “first” and writes the results to the database\n“second”.  You may need to authenticate one user defined in the\n“first” database and another in the “second”:\nvar credentialFirst = MongoCredential.CreateMongoCRCredential(\"first\", \"user1\", \"password1\");\nvar credentialSecond = MongoCredential.CreateMongoCRCredential(\"second\", \"user2\", \"password2\");\n\nvar settings = new MongoClientSettings\n{\n     Credentials = new[] { credentialFirst, credentialSecond }\n};\n\nvar mongoClient = new MongoClient(settings);\n\n\n\nNote\nAs of the 2.4 MongoDB release, this is no longer necessary, since you\nare able to define a user in one database and delegate privileges\nfor that user in another database.\n\n\n\nExternal Authentication¶\nExternal authentication refers to credentials validated outside of MongoDB.\nThe external authentication provider currently used is Kerberos.\nTo authenticate to a MongoDB cluster using Kerberos, you must specify\nthe GSSAPI mechanism and a user name. On Windows, it is also possible\nto provide the password.\nFundamentally, the process for connecting with Kerberos is the same\nfor Windows and Linux systems; however, Linux systems require the use\nof kinit to acquire the security credentials whereas Windows\nsystems perform this action transparently using SSPI based on the account\nrunning the current process when a password is not used.\nThe below examples work on both Windows and Linux systems.\nvar connectionString = \"mongodb://user%40REALM.COM@localhost/?authMechanism=GSSAPI\";\n\nvar client = new MongoClient(connectionString);\n\n\n\nNote\n%40 is the url encoded representation of the ‘@’ character.\n\nAlternatively, you can specify the authentication mechanism in the\ncode, as in both of the following:\nvar credential = MongoCredential.CreateGssapiCredential(\"user@REALM.COM\");\n\nvar settings = new MongoClientSettings\n{\n    Credentials = new[] { credential }\n};\n\nvar mongoClient = new MongoClient(settings);\n\n\n\nLinux Systems¶\nLinux does not integrate Kerberos authentication in the operating\nsystem (unlike Windows) and requires the use of an external library.\nThe MongoDB server and the .NET driver on Linux use the libgsasl\nto manage authentication.\nMono, the open source platform used to run the .NET driver on Linux,\nprovides a way to map the hard-coded names of native libraries to\ntheir actual names, which can be different based on your\ndistribution. Consider the documentation from the Mono project for this process.\nFor the .NET driver, you must map the libgsasl-7.dll\nlibrary.  An example configuration for Ubuntu looks like this:\n<configuration>\n  <dllmap dll=\"libgsasl-7.dll\" target=\"libgsasl.so.7\" />\n</configuration>\n\n\nName this file MongoDB.Driver.dll.config and save it in the same\ndirectory as the MongoDB.Driver.dll assembly. After mapping this\nlibrary, always run kinit before or during your application\nstartup process.\n\n\n\n\n                \n    \n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Authenticate to MongoDB with the C# Driver","preview":"This page is a brief overview of authenticating to a MongoDB cluster\nwith the MongoDB C# Driver using version 1.8 and above. Internal authentication refers to accounts stored inside MongoDB.\nCurrently","tags":""},{"slug":"tutorial/backup-and-restore-mongodb-on-amazon-ec2/","headings":[],"text":"Platforms > \n          EC2 Backup and Restore \n      \n    \n  \n                \n                  \nEC2 Backup and Restore\n\nOn this page\n\nBackup with --journal\nBackup without --journal\nFlush and Lock the Database\nBackup the Database Files\nUnlock the Database\nVerify the Backup\nRestore\nSee Also\n\n\nThis page describes how to backup, verify, and restore a MongoDB running\non EC2 using EBS Snapshots.\n\nSee also\nCloud Manager\norchestrates critical operational tasks you currently perform\nmanually across the servers in your MongoDB deployment. Download a\nfree trial of Cloud Manager to\nautomatically provision your EC2 instances.\n\nHow you backup MongoDB will depend on whether you are using the\n--journal option, which is available in versions 1.8 and above.\n\nBackup with --journal¶\nThe journal file allows for roll forward recovery. The journal files are\nlocated in the dbpath directory so will be snapshotted at the same time\nas the database files.\nIf the dbpath is mapped to a single EBS volume then proceed to\nBackup the Database Files.\nIf the dbpath is mapped to multiple EBS volumes, then in order to guarantee\nthe stability of the filesystem you will need to Flush and Lock the Database.\n\nNote\nSnapshotting with the journal is only possible if the journal\nresides on the same volume as the data files, so that one snapshot\noperation captures the journal state and data file state atomically.\n\n\n\nBackup without --journal¶\nIn order to correctly backup a MongoDB, you need to ensure that writes\nare suspended to the filesystem before you backup the filesystem. If\nwrites are not suspended then the backup may contain partially written\nor data which may fail to restore correctly.\nDepending on your version of MongoDB, use\nfsync and lock\nor, after MongoDB 2.0, use\ndb.fsyncLock().\nIf the filesystem is being used only by the database, then you can\nuse the snapshot facility of EBS volumes to create a backup. If you are\nusing the volume for any other application then you will need to ensure\nthat the filesystem is frozen as well (e.g. on XFS filesystem use\nxfs_freeze) before you initiate the EBS snapshot.\nThe overall process looks like:\n\n\n\nFlush and Lock the Database¶\nWrites have to be suspended to the filesystem in order to make a stable\ncopy of the database files.\nPrior to MongoDB version 2.0, this is achieved through the MongoDB\nshell using\nfsync and lock:\nmongo shell> use admin\nmongo shell> db.runCommand({fsync:1,lock:1});\n{\n   \"info\" : \"now locked against writes, use db.$cmd.sys.unlock.findOne() to unlock\",\n   \"ok\" : 1\n}\n\n\nMongoDB 2.0 added the\ndb.fsyncLock()\nmethod to lock the database and flush writes to disk and added the\ndb.fsyncUnlock()\nmethod to unlock the database after the snapshot has completed.\nDuring the time the database is locked, any write requests that this\ndatabase receives will be rejected. Any application code will need to\ndeal with these errors appropriately.\n\n\nBackup the Database Files¶\nThere are several ways to create an EBS Snapshot. The following examples use the\nAWS command line\ntool.\n\nFind the EBS Volumes Associated with MongoDB¶\nIf the mapping of EBS Block devices to the MongoDB data volumes is\nalready known, then this step can be skipped. The example below shows\nhow to determine the mapping for an LVM volume, please confirm with your\nSystem Administrator how the original system was setup if you are\nunclear.\n\n\nFind the EBS Block Devices Associated with the Running Instance¶\nshell> ec2-describe-instances\nRESERVATION   r-eb09aa81  289727918005    tokyo,default\nINSTANCE      i-78803e15  ami-4b4ba522    ec2-50-16-30-250.compute-1.amazonaws.com    ip-10-204-215-62.ec2.internal    running    scaleout    0    m1.large    2010-11-04T02:15:34+0000    us-east-1a    aki-0b4aa462    monitoring-disabled    50.16.30.250    10.204.215.62    ebs    paravirtual\nBLOCKDEVICE   /dev/sda1   vol-6ce9f105    2010-11-04T02:15:43.000Z\nBLOCKDEVICE   /dev/sdf    vol-96e8f0ff    2010-11-04T02:15:43.000Z\nBLOCKDEVICE    /dev/sdh   vol-90e8f0f9    2010-11-04T02:15:43.000Z\nBLOCKDEVICE   /dev/sdg    vol-68e9f101    2010-11-04T02:15:43.000Z\nBLOCKDEVICE   /dev/sdi    vol-94e8f0fd    2010-11-04T02:15:43.000Z\n\n\nAs can be seen in this example, there are a number of block devices\nassociated with this instance. You must determine which volumes make\nup the filesystem to snapshot.\n\n\nDetermine how the dbpath is Mapped to the File System¶\nLog onto the running MongoDB instance in EC2. To determine where the\ndatabase file are located, either look at the startup parameters for the\nmongod process or if mongod is running, then you\ncan examine the running process. In the following example, dbpath is\nset to /var/lib/mongodb/tokyo0.\nroot> ps -ef | grep mongo\nubuntu 10542 1 0 02:17 ? 00:00:00 /var/opt/mongodb/current/bin/mongod --port 27000 --shardsvr --dbpath /var/lib/mongodb/tokyo0 --fork --logpath /var/opt/mongodb/log/server.log --logappend --rest\n\n\n\n\nMap the dbpath to the Physical Devices¶\nUsing the df command, determine what the --dbpath directory is mapped to\nroot> df /var/lib/mongodb/tokyo0\nFilesystem           1K-blocks      Used Available Use% Mounted on\n/dev/mapper/data_vg-data_vol\n                     104802308      4320 104797988   1% /var/lib/mongodb\n\n\nNext determine the logical volume associated with this device. In the\nexample above, this is /dev/mapper/data_vg-data_vol.\nroot> lvdisplay /dev/mapper/data_vg-data_vol\n  --- Logical volume ---\n  LV Name                /dev/data_vg/data_vol\n  VG Name                data_vg\n  LV UUID                fixOyX-6Aiw-PnBA-i2bp-ovUc-u9uu-TGvjxl\n  LV Write Access        read/write\n  LV Status              available\n  # open                 1\n  LV Size                100.00 GiB\n...\n\n\nThis output indicates the volume group associated with this logical\nvolume, in this example data_vg. Next determine how this maps to the\nphysical volume.\nroot> pvscan\n  PV /dev/md0   VG data_vg   lvm2 [100.00 GiB / 0    free]\n  Total: 1 [100.00 GiB] / in use: 1 [100.00 GiB] / in no VG: 0 [0   ]\n\n\nFrom the physical volume, determine the associated physical devices, in\nthis example /dev/md0.\nroot> mdadm --detail /dev/md0\n/dev/md0:\n        Version : 00.90\n  Creation Time : Thu Nov  4 02:17:11 2010\n     Raid Level : raid10\n     Array Size : 104857472 (100.00 GiB 107.37 GB)\n  Used Dev Size : 52428736 (50.00 GiB 53.69 GB)\n   Raid Devices : 4\n...\n           UUID : 07552c4d:6c11c875:e5a1de64:a9c2f2fc (local to host ip-10-204-215-62)\n         Events : 0.19\n\n    Number   Major   Minor   RaidDevice State\n       0       8       80        0      active sync   /dev/sdf\n       1       8       96        1      active sync   /dev/sdg\n       2       8      112        2      active sync   /dev/sdh\n       3       8      128        3      active sync   /dev/sdi\n\n\nThe block devices /dev/sdf through /dev/sdi make up this\nphysical devices. Each of these volumes will need to be snapped in order\nto complete the backup of the filesystem.\n\n\nCreate the EBS Snapshot¶\nCreate the snapshot for each device. Using the\nec2-create-snapshot\ncommand, use the Volume Id for the device listed by the\nec2-describe-instances\ncommand.\nshell> ec2-create-snapshot -d backup-20101103 vol-96e8f0ff\nSNAPSHOT     snap-417af82b     vol-96e8f0ff     pending     2010-11-04T05:57:29+0000     289727918005     50     backup-20101103\nshell> ec2-create-snapshot -d backup-20101103 vol-90e8f0f9\nSNAPSHOT     snap-5b7af831     vol-90e8f0f9     pending     2010-11-04T05:57:35+0000     289727918005     50     backup-20101103\nshell> ec2-create-snapshot -d backup-20101103 vol-68e9f101\nSNAPSHOT     snap-577af83d     vol-68e9f101     pending     2010-11-04T05:57:42+0000     289727918005     50     backup-20101103\nshell> ec2-create-snapshot -d backup-20101103 vol-94e8f0fd\nSNAPSHOT     snap-2d7af847     vol-94e8f0fd     pending     2010-11-04T05:57:49+0000     289727918005     50     backup-20101103\n\n\n\n\n\nUnlock the Database¶\nOnce the snapshots are in pending state, the\ndatabase can be unlocked. Use the following operation to unlock the database.\nmongo shell> db.$cmd.sys.unlock.findOne();\n{ \"ok\" : 1, \"info\" : \"unlock requested\" }\n\n\nWhen unlocked, the database is available to process write requests.\n\n\nVerify the Backup¶\nIn order to verify the backup, you must do the following:\n\nCheck the status of each snapshot to ensure it is “completed.”\nCreate new volumes based on the snapshots and mount the new volumes.\nRun mongod and verify the collections.\n\n\nTypically, the verification is performed on another machine so that\nyou do not burden your production systems with the additional CPU and\nI/O load of the verification processing.\n\nDescribe the Snapshots¶\nUsing the ec2-describe-snapshots\ncommand, find the snapshots that make up the backup. Using a filter on\nthe description field, snapshots associated with the given backup are\neasily found. The search text used should match the text used in the -d\nflag passed to ec2-create-snapshot command when the backup was made.\nbackup shell> ec2-describe-snapshots --filter \"description=backup-20101103\"\nSNAPSHOT    snap-2d7af847    vol-94e8f0fd    completed    2010-11-04T05:57:49+0000    100%    289727918005    50    backup-20101103\nSNAPSHOT    snap-417af82b    vol-96e8f0ff    completed    2010-11-04T05:57:29+0000    100%    289727918005    50    backup-20101103\nSNAPSHOT    snap-577af83d    vol-68e9f101    completed    2010-11-04T05:57:42+0000    100%    289727918005    50    backup-20101103\nSNAPSHOT    snap-5b7af831    vol-90e8f0f9    completed    2010-11-04T05:57:35+0000    100%    289727918005    50    backup-20101103\n\n\n\n\nCreate New Volumes Based on the Snapshots¶\nUsing the ec2-create-volume\ncommand, create a new volumes based on each of the snapshots that make\nup the backup.\nbackup shell> ec2-create-volume --availability-zone us-east-1a --snapshot snap-2d7af847\nVOLUME    vol-06aab26f    50    snap-2d7af847    us-east-1a    creating    2010-11-04T06:44:27+0000\nbackup shell> ec2-create-volume --availability-zone us-east-1a --snapshot snap-417af82b\nVOLUME    vol-1caab275    50    snap-417af82b    us-east-1a    creating    2010-11-04T06:44:38+0000\nbackup shell> ec2-create-volume --availability-zone us-east-1a --snapshot snap-577af83d\nVOLUME    vol-12aab27b    50    snap-577af83d    us-east-1a    creating    2010-11-04T06:44:52+0000\nbackup shell> ec2-create-volume --availability-zone us-east-1a --snapshot snap-5b7af831\nVOLUME    vol-caaab2a3    50    snap-5b7af831    us-east-1a    creating    2010-11-04T06:45:18+0000\n\n\n\n\nAttach the New Volumes to the Instance¶\nUsing the ec2-attach-volume\ncommand, attach each volume to the instance where the backup will be\nverified.\nbackup shell> ec2-attach-volume --instance i-cad26ba7 --device /dev/sdp vol-06aab26f\nATTACHMENT     vol-06aab26f     i-cad26ba7     /dev/sdp     attaching     2010-11-04T06:49:32+0000\nbackup shell> ec2-attach-volume --instance i-cad26ba7 --device /dev/sdq vol-1caab275\nATTACHMENT     vol-1caab275     i-cad26ba7     /dev/sdq     attaching     2010-11-04T06:49:58+0000\nbackup shell> ec2-attach-volume --instance i-cad26ba7 --device /dev/sdr vol-12aab27b\nATTACHMENT     vol-12aab27b     i-cad26ba7     /dev/sdr     attaching     2010-11-04T06:50:13+0000\nbackup shell> ec2-attach-volume --instance i-cad26ba7 --device /dev/sds vol-caaab2a3\nATTACHMENT     vol-caaab2a3     i-cad26ba7     /dev/sds     attaching     2010-11-04T06:50:25+0000\n\n\n\n\nMount the Volumes¶\nFirst make the filesystem visible to the host operating system. This\nprocess will vary based on the particular volume management scheme\nthat you are using.  Check with your system administrator if you are\nunsure.\nIf you are using Logical Volume Manager, assemble the logical\ndevice from the physical devices. The UUID for the device will be the\nsame as the original UUID from which the backup was made, and can be\nobtained using the mdadm command.\nbackup shell> mdadm --assemble --auto-update-homehost -u\n07552c4d:6c11c875:e5a1de64:a9c2f2fc --no-degraded /dev/md0\nmdadm: /dev/md0 has been started with 4 drives.\n\n\nYou can confirm that the physical volumes and volume groups appear\ncorrectly to the O/S by executing the following:\nbackup shell> pvscan\n  PV /dev/md0 VG data_vg lvm2 [100.00 GiB / 0 free]\n  Total: 1 [100.00 GiB] / in use: 1 [100.00 GiB] / in no VG: 0 [0 ]\n\nbackup shell> vgscan\n  Reading all physical volumes. This may take a while...\n  Found volume group \"data_vg\" using metadata type lvm2\n\n\nNext, create the mount point and mount the filesystem:\nbackup shell> mkdir -p /var/lib/mongodb\n\nbackup shell> cat >> /etc/fstab << EOF\n/dev/mapper/data_vg-data_vol /var/lib/mongodb xfs noatime,noexec,nodiratime 0 0\nEOF\n\nbackup shell> mount /var/lib/mongodb\n\n\n\n\nStart the Database¶\nAfter the filesystem has been mounted, MongoDB can be started. Ensure\nthat the owner of the files is set to the correct user and group. Since\nthe backup was made with the database running, the lock file will need\nto be removed in order to start the database.\nbackup shell> chown -R mongodb /var/lib/mongodb/toyko0\nbackup shell> rm /var/lib/mongodb/tokyo0/mongod.lock\nbackup shell> mongod --dbpath /var/lib/mongodb/tokyo0\n\n\n\n\nVerify the Collections¶\nValidate\neach collection to ensure it does not contain any invalid BSON objects.\nmongo shell> db.blogs.validate({full:true})\n\n\n\n\n\nRestore¶\nRestore uses the same basic steps as the verification process:\n\ndb.shutdownServer()\n\nec2-create-volume\nand\nec2-attach-volume.\n\nMount the file system.\n\nRun mongod.\n\n\n\nAfter the filesystem is mounted you can decide to:\n\nCopy the database files from the backup into the current database\ndirectory\nStart mongod from the new mount point, specifying the new\nmount point in the --dbpath argument.\n\nAfter the database is started, it will be ready to transact. It will be\nat the specific point in time from the backup, so if it is part of a\nmaster/slave or replica set relationship, then the instance will need to\nsynchronize itself to get itself back up to date.\n\n\nSee Also¶\nMongoDB Cloud Manager for seamless automation, backup, and monitoring.\n\n\n\n                \n    \n      ←  \n      Amazon EC2\n      DigitalOcean\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"EC2 Backup and Restore","preview":"On this page This page describes how to backup, verify, and restore a MongoDB running\non EC2 using EBS Snapshots. See also Cloud Manager\norchestrates critical operational tasks you currently perform\nm","tags":""},{"slug":"tutorial/configure-red-hat-enterprise-linux-identity-management/","headings":[],"text":"MongoDB Integration and Tools > \n          Configure Red Hat Enterprise Linux Identity Management with MongoDB \n      \n    \n  \n                \n                  \nConfigure Red Hat Enterprise Linux Identity Management with MongoDB\n\nOn this page\n\nOverview\nRequirements\nSetup Procedure\nConfigure Authentication Type\nNext Steps\n\n\n\nOverview¶\nRed Hat Enterprise Linux (RHEL) 6.4\nand up includes a complete\nIdentity Management solution\ncalled Red Hat Enterprise Linux IdM. IdM integrates Kerberos authentication, directory\nservices, certificate management, DNS, and NTP into a single service.\nMongoDB Enterprise can leverage RHEL IdM’s Kerberos authentication and\ncertificate management infrastructure to generate and maintain the SSL\ncertificates required to encrypt data in motion. The following guide\nprovides instructions for integrating MongoDB Enterprise with RHEL IdM.\n\n\nRequirements¶\n\nRed Hat Enterprise Linux instance with IdM server deployed (see\nInstalling IdM Server for\nmore information)\nMongoDB Enterprise 2.4\n\n\n\nSetup Procedure¶\n\nPreparing a New MongoDB Server for Installation¶\nYou will need to install MongoDB and the RHEL IdM client pacakages on\nyour RHEL IdM client instance.\n\nInstall MongoDB Enterprise. MongoDB Enterprise has additional\nrequirements: you should first download and install those packages,\nand then download the archive for MongoDB Enterprise. The following\ncommands download the prerequisite packages and then download and\nuntar the MongoDB Enterprise package.\nyum install openssl net-snmp net-snmp-libs net-snmp-utils cyrus-sasl cyrus-sasl-lib cyrus-sasl-devel cyrus-sasl-gssapi\n\ncurl http://downloads.mongodb.com/linux/mongodb-linux-x86_64-subscription-rhel62-2.4.4.tgz > mongodb.tgz\ntar -zxvf mongodb.tgz\ncp -R -n mongodb-linux-x86_64-subscription-rhel62-2.4.4/ mongodb\n\n\nFor a more detailed MongoDB installation tutorial, or to install an\nolder version of MongoDB Enterprise (pre 2.4.x), see\nInstalling MongoDB Enterprise.\n\nNext install the IdM client and tools:\nyum install ipa-client ipa-admintools\n\n\n\n\n\n\nConfigure Hostname and DNS Resolution¶\nFor RHEL IdM’s services (e.g. Kerberos) to function properly, you must\nset a hostname on the system and update its\nDNS server to point to the RHEL IdM server instance IP address. You can\nupdate the hostname using commands that resemble the following:\n$ nano /etc/sysconfig/network\nHOSTNAME=idmclient.example.com\n$ service network restart\n$ hostname -f idmclient.example.com\n\n\nNext, update the DNS settings to point to the IP address of the RHEL\nIdM server:\n$ nano /etc/resolv.conf\nsearch example.com\nnameserver 10.192.206.229\n\n\n\n\nInstall the IdM Client¶\n\nNote\nTo run the IdM client installation you will need to have an\nadminstrative user (e.g. admin@EXAMPLE.COM) complete the enrollment\nprocess.\n\nHaving installed MongoDB Enterprise and correctly configured your\nHostname and DNS Resolution, you can install the RHEL IdM client. The\ninstallation process provides prompts for guidance.\nIf the IdM server was properly configured and the DNS information in\nthe previous step is correct, the process should be able to auto-detect\nthe required information.\nIf the auto-detection fails, consult the IdM client documentation.\nIf your installation is successful, the client installation process\nshould return something that resembles the following:\n$ ipa-client-install --enable-dns-updates\nDiscovery was successful!\nHostname: idmclient.example.com\nRealm: EXAMPLE.COM\nDNS Domain: example.com\nIPA Server: idmhost.example.com\nBaseDN: dc=example,dc=com\n\n\nA prompt will ask if you wish to configure the system using the\nauto-detected values. You should answer yes, as in the following:\nContinue to configure the system with these values? [no]: yes\nUser authorized to enroll computers: admin@EXAMPLE.COM\nSynchronizing time with KDC...\nPassword for admin@EXAMPLE.COM:\nEnrolled in IPA realm EXAMPLE.COM\nCreated /etc/ipa/default.conf\nNew SSSD config will be created\nConfigured /etc/sssd/sssd.conf\nConfigured /etc/krb5.conf for IPA realm EXAMPLE.COM\ntrying https://idmhost.example.com/ipa/xml\nHostname (idmclient.example.com) not found in DNS\nDNS server record set to: idmclient.example.com -> 10.190.178.79\nAdding SSH public key from /etc/ssh/ssh_host_dsa_key.pub\nAdding SSH public key from /etc/ssh/ssh_host_rsa_key.pub\nForwarding 'host_mod' to server u'https://idmhost.example.com/ipa/xml'\nSSSD enabled\nConfigured /etc/openldap/ldap.conf\nNTP enabled\nConfigured /etc/ssh/ssh_config\nConfigured /etc/ssh/sshd_config\nClient configuration complete.\n\n\n\n\nAdd Reverse DNS Lookup for the Server¶\nAfter installing the IdM client, you must authenticate with Kerberos\nas an administrative user (e.g. admin@EXAMPLE.COM) using the following\ncommand:\n$ kinit admin\n\n\nThis enables you to access the ipa command-line interface.\nYou will also need to add a reverse DNS entry to the IdM server.\n[SOMETHING] uses the reverse DNS entry to ensure that Kerberos\nauthentication works properly.\nTo add a reverse DNS entry, you must first add a reverse DNS zone,\nand then add a PTR record pointing to the IdM client host.\n\nExample\nThis example uses the following sample information:\n\nIdM server hostname: idmhost.example.com\nIdM client hostname: idmclient.example.com\nIdM client IP address: 1.2.3.4\nReverse DNS name: 3.2.1.in-addr.arpa. (reversed first three octets of the IP address)\nDNS record: 4 (last octet of the IP address)\n\nThe first command adds a reverse DNS zone.\n$ ipa dnszone-add 178.190.10.in-addr.arpa. --name-server idmhost.example.com. --force\n\n\nThe command will return output that resembles the following:\nZone name: 3.2.1.in-addr.arpa.\nAuthoritative nameserver: idmhost.example.com.\nAdministrator e-mail address: hostmaster.3.2.1.in-addr.arpa.\nSOA serial: 1372133625\nSOA refresh: 3600\nSOA retry: 900\nSOA expire: 1209600\nSOA minimum: 3600\nBIND update policy: grant EXAMPLE.COM krb5-subdomain 3.2.1.in-addr.arpa. PTR;\nActive zone: TRUE\nDynamic update: FALSE\nAllow query: any;\nAllow transfer: none;\n\n\nNext, you will add a PTR record pointing to the IdM client host,\nusing the following command:\n$ ipa dnsrecord-add 3.2.1.in-addr.arpa. 79 --ptr-hostname idmclient.example.com.\n\n\n\nNote\nYou must place a period (.) after zone names and host names.\n\n\n\n\n\nConfigure Authentication Type¶\nRHEL IdM supports authentication using Kerberos and also provides\nsupport for SSL certificate management. The following sections provide\ninstructions for configuring authentication options.\n\nConfigure Kerberos Authentication¶\nNow that you have successfully configured the RHEL IdM client and your\nMongoDB server, you must create a MongoDB service entry on the IdM\nserver. This enables RHEL IdM to generate a keytab file, which MongoDB\nrequires in order to use Kerberos for user authentication.\n\nCreate the service entry:\nipa service-add mongodb/idmclient.example.com\n\n\n\nNote\nThe service name must be mongodb/[hostname]@[domain] in order to\nbe valid\n\n\nRetrieve the MongoDB keytab file and place it on the IdM client:\nipa-getkeytab -s idmhost.example.com -p mongodb/idmclient.example.com -k /etc/mongodb.keytab\n\n\nTo verify the keytab file is correct, use the klist\ncommand to confirm principal naming details, use the following\ncommand:\nklist -k /etc/mongodb.keytab\n\n\nThe command should return the following output:\nKeytab name: FILE:/etc/mongodb.keytab\nKVNO Principal\n---- --------------------------------------------------------------------------\n1 mongodb/idmclient.example.com@EXAMPLE.COM\n\n\n\n\n\n\nConfigure SSL Certificate Management¶\nIn addition to Kerberos, RHEL IdM also provides support for SSL\ncertificate management. RHEL IdM can generate certificates for a MongoDB\nserver and MongoDB clients. This enables encryption “in-motion”\namong MongoDB instances and from MongoDB clients.\n\nTo generate a MongoDB server certificate, first issue a certificate\nrequest using a command that resembles the following:\n  ipa-getcert request -r -f /etc/cert/mongodb-server.crt -k /etc/cert/mongodb-server.key -N CN=idmclient.example.com -D idmclient.example.com -K mongodb/idmclient.example.com\n\nHostnames for the ``-D`` and ``CN`` parameters must match the\nhostname of the MongoDB server. The principal name for the\n``-K`` parameter must match the MongoDB service name created\nwhen configuring Kerberos authentication.\n\n\n\nWhen the request returns, concatenate the mongodb-server.key\nand mongodb-server.crt files to create a pem file:\ncat /etc/cert/mongodb-server.key /etc/cert/mongodb-server.crt > /etc/cert/mongodb-server.pem\n\n\n\nNote\nThe owner and group owner of the pem file must match the\nuser that MongoDB is running under\n\n\n\nMongoDB 2.4 supports the notion of a certificate authority (CA) that\ncna establish a “trust chain” of certificates to verify authenticity.\nOn RHEL 6.x systems with the IdM client configured, you can access the\ncertificate authority at /etc/ipa/ca.crt.\nTo generate a MongoDB client certificate, issue another certificate\nrequest and create a pem file, with the following sequence of\ncommands:\nipa-getcert request -f /etc/cert/mongodb-client.crt -k /etc/cert/mongodb-client.key\ncat /etc/cert/mongodb-client.key /etc/cert/mongodb-client.crt > /etc/cert/mongodb-client.pem\n\n\nWhen the command returns, you will have created the\ncertificates necessary for MongoDB to operate with SSL (i.e. the server\ncertificate, CA certificate, and the client certificate).\nRefer to the steps in Connect to MongoDB with SSL to configure MongoDB to use SSL encryption.\n\n\n\nNext Steps¶\nNow that you have successfully set up RHEL Identity Management, and\nconfigured Kerberos, you can refer to the following documents for further\nconfiguration options and deployment instructions.\n\nDeploying MongoDB with Kerberos authentication.\nDetailed instructions for deploying MongoDB using Kerberos\nauthentication, including creating user privilege documents,\nstaring MongoDB with Kerberos support, and provides\ntroubleshooting guidelines.\nOperational Procedures using RHEL IdM\nDetails procedures for adding a new user, granting users access\nprivileges, revoking access, and configuring password policies\nusing RHEL IdM.\n\nAdditionally, consider the specific driver references for configuring\nMongoDB drivers to connect to MongoDB using Kerberos authentication.\n\n\n\n                \n    \n      ←  \n      Hadoop and MongoDB Use Cases\n      Operational Procedures using Red Hat Enterprise Linux Identity Management\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Configure Red Hat Enterprise Linux Identity Management with MongoDB","preview":"On this page Red Hat Enterprise Linux (RHEL) 6.4\nand up includes a complete\nIdentity Management solution\ncalled Red Hat Enterprise Linux IdM. IdM integrates Kerberos authentication, directory\nservices","tags":""},{"slug":"tutorial/getting-started-with-hadoop/","headings":[],"text":"MongoDB Integration and Tools > \n          Getting Started with Hadoop \n      \n    \n  \n                \n                  \nGetting Started with Hadoop\n\nOn this page\n\nPrerequisites\nExamples\nAdditional Resources\n\n\nMongoDB and Hadoop are a powerful combination and can be used together\nto deliver complex analytics and data processing for data stored in\nMongoDB. The following guide shows how you can start working with the\nMongoDB Connector for Hadoop. Once you become familiar with the connector, you\ncan use it to pull your MongoDB data into Hadoop Map-Reduce jobs,\nprocess the data and return results back to a MongoDB collection.\n\nPrerequisites¶\n\nHadoop¶\nIn order to use the following guide, you should already have Hadoop up and\nrunning. This can range from a deployed cluster containing multiple nodes or a\nsingle node pseudo-distributed Hadoop installation running locally. As long as\nyou are able to run any of the examples on your Hadoop installation, you should\nbe all set. The Hadoop connector supports all Apache Hadoop versions 2.X and up,\nincluding distributions based on these versions such as CDH4, CDH5, and HDP 2.0\nand up.\n\n\nMongoDB¶\nInstall and run the latest version of MongoDB. In\naddition, the MongoDB commands should be in your system search path\n(i.e. $PATH).\nIf your mongod requires authorization [1], the user\naccount used by the Hadoop connector must be able to run the\nsplitVector command on the input database. You can either give the\nuser the clusterManager role, or create a custom role for this:\ndb.createRole({\n  role: \"hadoopSplitVector\",\n  privileges: [{\n    resource: {\n      db: \"myDatabase\",\n      collection: \"myCollection\"\n    },\n    actions: [\"splitVector\"]\n  }],\n  roles:[]\n});\ndb.createUser({\n  user: \"hadoopUser\",\n  pwd: \"secret\",\n  roles: [\"hadoopSplitVector\", \"readWrite\"]\n});\n\n\nNote that the splitVector command cannot be run through a mongos, so the\nHadoop connector will automatically connect to the primary shard in order to run\nthe command. If your input collection is unsharded and the connector reads\nthrough a mongos, make sure that your MongoDB Hadoop user is created on the\nprimary shard as well.\n\n\n\n[1]mongod requires clients to\nauthenticate as user when you specify the\nsecurity.authorization or security.keyFile\nsettings or the corresponding --auth or\n--keyFile command line options.\n\n\n\n\nMiscellaneous¶\nIn addition to Hadoop, you should also have git, python, and Java 7 or\ngreater installed.\n\n\n\nExamples¶\nThe MongoDB Connector for Hadoop ships with a few examples of how to use the\nconnector in your own setup. In this guide, we’ll focus on the Treasury Yield example.\nThis example can be run via gradle with the following command:\n./gradlew jar testJar historicalYield\n\n\nYou should see a lot of data scroll past.  To understand what’s going on, let’s break\ndown the steps.  The first thing this task does is to import examples/treasury_yield/src/main/resources/yield_historical_in.json\nin to mongo.  You can view that data as shown below:\n$ mongo mongo_hadoop\nMongoDB shell version: 3.0.4\nconnecting to: mongo_hadoop\n> show collections\nsystem.indexes\nyield_historical.in\nyield_historical.out\n> db.yield_historical.in.find()\n{ \"_id\" : ISODate(\"1990-01-02T00:00:00Z\"), \"dayOfWeek\" : \"TUESDAY\", \"bc3Year\" : 7.9, \"bc5Year\" : 7.87, \"bc10Year\" : 7.94, \"bc20Year\" : null, \"bc1Month\" : null, \"bc2Year\" : 7.87, \"bc3Month\" : 7.83, \"bc30Year\" : 8, \"bc1Year\" : 7.81, \"bc7Year\" : 7.98, \"bc6Month\" : 7.89 }\n{ \"_id\" : ISODate(\"1990-01-03T00:00:00Z\"), \"dayOfWeek\" : \"WEDNESDAY\", \"bc3Year\" : 7.96, \"bc5Year\" : 7.92, \"bc10Year\" : 7.99, \"bc20Year\" : null, \"bc1Month\" : null, \"bc2Year\" : 7.94, \"bc3Month\" : 7.89, \"bc30Year\" : 8.04, \"bc1Year\" : 7.85, \"bc7Year\" : 8.04, \"bc6Month\" : 7.94 }\n...\nhas more\n>\n\n\nWhen you run the examples, gradle will download the appropriate hadoop distribution bundle, extract it,\nand copy over the appropriate dependencies for you.  Subsequent runs will not redownload those files so it’s\nsafe to switch between versions of hadoop.  Gradle will manage all that for you so you need not worry about\nany of those details.  It is recommend to run ./gradlew clean when changing versions of hadoop to\nensure everything is built against the correct versions of libraries.\n\nWhere to go from here¶\nRead the full documentation on the MongoDB Connector for Hadoop\nhere.\nTo modify configuration options, you can put additional lines in the historicalYield task to set\nproperties passed to hadoop at runtime. Read the full comments of the script to see details on\nusing these options to read/write from BSON as well as mongoDB collections.\n\n\n\nAdditional Resources¶\n\nHadoop and MongoDB Integration Overview\nUsing MongoDB with Hadoop & Spark: Part 1\nUsing MongoDB with Hadoop & Spark: Part 2\nUsing MongoDB with Hadoop & Spark: Part 3\n\n\n\n\n                \n    \n      ←  \n      MongoDB Connector for Hadoop\n      Hadoop and MongoDB Use Cases\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Getting Started with Hadoop","preview":"On this page MongoDB and Hadoop are a powerful combination and can be used together\nto deliver complex analytics and data processing for data stored in\nMongoDB. The following guide shows how you can s","tags":""},{"slug":"tutorial/joyent-cloud/","headings":[],"text":"Platforms > \n          Joyent Cloud \n      \n    \n  \n                \n                  \nJoyent Cloud\nFor installing MongoDB on a Joyent Node SmartMachine, see\nJoyentCloud documentation. You must have login\ncredentials to view documentation.\nFor the quickest start, you can use the\nJoyent SmartMachine for MongoDB Appliance.\nThe prebuilt MongoDB Solaris 64 binaries work with Joyent accelerators.\nSome newer gcc libraries are required to run. See the sample setup session below.\n# assuming a 64 bit accelerator\n/usr/bin/isainfo -kv\n\n# get mongodb\n# note this is 'latest' you may want a different version\ncurl -O http://downloads.mongodb.org/sunos5/mongodb-sunos5-x86_64-latest.tgz\ngzip -d mongodb-sunos5-x86_64-latest.tgz\ntar -xf mongodb-sunos5-x86_64-latest.tar\nmv \"mongodb-sunos5-x86_64-2009-10-26\" mongo\n\ncd mongo\n\n# get extra libraries we need (else you will get a libstdc++.so.6 dependency issue)\ncurl -O http://downloads.mongodb.org.s3.amazonaws.com/sunos5/mongo-extra-64.tgz\ngzip -d mongo-extra-64.tgz\ntar -xf mongo-extra-64.tar\n# just as an example - you will really probably want to put these somewhere better:\nexport LD_LIBRARY_PATH=mongo-extra-64\nbin/mongod --help\n\n\n\n\n                \n    \n      ←  \n      dotCloud\n      Modulus\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Joyent Cloud","preview":"For installing MongoDB on a Joyent Node SmartMachine, see\nJoyentCloud documentation. You must have login\ncredentials to view documentation. For the quickest start, you can use the\nJoyent SmartMachine","tags":""},{"slug":"tutorial/manage-red-hat-enterprise-linux-identity-management/","headings":[],"text":"MongoDB Integration and Tools > \n          Operational Procedures using Red Hat Enterprise Linux Identity Management \n      \n    \n  \n                \n                  \nOperational Procedures using Red Hat Enterprise Linux Identity Management\n\nOn this page\n\nUser Management\nConfiguring Password Policies\n\n\nThe Red Hat Enterprise Linux Identity Management solution, RHEL IdM\nintegrates Kerberos authentication, directory services, certificate\nmanagement, DNS and NTP in a single service.\nThe following sections provide instructions for adding and removing\nusers, and managing user permissions, as well as providing\ninstructions for managing password policies within RHEL IdM.\n\nUser Management¶\n\nAdding a New User¶\nTo add a new user, you must first create that user within IdM and then\nmap that user to a set of privileges within MongoDB.\n\nNote\nThis procedure requires administrative privileges within IdM and\nMongoDB\n\n\nAuthenticate as an administrator (e.g. admin@EXAMPLE.COM) in Kerberos\nkinit admin\n\n\n\nAdd a new user by issuing a command similar to the following:\nipa user-add johnsmith --password\n\n\nFollow the prompts to complete adding the new user.\n\nConnect to MongoDB and authenticate as user with at least\nuserAdmin or userAdminAnyDatabase privileges,\nmongo --authenticationMechanism=GSSAPI \\\n      --authenticationDatabase='$external' \\\n      --username admin@EXAMPLE.COM\n\n\n\nAdd the new user to a database and provide the appropriate\nprivileges. In the following example, johnsmith@EXAMPLE.COM\nis granted “read” privileges on the “test” database.\nuse test\ndb.addUser( {\n               \"user\": \"johnsmith@EXAMPLE.COM\",\n               \"roles\": [ \"read\" ],\n               \"userSource\" : \"$external\"\n            } )\n\n\n\n\n\nSee also\nsystem.users Privilege\nDocuments and User\nPrivileges in MongoDB.\n\n\n\nRevoke User Access¶\nTo revoke a user’s access, you must complete two steps: first, you must\nremove the specified user from the databases they have access to,\nand second, you must remove the user from the IdM infrastructure.\n\nAuthenticate as an administrator (e.g. admin@EXAMPLE.COM) in Kerberos\nand connect to MongoDB:\nkinit admin\nmongo --authenticationMechanism=GSSAPI \\\n      --authenticationDatabase='$external' \\\n      --username admin@EXAMPLE.COM\n\n\n\nRemove the user from a database using db.removeUser.\nThe following example removes user\njohnsmith@EXAMPLE.COM from the test database:\nuse test\ndb.removeUser(\"johnsmith@EXAMPLE.COM\")\n\n\nRepeat these steps for any databases that the user has access to.\n\n\nYou can now either disable or remove the user. Disabled users still exist\nwithin the IdM system, but no longer have access to any IdM services\n(e.g. Kerberos). It is still possible to reactive a disabled user\nby granting them access to a database.\nBy contrast, removing a user deletes their information from IdM and they\ncannot be reactivated in the future.\nTo disable a user (in this case, johnsmith), issue a command that\nresembles the following:\nkinit admin\nipa user-disable johnsmith\n\n\nTo remove the user, use the user-del instead, as in the following:\nkinit admin\nipa user-del johnsmith\n\n\nWhether disabled or removed, the user in question will no longer have\naccess to Kerberos and will be unable to authenticate to any IdM\nclients.\n\n\n\nConfiguring Password Policies¶\nBy default, RHEL IdM provides a global password policy for all users and\ngroups. To view the policy details, connect as an administrator,\nand execute the pwpolicy-show command, is the following:\nkinit admin\nipa pwpolicy-show\n\n\nTo view the policies applied to a particular user, you can add the\n--user=<username> flag, as in the following:\nkinit admin\nipa pwpolicy-show --user=johnsmith\n\n\nYou can edit password policies to update parameters such as lockout\ntime or minimum length. The following command changes the global\npolicy’s minimum length allowable for passwords, setting the minimum\nlength to 10 characters.\nkinit admin\nipa pwpolicy-mod --minlength=10\n\n\nFor more information, refer to Defining Password Policies\nwithin the IdM documentation.\n\n\n\n                \n    \n      ←  \n      Configure Red Hat Enterprise Linux Identity Management with MongoDB\n      HTTP Interface\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Operational Procedures using Red Hat Enterprise Linux Identity Management","preview":"On this page The Red Hat Enterprise Linux Identity Management solution, RHEL IdM\nintegrates Kerberos authentication, directory services, certificate\nmanagement, DNS and NTP in a single service. The fo","tags":""},{"slug":"use-cases/category-hierarchy/","headings":[],"text":"Use Cases > \n          Category Hierarchy \n      \n    \n  \n                \n                  \nCategory Hierarchy\n\nOn this page\n\nOverview\nOperations\nSharding\n\n\n\nOverview¶\nThis document provides the basic design for modeling a product\nhierarchy stored in MongoDB as well as a collection of common\noperations for interacting with this data that will help you begin to\nwrite an E-commerce product category hierarchy.\n\nSee also\n“Product Catalog”\n\n\nSolution¶\nTo model a product category hierarchy, this solution keeps each\ncategory in its own document that also has a list of its ancestors or\n“parents.” This document uses music genres as the basis of its\nexamples:\n\n\nInitial category hierarchy\n\nBecause these kinds of categories change infrequently, this model\nfocuses on the operations needed to keep the hierarchy up-to-date\nrather than the performance profile of update operations.\n\n\nSchema¶\nThis schema has the following properties:\n\nA single document represents each category in the hierarchy.\nAn ObjectId identifies each category document for internal\ncross-referencing.\nEach category document has a human-readable name and a\nURL compatible slug field.\nThe schema stores a list of ancestors for each category to\nfacilitate displaying a query and its ancestors using only a single\nquery.\n\nConsider the following prototype:\n{ \"_id\" : ObjectId(\"4f5ec858eb03303a11000002\"),\n  \"name\" : \"Modal Jazz\",\n  \"parent\" : ObjectId(\"4f5ec858eb03303a11000001\"),\n  \"slug\" : \"modal-jazz\",\n  \"ancestors\" : [\n         { \"_id\" : ObjectId(\"4f5ec858eb03303a11000001\"),\n        \"slug\" : \"bop\",\n        \"name\" : \"Bop\" },\n         { \"_id\" : ObjectId(\"4f5ec858eb03303a11000000\"),\n           \"slug\" : \"ragtime\",\n           \"name\" : \"Ragtime\" } ]\n}\n\n\n\n\n\nOperations¶\nThis section outlines the category hierarchy manipulations that you\nmay need in an E-Commerce site.  All examples in this document use the\nPython programming language and the PyMongo\ndriver for MongoDB, but you can implement this system using\nany language you choose.\n\nRead and Display a Category¶\n\nQuerying¶\nUse the following option to read and display a category\nhierarchy. This query will use the slug field to return the\ncategory information and a “bread crumb” trail from the current\ncategory to the top level category.\ncategory = db.categories.find(\n    {'slug':slug},\n    {'_id':0, 'name':1, 'ancestors.slug':1, 'ancestors.name':1 })\n\n\n\n\nIndexing¶\nCreate a unique index on the slug field with the following\noperation on the Python/PyMongo console:\n>>> db.categories.ensure_index('slug', unique=True)\n\n\n\n\n\nAdd a Category to the Hierarchy¶\nTo add a category you must first determine its ancestors. Take adding\na new category “Swing” as a child of “Ragtime”, as below:\n\n\nAdding a category\n\nThe insert operation would be trivial except for the ancestors. To\ndefine this array, consider the following helper function:\ndef build_ancestors(_id, parent_id):\n    parent = db.categories.find_one(\n        {'_id': parent_id},\n        {'name': 1, 'slug': 1, 'ancestors':1})\n    parent_ancestors = parent.pop('ancestors')\n    ancestors = [ parent ] + parent_ancestors\n    db.categories.update(\n        {'_id': _id},\n        {'$set': { 'ancestors': ancestors } })\n\n\nYou only need to travel “up” one level in the hierarchy to get the\nancestor list for “Ragtime” that you can use to build the ancestor\nlist for “Swing.” Then create a document with the following set of\noperations:\ndoc = dict(name='Swing', slug='swing', parent=ragtime_id)\nswing_id = db.categories.insert(doc)\nbuild_ancestors(swing_id, ragtime_id)\n\n\n\nNote\nSince these queries and updates all selected based on _id, you\nonly need the default MongoDB-supplied index on _id to support\nthis operation efficiently.\n\n\n\nChange the Ancestry of a Category¶\nThis section address the process for reorganizing the hierarchy by\nmoving “bop” under “swing” as follows:\n\n\nChange the parent of a category\n\n\nProcedure¶\nUpdate the bop document to reflect the change in ancestry with the\nfollowing operation:\ndb.categories.update(\n    {'_id':bop_id}, {'$set': { 'parent': swing_id } } )\n\n\nThe following helper function, rebuilds the ancestor fields to ensure\ncorrectness. [1]\ndef build_ancestors_full(_id, parent_id):\n    ancestors = []\n    while parent_id is not None:\n        parent = db.categories.find_one(\n            {'_id': parent_id},\n            {'parent': 1, 'name': 1, 'slug': 1, 'ancestors':1})\n        parent_id = parent.pop('parent')\n        ancestors.append(parent)\n    db.categories.update(\n        {'_id': _id},\n        {'$set': { 'ancestors': ancestors } })\n\n\nYou can use the following loop to reconstruct all the descendants of\nthe “bop” category:\nfor cat in db.categories.find(\n    {'ancestors._id': bop_id},\n    {'parent': 1}):\n    build_ancestors_full(cat['_id'], cat['parent'])\n\n\n\n\n\n[1]Your application cannot guarantee that the\nancestor list of a parent category is correct, because MongoDB may\nprocess the categories out-of-order.\n\n\n\n\nIndexing¶\nCreate an index on the ancestors._id field to support the update\noperation.\ndb.categories.ensure_index('ancestors._id')\n\n\n\n\n\nRename a Category¶\nTo a rename a category you need to both update the category itself and\nalso update all the descendants. Consider renaming “Bop” to\n“BeBop” as in the following figure:\n\n\nRename a category\n\nFirst, you need to update the category name with the following operation:\ndb.categories.update(\n    {'_id':bop_id}, {'$set': { 'name': 'BeBop' } } )\n\n\nNext, you need to update each descendant’s ancestors list:\ndb.categories.update(\n    {'ancestors._id': bop_id},\n    {'$set': { 'ancestors.$.name': 'BeBop' } },\n    multi=True)\n\n\nThis operation uses:\n\nthe positional operation $ to match the exact “ancestor” entry\nthat matches the query, and\nthe multi option to update all documents that match this query.\n\n\nNote\nIn this case, the index you have already defined on\nancestors._id is sufficient to ensure good performance.\n\n\n\n\nSharding¶\nFor most deployments, sharding this collection has\nlimited value because the collection will be very small. If you do\nneed to shard, because most updates query the _id field, this\nfield is a suitable shard key. Shard the collection with the\nfollowing operation in the Python/PyMongo console.\n>>> db.command('shardCollection', 'categories', {\n...     'key': {'_id': 1} })\n{ \"collectionsharded\" : \"categories\", \"ok\" : 1 }\n\n\n\n\n\n                \n    \n      ←  \n      Inventory Management\n      Metadata and Asset Management\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Category Hierarchy","preview":"On this page This document provides the basic design for modeling a product\nhierarchy stored in MongoDB as well as a collection of common\noperations for interacting with this data that will help you b","tags":""},{"slug":"use-cases/hadoop/","headings":[],"text":"MongoDB Integration and Tools > \n          Hadoop and MongoDB Use Cases \n      \n    \n  \n                \n                  \nHadoop and MongoDB Use Cases\n\nOn this page\n\nBatch Aggregation\nData Warehouse\nETL Data\nAdditional Resources\n\n\nThe following are some example deployments with MongoDB and Hadoop. The\ngoal is to provide a high-level description of how MongoDB and Hadoop\ncan fit together in a typical Big Data stack. In each of the following\nexamples MongoDB is used as the “operational” real-time data store and\nHadoop is used for offline batch data processing and analysis.\n\nBatch Aggregation¶\nIn several scenarios the built-in aggregation functionality provided by\nMongoDB is sufficient for analyzing your data. However in certain cases,\nsignificantly more complex data aggregation may be necessary. This is\nwhere Hadoop can provide a powerful framework for complex analytics.\nIn this scenario data is pulled from MongoDB and processed within Hadoop\nvia one or more MapReduce jobs. Data may also be brought in from\nadditional sources within these MapReduce jobs to develop a\nmulti-datasource solution. Output from these MapReduce jobs can then be\nwritten back to MongoDB for later querying and ad-hoc analysis.\nApplications built on top of MongoDB can now use the information from\nthe batch analytics to present to the end user or to drive other\ndownstream features.\n\n\n\nData Warehouse¶\nIn a typical production scenario, your application’s data may live in\nmultiple datastores, each with their own query language and\nfunctionality. To reduce complexity in these scenarios, Hadoop can be\nused as a data warehouse and act as a centralized repository for data\nfrom the various sources.\nIn this situation, you could have periodic MapReduce jobs that load\ndata from MongoDB into Hadoop. This could be in the form of “daily” or\n“weekly” data loads pulled from MongoDB via MapReduce. Once the data\nfrom MongoDB is available from within Hadoop, and data from other\nsources are also available, the larger dataset data can be queried\nagainst. Data analysts now have the option of using either MapReduce or\nPig to create jobs that query the larger datasets that incorporate data\nfrom MongoDB.\n\n\n\nETL Data¶\nMongoDB may be the operational datastore for your application but there\nmay also be other datastores that are holding your organization’s data.\nIn this scenario it is useful to be able to move data from one datastore\nto another, either from your application’s data to another database or\nvice versa. Moving the data is much more complex than simply piping it\nfrom one mechanism to another, which is where Hadoop can be used.\nIn this scenario, Map-Reduce jobs are used to extract, transform and\nload data from one store to another. Hadoop can act as a complex ETL\nmechanism to migrate data in various forms via one or more MapReduce\njobs that pull the data from one store, apply multiple transformations\n(applying new data layouts or other aggregation) and loading the data to\nanother store. This approach can be used to move data from or to\nMongoDB, depending on the desired result.\n\n\n\n\nAdditional Resources¶\n\nHadoop and MongoDB Integration Overview\nUsing MongoDB with Hadoop & Spark: Part 1\nUsing MongoDB with Hadoop & Spark: Part 2\nUsing MongoDB with Hadoop & Spark: Part 3\n\n\n\n\n                \n    \n      ←  \n      Getting Started with Hadoop\n      Configure Red Hat Enterprise Linux Identity Management with MongoDB\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Hadoop and MongoDB Use Cases","preview":"On this page The following are some example deployments with MongoDB and Hadoop. The\ngoal is to provide a high-level description of how MongoDB and Hadoop\ncan fit together in a typical Big Data stack.","tags":""},{"slug":"use-cases/hierarchical-aggregation/","headings":[],"text":"Use Cases > \n          Hierarchical Aggregation \n      \n    \n  \n                \n                  \nHierarchical Aggregation\n\nOn this page\n\nOverview\nSchema\nOperations\nSharding\n\n\n\nOverview¶\n\nBackground¶\nIf you collect a large amount of data, but do not pre-aggregate, and you want to have\naccess to aggregated information and reports, then you need a method\nto aggregate these data into a usable form. This document provides an\noverview of these aggregation patterns and processes.\nFor clarity, this case study assumes that the incoming event data\nresides in a collection named events. For details on how you might\nget the event data into the events collection, please see\n“Storing Log Data” document. This document\ncontinues using this example.\n\n\nSolution¶\nThe first step in the aggregation process is to aggregate event data\ninto the finest required granularity. Then use this aggregation to\ngenerate the next least specific level granularity and this repeat\nprocess until you have generated all required views.\nThe solution uses several collections: the raw data (i.e. events)\ncollection as well as collections for aggregated hourly, daily,\nweekly, monthly, and yearly statistics. All aggregations use the\nmapReduce command, in a\nhierarchical process. The following figure illustrates the input and\noutput of each job:\n\n\nHierarchy of data aggregation.\n\n\nNote\nAggregating raw events into an hourly collection is qualitatively\ndifferent from the operation that aggregates hourly statistics into\nthe daily collection.\n\n\nSee also\nmap-reduce, mapReduce, and the\nMap-Reduce Documentation page for more\ninformation on the Map-reduce data aggregation paradigm.\n\n\n\n\nSchema¶\nWhen designing the schema for event storage, it’s important to track\nthe events included in the aggregation and events that are not yet\nincluded.\n\nRelational Approach\nA simple tactic from relational database, uses an auto-incremented\ninteger as the primary key. However, this introduces a significant\nperformance penalty for event logging process because the\naggregation process must fetch new keys one at a time.\n\nIf you can batch your inserts into the events collection, you can\nuse an auto-increment primary key by using the\nfind_and_modify\ncommand to generate the _id values, as in the following example:\n>>> obj = db.my_sequence.find_and_modify(\n...     query={'_id':0},\n...     update={'$inc': {'inc': 50}}\n...     upsert=True,\n...     new=True)\n>>> batch_of_ids = range(obj['inc']-50, obj['inc'])\n\n\nHowever, in most cases you can simply include a timestamp with each\nevent that you can use to distinguish processed events from\nunprocessed events.\nThis example assumes that you are calculating average session length\nfor logged-in users on a website. The events will have the following\nform:\n{\n    \"userid\": \"rick\",\n    \"ts\": ISODate('2010-10-10T14:17:22Z'),\n    \"length\":95\n}\n\n\nThe operations described in the next session will calculate total and\naverage session times for each user at the hour, day, week, month and\nyear. For each aggregation you will want to store the number of\nsessions so that MongoDB can incrementally recompute the average\nsession times. The aggregate document will resemble the following:\n{\n   _id: { u: \"rick\", d: ISODate(\"2010-10-10T14:00:00Z\") },\n   value: {\n       ts: ISODate('2010-10-10T15:01:00Z'),\n       total: 254,\n       count: 10,\n       mean: 25.4 }\n}\n\n\n\nNote\nThe timestamp value in the _id sub-document, which will allow\nyou to incrementally update documents at various levels of the\nhierarchy.\n\n\n\nOperations¶\nThis section assumes that all events exist in the events\ncollection and have a timestamp. The operations, thus are to aggregate\nfrom the events collection into the smallest aggregate–hourly\ntotals– and then aggregate from the hourly totals into coarser\ngranularity levels. In all cases, these operations will store\naggregation time as a last_run variable.\n\nCreating Hourly Views from Event Collections¶\n\nAggregation¶\n\nNote\nAlthough this solution uses Python and PyMongo to connect with MongoDB, you must pass JavaScript\nfunctions (i.e. mapf, reducef, and finalizef) to the\nmapReduce command.\n\nBegin by creating a map function, as below:\nmapf_hour = bson.Code('''function() {\n    var key = {\n        u: this.userid,\n        d: new Date(\n            this.ts.getFullYear(),\n            this.ts.getMonth(),\n            this.ts.getDate(),\n            this.ts.getHours(),\n            0, 0, 0);\n    emit(\n        key,\n        {\n            total: this.length,\n            count: 1,\n            mean: 0,\n            ts: new Date(); });\n}''')\n\n\nIn this case, it emits key-value pairs that contain the data you want\nto aggregate as you’d expect. The function also emits a ts value\nthat makes it possible to cascade aggregations to coarser grained\naggregations (i.e. hour to day, etc.)\nConsider the following reduce function:\nreducef = bson.Code('''function(key, values) {\n    var r = { total: 0, count: 0, mean: 0, ts: null };\n    values.forEach(function(v) {\n        r.total += v.total;\n        r.count += v.count;\n    });\n    return r;\n}''')\n\n\nThe reduce function returns a document in the same format as the\noutput of the map function. This pattern for map and reduce functions\nmakes map-reduce processes easier to test and debug.\nWhile the reduce function ignores the mean and ts (timestamp)\nvalues, the finalize step, as follows, computes these data:\nfinalizef = bson.Code('''function(key, value) {\n    if(value.count > 0) {\n       value.mean = value.total / value.count;\n    }\n    value.ts = new Date();\n    return value;\n}''')\n\n\nWith the above function the map_reduce\noperation itself will resemble the following:\ncutoff = datetime.utcnow() - timedelta(seconds=60)\nquery = { 'ts': { '$gt': last_run, '$lt': cutoff } }\n\ndb.events.map_reduce(\n    map=mapf_hour,\n    reduce=reducef,\n    finalize=finalizef,\n    query=query,\n    out={ 'reduce': 'stats.hourly' })\n\nlast_run = cutoff\n\n\nThe cutoff variable allows you to process all events that have\noccurred since the last run but before 1 minute ago. This allows for\nsome delay in logging events. You can safely run this aggregation as\noften as you like, provided that you update the last_run variable\neach time.\n\n\nIndexing¶\nCreate an index on the timestamp (i.e. the ts field) to support\nthe query selection of the map_reduce\noperation. Use the following operation at the Python/PyMongo console:\n>>> db.events.ensure_index('ts')\n\n\n\n\n\nDeriving Day-Level Data¶\n\nAggregation¶\nTo calculate daily statistics, use the hourly statistics\nas input. Begin with the following map function:\nmapf_day = bson.Code('''function() {\n    var key = {\n        u: this._id.u,\n        d: new Date(\n            this._id.d.getFullYear(),\n            this._id.d.getMonth(),\n            this._id.d.getDate(),\n            0, 0, 0, 0) };\n    emit(\n        key,\n        {\n            total: this.value.total,\n            count: this.value.count,\n            mean: 0,\n            ts: null });\n}''')\n\n\nThe map function for deriving day-level data differs from the initial\naggregation above in the following ways:\n\nthe aggregation key is the (userid, date) rather than (userid, hour)\nto support daily aggregation.\n\nthe keys and values emitted (i.e. emit()) are actually the total\nand count values from the hourly aggregates rather than properties\nfrom event documents.\nThis is the case for all the higher-level aggregation operations.\n\n\nBecause the output of this map function is the same as the previous\nmap function, you can use the same reduce and finalize functions.\nThe actual code driving this level of aggregation is as follows:\ncutoff = datetime.utcnow() - timedelta(seconds=60)\nquery = { 'value.ts': { '$gt': last_run, '$lt': cutoff } }\n\ndb.stats.hourly.map_reduce(\n    map=mapf_day,\n    reduce=reducef,\n    finalize=finalizef,\n    query=query,\n    out={ 'reduce': 'stats.daily' })\n\nlast_run = cutoff\n\n\nThere are a couple of things to note here. First of all, the query is\nnot on ts now, but value.ts, the timestamp written during the\nfinalization of the hourly aggregates. Also note that you are, in fact,\naggregating from the stats.hourly collection into the stats.daily\ncollection.\n\n\nIndexing¶\nBecause you will run the query option regularly which finds on the\nvalue.ts field, you may wish to create an index to support\nthis. Use the following operation in the Python/PyMongo shell to\ncreate this index:\n>>> db.stats.hourly.ensure_index('value.ts')\n\n\n\n\n\nWeekly and Monthly Aggregation¶\n\nAggregation¶\nYou can use the aggregated day-level data to generate weekly and\nmonthly statistics. A map function for generating weekly data follows:\nmapf_week = bson.Code('''function() {\n    var key = {\n        u: this._id.u,\n        d: new Date(\n            this._id.d.valueOf()\n            - dt.getDay()*24*60*60*1000) };\n    emit(\n        key,\n        {\n            total: this.value.total,\n            count: this.value.count,\n            mean: 0,\n            ts: null });\n}''')\n\n\nHere, to get the group key, the function takes the current and\nsubtracts days until you get the beginning of the week. In the weekly\nmap function, you’ll use the first day of the month as the group key,\nas follows:\nmapf_month = bson.Code('''function() {\n        d: new Date(\n            this._id.d.getFullYear(),\n            this._id.d.getMonth(),\n            1, 0, 0, 0, 0) };\n    emit(\n        key,\n        {\n            total: this.value.total,\n            count: this.value.count,\n            mean: 0,\n            ts: null });\n}''')\n\n\nThese map functions are identical to each other except for the date\ncalculation.\n\n\nIndexing¶\nCreate additional indexes to support the weekly and monthly\naggregation options on the value.ts field. Use the following\noperation in the Python/PyMongo shell.\n>>> db.stats.daily.ensure_index('value.ts')\n>>> db.stats.monthly.ensure_index('value.ts')\n\n\n\n\nRefactor Map Functions¶\nUse Python’s string interpolation to refactor the map\nfunction definitions as follows:\nmapf_hierarchical = '''function() {\n    var key = {\n        u: this._id.u,\n        d: %s };\n    emit(\n        key,\n        {\n            total: this.value.total,\n            count: this.value.count,\n            mean: 0,\n            ts: null });\n}'''\n\nmapf_day = bson.Code(\n    mapf_hierarchical % '''new Date(\n            this._id.d.getFullYear(),\n            this._id.d.getMonth(),\n            this._id.d.getDate(),\n            0, 0, 0, 0)''')\n\nmapf_week = bson.Code(\n    mapf_hierarchical % '''new Date(\n            this._id.d.valueOf()\n            - dt.getDay()*24*60*60*1000)''')\n\nmapf_month = bson.Code(\n    mapf_hierarchical % '''new Date(\n            this._id.d.getFullYear(),\n            this._id.d.getMonth(),\n            1, 0, 0, 0, 0)''')\n\nmapf_year = bson.Code(\n    mapf_hierarchical % '''new Date(\n            this._id.d.getFullYear(),\n            0, 1, 0, 0, 0, 0)''')\n\n\nYou can create a h_aggregate function to wrap the\nmap_reduce\noperation, as below, to reduce code duplication:\ndef h_aggregate(icollection, ocollection, mapf, cutoff, last_run):\n     query = { 'value.ts': { '$gt': last_run, '$lt': cutoff } }\n     icollection.map_reduce(\n         map=mapf,\n         reduce=reducef,\n         finalize=finalizef,\n         query=query,\n         out={ 'reduce': ocollection.name })\n\n\nWith h_aggregate defined, you can perform all aggregation\noperations as follows:\ncutoff = datetime.utcnow() - timedelta(seconds=60)\n\nh_aggregate(db.events, db.stats.hourly, mapf_hour, cutoff, last_run)\nh_aggregate(db.stats.hourly, db.stats.daily, mapf_day, cutoff, last_run)\nh_aggregate(db.stats.daily, db.stats.weekly, mapf_week, cutoff, last_run)\nh_aggregate(db.stats.daily, db.stats.monthly, mapf_month, cutoff, last_run)\nh_aggregate(db.stats.monthly, db.stats.yearly, mapf_year, cutoff, last_run)\n\nlast_run = cutoff\n\n\nAs long as you save and restore the last_run variable between\naggregations, you can run these aggregations as often as you like\nsince each aggregation operation is incremental.\n\n\n\n\nSharding¶\nEnsure that you choose a shard key that is not the incoming\ntimestamp, but rather something that varies significantly in the most\nrecent documents. In the example above, consider using the userid\nas the most significant part of the shard key.\nTo prevent a single, active user from creating a large, chunk\nthat MongoDB cannot split, use a compound shard key with (username,\ntimestamp) on the events collection. Consider the following:\n>>> db.command('shardCollection','events', {\n... 'key' : { 'userid': 1, 'ts' : 1} } )\n{ \"collectionsharded\": \"events\", \"ok\" : 1 }\n\n\nTo shard the aggregated collections you must use the _id field, so you can\nissue the following group of shard operations in the Python/PyMongo shell:\ndb.command('shardCollection', 'stats.daily', {\n     'key': { '_id': 1 } })\ndb.command('shardCollection', 'stats.weekly', {\n     'key': { '_id': 1 } })\ndb.command('shardCollection', 'stats.monthly', {\n     'key': { '_id': 1 } })\ndb.command('shardCollection', 'stats.yearly', {\n     'key': { '_id': 1 } })\n\n\nYou should also update the h_aggregate  map-reduce wrapper to\nsupport sharded output Add 'sharded':True to the out\nargument. See the full sharded h_aggregate function:\ndef h_aggregate(icollection, ocollection, mapf, cutoff, last_run):\n     query = { 'value.ts': { '$gt': last_run, '$lt': cutoff } }\n     icollection.map_reduce(\n         map=mapf,\n         reduce=reducef,\n         finalize=finalizef,\n         query=query,\n         out={ 'reduce': ocollection.name, 'sharded': True })\n\n\n\n\n\n                \n    \n      ←  \n      Pre-Aggregated Reports (MMAPv1)\n      Product Catalog\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Hierarchical Aggregation","preview":"On this page If you collect a large amount of data, but do not pre-aggregate, and you want to have\naccess to aggregated information and reports, then you need a method\nto aggregate these data into a u","tags":""},{"slug":"use-cases/inventory-management/","headings":[],"text":"Use Cases > \n          Inventory Management \n      \n    \n  \n                \n                  \nInventory Management\n\nOn this page\n\nOverview\nOperations\nSharding\nAdditional Resources\n\n\n\nOverview¶\nThis case study provides an overview of practices and patterns for\ndesigning and developing the inventory management portions of an\nE-commerce application.\n\nSee also\n“Product Catalog.”\n\n\nProblem¶\nCustomers in e-commerce stores regularly add and remove items from\ntheir “shopping cart,” change quantities multiple times, abandon the\ncart at any point, and sometimes have problems during and after\ncheckout that require a hold or canceled order. These activities make\nit difficult to maintain inventory systems and counts and ensure that\ncustomers cannot “buy” items that are unavailable while they shop in\nyour store.\n\n\nSolution¶\nThis solution keeps the traditional metaphor of the shopping cart, but\nthe shopping cart will age. After a shopping cart has been inactive\nfor a certain period of time, all items in the cart re-enter the\navailable inventory and the cart is empty. The state transition\ndiagram for a shopping cart is below:\n\n\n\n\n\nSchema¶\nInventory collections must maintain counts of the current available\ninventory of each stock-keeping unit (SKU; or item) as well as a list\nof items in carts that may return to the available inventory if they\nare in a shopping cart that times out. In the following example, the\n_id field stores the SKU:\n{\n    _id: '00e8da9b',\n    qty: 16,\n    carted: [\n        { qty: 1, cart_id: 42,\n          timestamp: ISODate(\"2012-03-09T20:55:36Z\"), },\n        { qty: 2, cart_id: 43,\n          timestamp: ISODate(\"2012-03-09T21:55:36Z\"), },\n    ]\n}\n\n\n\nNote\nThese examples use a simplified schema.  In a production\nimplementation, you may choose to merge this schema with the\nproduct catalog schema described in the\n“Product Catalog” document.\n\nThe SKU above has 16 items in stock, 1 item a cart, and 2 items in a\nsecond cart.  This leaves a total of 19 unsold items of merchandise.\nTo model the shopping cart objects, you need to maintain sku,\nquantity, fields embedded in a shopping cart document:\n{\n    _id: 42,\n    last_modified: ISODate(\"2012-03-09T20:55:36Z\"),\n    status: 'active',\n    items: [\n        { sku: '00e8da9b', qty: 1, item_details: {...} },\n        { sku: '0ab42f88', qty: 4, item_details: {...} }\n    ]\n}\n\n\n\nNote\nThe item_details field in each line item allows your\napplication to display the cart contents to the user without\nrequiring a second query to fetch details from the catalog\ncollection.\n\n\n\n\nOperations¶\nThis section introduces operations that you may use to support an\ne-commerce site. All examples in this document use the Python\nprogramming language and the PyMongo\ndriver for MongoDB, but you can implement this system using\nany language you choose.\n\nAdd an Item to a Shopping Cart¶\nMoving an item from the available inventory to a cart is a fundamental\nrequirement for a shopping cart system. The most important requirement\nis to ensure that your application will never move an unavailable item\nfrom the inventory to the cart.\nEnsure that inventory is only updated if there is sufficient inventory\nto satisfy the request with the following add_item_to_cart\nfunction operation.\ndef add_item_to_cart(cart_id, sku, qty, details):\n    now = datetime.utcnow()\n\n    # Make sure the cart is still active and add the line item\n    result = db.cart.update(\n        {'_id': cart_id, 'status': 'active' },\n        { '$set': { 'last_modified': now },\n          '$push': {\n              'items': {'sku': sku, 'qty':qty, 'details': details } } },\n        w=1)\n    if not result['updatedExisting']:\n        raise CartInactive()\n\n    # Update the inventory\n    result = db.inventory.update(\n        {'_id':sku, 'qty': {'$gte': qty}},\n        {'$inc': {'qty': -qty},\n         '$push': {\n             'carted': { 'qty': qty, 'cart_id':cart_id,\n                         'timestamp': now } } },\n        w=1)\n    if not result['updatedExisting']:\n        # Roll back our cart update\n        db.cart.update(\n            {'_id': cart_id },\n            { '$pull': { 'items': {'sku': sku } } })\n        raise InadequateInventory()\n\n\n\nThe system does not trust that the available inventory can satisfy a request\nFirst this operation checks to make sure that the cart is\n“active” before adding an item. Then, it verifies that the available\ninventory to satisfy the request before decrementing inventory.\nIf there is not adequate inventory, the system removes the cart\nupdate: by specifying w=1 and checking the result allows\nthe application to report an error if the cart is inactive or\navailable quantity is insufficient to satisfy the request.\n\n\nNote\nThis operation requires no indexes beyond the\ndefault index on the _id field.\n\n\n\nModifying the Quantity in the Cart¶\nThe following process underlies adjusting the quantity of items in a\nusers cart. The application must ensure that when a user increases the\nquantity of an item, in addition to updating the carted entry for\nthe user’s cart, that the inventory exists to cover the modification.\ndef update_quantity(cart_id, sku, old_qty, new_qty):\n    now = datetime.utcnow()\n    delta_qty = new_qty - old_qty\n\n    # Make sure the cart is still active and add the line item\n    result = db.cart.update(\n        {'_id': cart_id, 'status': 'active', 'items.sku': sku },\n        {'$set': {\n             'last_modified': now,\n             'items.$.qty': new_qty },\n        },\n        w=1)\n    if not result['updatedExisting']:\n        raise CartInactive()\n\n    # Update the inventory\n    result = db.inventory.update(\n        {'_id':sku,\n         'carted.cart_id': cart_id,\n         'qty': {'$gte': delta_qty} },\n        {'$inc': {'qty': -delta_qty },\n         '$set': { 'carted.$.qty': new_qty, 'timestamp': now } },\n        w=1)\n    if not result['updatedExisting']:\n        # Roll back our cart update\n        db.cart.update(\n            {'_id': cart_id, 'items.sku': sku },\n            {'$set': { 'items.$.qty': old_qty } })\n        raise InadequateInventory()\n\n\n\nNote\nThat the positional operator $ updates the particular\ncarted entry and item that matched the query.\nThis allows the application to update the inventory and keep track\nof the data needed to “rollback” the cart in a single atomic\noperation. The code also ensures that the cart is active.\n\n\nNote\nThis operation requires no indexes beyond the\ndefault index on the _id field.\n\n\n\nChecking Out¶\nThe checkout operation must: validate the method of payment and remove\nthe carted items after the transaction succeeds. Consider the\nfollowing procedure:\ndef checkout(cart_id):\n    now = datetime.utcnow()\n\n    # Make sure the cart is still active and set to 'pending'. Also\n    #     fetch the cart details so we can calculate the checkout price\n    cart = db.cart.find_and_modify(\n        {'_id': cart_id, 'status': 'active' },\n        update={'$set': { 'status': 'pending','last_modified': now } } )\n    if cart is None:\n        raise CartInactive()\n\n    # Validate payment details; collect payment\n    try:\n        collect_payment(cart)\n        db.cart.update(\n            {'_id': cart_id },\n            {'$set': { 'status': 'complete' } } )\n        db.inventory.update(\n            {'carted.cart_id': cart_id},\n            {'$pull': {'cart_id': cart_id} },\n            multi=True)\n    except:\n        db.cart.update(\n            {'_id': cart_id },\n            {'$set': { 'status': 'active' } } )\n        raise\n\n\nBegin by “locking” the cart by setting its status to “pending” Then\nthe system will verify that the cart is still active and collect\npayment data. Then, the findAndModify command makes it possible to update the cart atomically\nand return its details to capture payment information. Then:\n\nIf the payment is successful, then the application will remove the\ncarted items from the inventory documents and set the cart to\ncomplete.\nIf payment is unsuccessful, the application will unlock the cart by\nsetting its status to active and report a payment error.\n\n\nNote\nThis operation requires no indexes beyond the\ndefault index on the _id field.\n\n\n\nReturning Inventory from Timed-Out Carts¶\n\nProcess¶\nPeriodically, your application must “expire” inactive carts and return\ntheir items to available inventory. In the example that follows the\nvariable timeout controls the length of time before a cart\nexpires:\ndef expire_carts(timeout):\n    now = datetime.utcnow()\n    threshold = now - timedelta(seconds=timeout)\n\n    # Lock and find all the expiring carts\n    db.cart.update(\n        {'status': 'active', 'last_modified': { '$lt': threshold } },\n        {'$set': { 'status': 'expiring' } },\n        multi=True )\n\n    # Actually expire each cart\n    for cart in db.cart.find({'status': 'expiring'}):\n\n        # Return all line items to inventory\n        for item in cart['items']:\n            db.inventory.update(\n                { '_id': item['sku'],\n                  'carted.cart_id': cart['id'],\n                  'carted.qty': item['qty']\n                },\n                {'$inc': { 'qty': item['qty'] },\n                 '$pull': { 'carted': { 'cart_id': cart['id'] } } })\n\n        db.cart.update(\n            {'_id': cart['id'] },\n            {'$set': { 'status': 'expired' })\n\n\nThis procedure:\n\nfinds all carts that are older than the threshold and are due\nfor expiration.\nfor each “expiring” cart, return all items to the available\ninventory.\nonce the items return to the available inventory, set the\nstatus field to expired.\n\n\n\nIndexing¶\nTo support returning inventory from timed-out cart, create an index to\nsupport queries on\ntheir status and last_modified fields. Use the following\noperations in the Python/PyMongo shell:\ndb.cart.ensure_index([('status', 1), ('last_modified', 1)])\n\n\n\n\n\nError Handling¶\nThe above operations do not account for one possible failure\nsituation: if an exception occurs after updating the shopping cart but\nbefore updating the inventory collection. This would result in a\nshopping cart that may be absent or expired but items have not\nreturned to available inventory.\nTo account for this case, your application will need a periodic\ncleanup operation that finds inventory items that have carted\nitems and check that to ensure that they exist in a user’s cart, and\nreturn them to available inventory if they do not.\ndef cleanup_inventory(timeout):\n    now = datetime.utcnow()\n    threshold = now - timedelta(seconds=timeout)\n\n    # Find all the expiring carted items\n    for item in db.inventory.find(\n        {'carted.timestamp': {'$lt': threshold }}):\n\n        # Find all the carted items that matched\n        carted = dict(\n                  (carted_item['cart_id'], carted_item)\n                  for carted_item in item['carted']\n                  if carted_item['timestamp'] < threshold)\n\n        # First Pass: Find any carts that are active and refresh the carted items\n        for cart in db.cart.find(\n            { '_id': {'$in': carted.keys() },\n            'status':'active'}):\n            cart = carted[cart['_id']]\n\n            db.inventory.update(\n                { '_id': item['_id'],\n                  'carted.cart_id': cart['_id'] },\n                { '$set': {'carted.$.timestamp': now } })\n            del carted[cart['_id']]\n\n        # Second Pass: All the carted items left in the dict need to now be\n        #    returned to inventory\n        for cart_id, carted_item in carted.items():\n            db.inventory.update(\n                { '_id': item['_id'],\n                  'carted.cart_id': cart_id,\n                  'carted.qty': carted_item['qty'] },\n                { '$inc': { 'qty': carted_item['qty'] },\n                  '$pull': { 'carted': { 'cart_id': cart_id } } })\n\n\nTo summarize: This operation finds all “carted” items that have time\nstamps older than the threshold. Then, the process makes two passes\nover these items:\n\nOf the items with time stamps older than the threshold, if the cart\nis still active, it resets the time stamp to maintain the carts.\nOf the stale items that remain in inactive carts, the operation\nreturns these items to the inventory.\n\n\nNote\nThe function above is safe for use because it checks to ensure that\nthe cart has expired before returning items from the cart to\ninventory. However, it could be long-running and slow other updates\nand queries.\nUse judiciously.\n\n\n\n\nSharding¶\nIf you need to shard the data for this system, the _id\nfield is an ideal shard key for both carts and products\nbecause most update operations use the _id field. This allows\nmongos to route all updates that select on _id to a\nsingle mongod process.\nThere are two drawbacks for using _id as a shard key:\n\nIf the cart collection’s _id is an incrementing value, all new\ncarts end up on a single shard.\nYou can mitigate this effect by choosing a random value upon the\ncreation of a cart, such as a hash (i.e. MD5 or SHA-1) of an\nObjectID, as the _id. The process for this operation would\nresemble the following:\nimport hashlib\nimport bson\n\ncart_id = bson.ObjectId()\ncart_id_hash = hashlib.md5(str(cart_id)).hexdigest()\n\ncart = { \"_id\": cart_id, \"cart_hash\": cart_id_hash }\ndb.cart.insert(cart)\n\n\n\nCart expiration and inventory adjustment requires update operations\nand queries to broadcast to all shards when using _id as a shard\nkey.\nThis may be less relevant as the expiration functions run relatively\ninfrequently and you can queue them or artificially slow them down\n(as with judicious use of sleep())\nto minimize server load.\n\n\nUse the following commands in the Python/PyMongo console to shard the\ncart and inventory collections:\n>>> db.command('shardCollection', 'inventory'\n...            'key': { '_id': 1 } )\n{ \"collectionsharded\" : \"inventory\", \"ok\" : 1 }\n>>> db.command('shardCollection', 'cart')\n...            'key': { '_id': 1 } )\n{ \"collectionsharded\" : \"cart\", \"ok\" : 1 }\n\n\n\n\nAdditional Resources¶\nRetail Reference Architecture Part 1 (Blog Post)\n\n\n\n                \n    \n      ←  \n      Product Catalog\n      Category Hierarchy\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Inventory Management","preview":"On this page This case study provides an overview of practices and patterns for\ndesigning and developing the inventory management portions of an\nE-commerce application. See also “Product Catalog.” Cus","tags":""},{"slug":"use-cases/metadata-and-asset-management/","headings":[],"text":"Use Cases > \n          Metadata and Asset Management \n      \n    \n  \n                \n                  \nMetadata and Asset Management\n\nOn this page\n\nOverview\nOperations\nSharding\n\n\n\nOverview¶\nThis document describes the design and pattern of a content management\nsystem using MongoDB modeled on the popular Drupal\nCMS.\n\nProblem¶\nYou are designing a content management system (CMS) and you want to\nuse MongoDB to store the content of your sites.\n\n\nSolution¶\nTo build this system you will use MongoDB’s flexible schema to store\nall content “nodes” in a single collection regardless of type. This\nguide will provide prototype schema and describe common operations for\nthe following primary node types:\n\nBasic Page\nBasic pages are useful for displaying infrequently-changing text\nsuch as an ‘about’ page. With a basic page, the salient information\nis the title and the content.\nBlog entry\nBlog entries record a “stream” of posts from users on the CMS and\nstore title, author, content, and date as relevant information.\nPhoto\nPhotos participate in photo galleries, and store title,\ndescription, author, and date along with the actual photo binary\ndata.\n\nThis solution does not describe schema or process for storing or using\nnavigational and organizational information.\n\n\nSchema¶\nAlthough documents in the nodes collection\ncontain content of different types, all documents have a similar\nstructure and a set of common fields. Consider the following\nprototype document for a “basic page” node type:\n{\n    _id: ObjectId(…),\n    nonce: ObjectId(…),\n    metadata: {\n        type: 'basic-page'\n        section: 'my-photos',\n        slug: 'about',\n        title: 'About Us',\n        created: ISODate(...),\n        author: { _id: ObjectId(…), name: 'Rick' },\n        tags: [ ... ],\n        detail: { text: '# About Us\\n…' }\n    }\n}\n\n\nMost fields are descriptively titled. The section field identifies\ngroupings of items, as in a photo gallery, or a particular blog . The\nslug field holds a URL-friendly unique representation of the node,\nusually that is unique within its section for generating URLs.\nAll documents also have a detail field that varies with the\ndocument type. For the basic page above, the detail field might hold\nthe text of the page. For a blog entry, the detail field might\nhold a sub-document. Consider the following prototype:\n{\n    …\n    metadata: {\n        …\n        type: 'blog-entry',\n        section: 'my-blog',\n        slug: '2012-03-noticed-the-news',\n        …\n        detail: {\n            publish_on: ISODate(…),\n            text: 'I noticed the news from Washington today…'\n        }\n    }\n}\n\n\nPhotos require a different approach. Because photos can be potentially\nlarger than these documents, it’s important to separate the binary photo\nstorage from the nodes metadata.\nGridFS provides the ability to store larger files in MongoDB.\nGridFS stores data in two collections, in this case,\ncms.assets.files, which stores metadata, and cms.assets.chunks\nwhich stores the data itself. Consider the following prototype\ndocument from the cms.assets.files collection:\n{\n    _id: ObjectId(…),\n    length: 123...,\n    chunkSize: 262144,\n    uploadDate: ISODate(…),\n    contentType: 'image/jpeg',\n    md5: 'ba49a...',\n    metadata: {\n        nonce: ObjectId(…),\n        slug: '2012-03-invisible-bicycle',\n        type: 'photo',\n        section: 'my-album',\n        title: 'Kitteh',\n        created: ISODate(…),\n        author: { _id: ObjectId(…), name: 'Jared' },\n        tags: [ … ],\n        detail: {\n            filename: 'kitteh_invisible_bike.jpg',\n            resolution: [ 1600, 1600 ], … }\n    }\n}\n\n\n\nNote\nThis document embeds the basic node document fields, which allows\nyou to use the same code to manipulate nodes, regardless of type.\n\n\n\n\nOperations¶\nThis section outlines a number of common operations for building and\ninteracting with the metadata and asset layer of the cms for all node\ntypes.  All examples in this document use the Python programming\nlanguage and the PyMongo driver for\nMongoDB, but you can implement this system using any language you\nchoose.\n\nCreate and Edit Content Nodes¶\n\nProcedure¶\nThe most common operations inside of a CMS center on creating and\nediting content. Consider the following\ninsert()\noperation:\ndb.cms.nodes.insert({\n    'nonce': ObjectId(),\n    'metadata': {\n        'section': 'myblog',\n        'slug': '2012-03-noticed-the-news',\n        'type': 'blog-entry',\n        'title': 'Noticed in the News',\n        'created': datetime.utcnow(),\n        'author': { 'id': user_id, 'name': 'Rick' },\n        'tags': [ 'news', 'musings' ],\n        'detail': {\n            'publish_on': datetime.utcnow(),\n            'text': 'I noticed the news from Washington today…' }\n        }\n     })\n\n\nOnce inserted, your application must have some way of preventing\nmultiple concurrent updates. The schema uses the special nonce\nfield to help detect concurrent edits. By using the nonce field in\nthe query portion of the update\noperation, the application will generate an error if there is an\nediting collision. Consider the following update\ndef update_text(section, slug, nonce, text):\n    result = db.cms.nodes.update(\n        { 'metadata.section': section,\n          'metadata.slug': slug,\n          'nonce': nonce },\n        { '$set':{'metadata.detail.text': text, 'nonce': ObjectId() } },\n        w=1)\n    if not result['updatedExisting']:\n        raise ConflictError()\n\n\nYou may  also want to perform metadata edits to the item such as adding\ntags:\ndb.cms.nodes.update(\n    { 'metadata.section': section, 'metadata.slug': slug },\n    { '$addToSet': { 'tags': { '$each': [ 'interesting', 'funny' ] } } })\n\n\nIn this example the $addToSet operator will only add\nvalues to the tags field if they do not already exist in the\ntags array, there’s no need to supply or update the nonce.\n\n\nIndex Support¶\nTo support updates and queries on the metadata.section, and\nmetadata.slug, fields and to ensure that two editors don’t\ncreate two documents with the same section name or slug. Use the\nfollowing operation at the Python/PyMongo console:\n>>> db.cms.nodes.ensure_index([\n...    ('metadata.section', 1), ('metadata.slug', 1)], unique=True)\n\n\nThe unique=True option prevents two documents from colliding. If\nyou want an index to support queries on the above fields and the\nnonce field create the following index:\n>>> db.cms.nodes.ensure_index([\n...    ('metadata.section', 1), ('metadata.slug', 1), ('nonce', 1) ])\n\n\nHowever, in most cases, the first index will be sufficient to support\nthese operations.\n\n\n\nUpload a Photo¶\n\nProcedure¶\nTo update a photo object, use the following operation, which builds\nupon the basic update procedure:\ndef upload_new_photo(\n    input_file, section, slug, title, author, tags, details):\n    fs = GridFS(db, 'cms.assets')\n    with fs.new_file(\n        content_type='image/jpeg',\n        metadata=dict(\n            type='photo',\n            locked=datetime.utcnow(),\n            section=section,\n            slug=slug,\n            title=title,\n            created=datetime.utcnow(),\n            author=author,\n            tags=tags,\n            detail=detail)) as upload_file:\n        while True:\n            chunk = input_file.read(upload_file.chunk_size)\n            if not chunk: break\n            upload_file.write(chunk)\n    # unlock the file\n    db.assets.files.update(\n        {'_id': upload_file._id},\n        {'$set': { 'locked': None } } )\n\n\nBecause uploading the photo spans multiple documents and is a\nnon-atomic operation, you must “lock” the file during upload by\nwriting datetime.utcnow() in the\nrecord. This helps when there are multiple concurrent editors and lets\nthe application detect stalled file uploads. This operation assumes\nthat, for photo upload, the last update will succeed:\ndef update_photo_content(input_file, section, slug):\n    fs = GridFS(db, 'cms.assets')\n\n    # Delete the old version if it's unlocked or was locked more than 5\n    #    minutes ago\n    file_obj = db.cms.assets.find_one(\n        { 'metadata.section': section,\n          'metadata.slug': slug,\n          'metadata.locked': None })\n    if file_obj is None:\n        threshold = datetime.utcnow() - timedelta(seconds=300)\n        file_obj = db.cms.assets.find_one(\n            { 'metadata.section': section,\n              'metadata.slug': slug,\n              'metadata.locked': { '$lt': threshold } })\n    if file_obj is None: raise FileDoesNotExist()\n    fs.delete(file_obj['_id'])\n\n    # update content, keep metadata unchanged\n    file_obj['locked'] = datetime.utcnow()\n    with fs.new_file(**file_obj):\n        while True:\n            chunk = input_file.read(upload_file.chunk_size)\n            if not chunk: break\n            upload_file.write(chunk)\n    # unlock the file\n    db.assets.files.update(\n        {'_id': upload_file._id},\n        {'$set': { 'locked': None } } )\n\n\nAs with the basic operations, you can use a much more simple operation\nto edit the tags:\ndb.cms.assets.files.update(\n    { 'metadata.section': section, 'metadata.slug': slug },\n    { '$addToSet': { 'metadata.tags': { '$each': [ 'interesting', 'funny' ] } } })\n\n\n\n\nIndex Support¶\nCreate a unique index on { metadata.section: 1, metadata.slug: 1 }\nto support the above operations and prevent users from creating or\nupdating the same file concurrently. Use the following operation in\nthe Python/PyMongo console:\n>>> db.cms.assets.files.ensure_index([\n...    ('metadata.section', 1), ('metadata.slug', 1)], unique=True)\n\n\n\n\n\nLocate and Render a Node¶\nTo locate a node based on the value of metadata.section and\nmetadata.slug, use the following find_one operation.\nnode = db.nodes.find_one({'metadata.section': section, 'metadata.slug': slug })\n\n\n\nNote\nThe index defined (section, slug) created to support the\nupdate operation, is sufficient to support this operation as well.\n\n\n\nLocate and Render a Photo¶\nTo locate an image based on the value of metadata.section and\nmetadata.slug, use the following find_one operation.\nfs = GridFS(db, 'cms.assets')\nwith fs.get_version({'metadata.section': section, 'metadata.slug': slug }) as img_fpo:\n     # do something with the image file\n\n\n\nNote\nThe index defined (section, slug) created to support the\nupdate operation, is sufficient to support this operation as well.\n\n\n\nSearch for Nodes by Tag¶\n\nQuerying¶\nTo retrieve a list of nodes based on their tags, use the following\nquery:\nnodes = db.nodes.find({'metadata.tags': tag })\n\n\n\n\nIndexing¶\nCreate an index on the tags field in the cms.nodes collection,\nto support this query:\n>>> db.cms.nodes.ensure_index('tags')\n\n\n\n\n\nSearch for Images by Tag¶\n\nProcedure¶\nTo retrieve a list of images based on their tags, use the following\noperation:\nimage_file_objects = db.cms.assets.files.find({'metadata.tags': tag })\nfs = GridFS(db, 'cms.assets')\nfor image_file_object in db.cms.assets.files.find(\n    {'metadata.tags': tag }):\n    image_file = fs.get(image_file_object['_id'])\n    # do something with the image file\n\n\n\n\nIndexing¶\nCreate an index on the tags field in the cms.assets.files\ncollection, to support this query:\n>>> db.cms.assets.files.ensure_index('tags')\n\n\n\n\n\nGenerate a Feed of Recently Published Blog Articles¶\n\nQuerying¶\nUse the following operation to generate a list of recent blog posts\nsorted in descending order by date, for use on the index page of your\nsite, or in an .rss or .atom feed.\narticles = db.nodes.find({\n    'metadata.section': 'my-blog'\n    'metadata.published': { '$lt': datetime.utcnow() } })\narticles = articles.sort({'metadata.published': -1})\n\n\n\nNote\nIn many cases you will want to limit the number of nodes returned\nby this query.\n\n\n\nIndexing¶\nCreate a compound index on the\n{ metadata.section: 1, metadata.published: 1 }\nfields to support this query and sort operation.\n>>> db.cms.nodes.ensure_index(\n...     [ ('metadata.section', 1), ('metadata.published', -1) ])\n\n\n\nNote\nFor all sort or range queries, ensure that field with the sort or\nrange operation is the final field in the index.\n\n\n\n\n\nSharding¶\nIn a CMS, read performance is more critical than write performance. To\nachieve the best read performance in a sharded cluster, ensure\nthat the mongos can route queries to specific shards.\nAlso remember that MongoDB can not enforce unique indexes across\nshards. Using a compound shard key that consists of\nmetadata.section and metadata.slug, will provide the same\nsemantics as describe above.\n\nWarning\nConsider the actual use and workload of your cluster before\nconfiguring sharding for your cluster.\n\nUse the following operation at the Python/PyMongo shell:\n>>> db.command('shardCollection', 'cms.nodes', {\n...     key : { 'metadata.section': 1, 'metadata.slug' : 1 } })\n{ \"collectionsharded\": \"cms.nodes\", \"ok\": 1}\n>>> db.command('shardCollection', 'cms.assets.files', {\n...     key : { 'metadata.section': 1, 'metadata.slug' : 1 } })\n{ \"collectionsharded\": \"cms.assets.files\", \"ok\": 1}\n\n\nTo shard the cms.assets.chunks collection, you must use the\n_id field as the shard key. The following operation will\nshard the collection\n>>> db.command('shardCollection', 'cms.assets.chunks', {\n...     key : { 'files_id': 1 } })\n{ \"collectionsharded\": \"cms.assets.chunks\", \"ok\": 1}\n\n\nSharding on the files_id field ensures routable queries because all\nreads from GridFS must first look up the document in\ncms.assets.files and then look up the chunks separately.\n\n\n\n                \n    \n      ←  \n      Category Hierarchy\n      Storing Comments\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Metadata and Asset Management","preview":"On this page This document describes the design and pattern of a content management\nsystem using MongoDB modeled on the popular Drupal\nCMS. You are designing a content management system (CMS) and you","tags":""},{"slug":"use-cases/pre-aggregated-reports-mmapv1/","headings":[],"text":"Use Cases > \n          Pre-Aggregated Reports (MMAPv1) \n      \n    \n  \n                \n                  \nPre-Aggregated Reports (MMAPv1)\n\nOn this page\n\nOverview\nSchema\nOperations\nSharding\n\n\n\nOverview¶\n\nImportant\nFor MMAPv1 storage engine only.\n\nThis document outlines the basic patterns and principles for using\nMongoDB as an engine for collecting and processing events in real time\nfor use in generating up to the minute or second reports.\n\nProblem¶\nServers and other systems can generate a large number of documents,\nand it can be difficult to access and analyze such large collections\nof data originating from multiple servers.\nThis document makes the following assumptions about real-time\nanalytics:\n\nThere is no need to retain transactional event data in MongoDB, and\nhow your application handles transactions is outside of the scope of\nthis document.\nYou require up-to-the minute data, or up-to-the-second if possible.\nThe queries for ranges of data (by time) must be as fast as\npossible.\n\n\nSee also\n“Storing Log Data.”\n\n\n\nSolution¶\nThe solution described below assumes a simple scenario using data from\nweb server access logs. With this data, you will want to return the\nnumber of hits to a collection of web sites at various levels of\ngranularity based on time (i.e. by minute, hour, day, week, and month)\nas well as by the path of a resource.\nTo achieve the required performance to support these tasks,\nupserts and increment operations\nwill allow you to calculate statistics, produce simple range-based\nqueries, and generate filters to support time-series charts of\naggregated data.\n\n\n\nSchema¶\nSchemas for real-time analytics systems must support simple and fast\nquery and update operations. In particular, attempt to avoid the\nfollowing situations which can degrade performance:\n\ndocuments growing significantly after creation.\nWith MMAPv1, document growth forces MongoDB to move the document on\ndisk, which can be time and resource consuming relative to other\noperations;\n\nqueries requiring MongoDB to scan documents in the collection without\nusing indexes; and\n\ndeeply nested documents that make accessing particular fields slow.\n\n\nIntuitively, you may consider keeping “hit counts” in individual\ndocuments with one document for every unit of time (i.e. minute, hour,\nday, etc.) However, queries must return multiple documents for all\nnon-trivial time-range queries, which can slow overall query\nperformance.\nPreferably, to maximize query performance, use more complex documents,\nand keep several aggregate values in each document. The remainder of\nthis section outlines several schema designs that you may consider for\nthis real-time analytics system. While there is no single pattern for\nevery problem, each pattern is more well suited to specific classes of\nproblems.\n\nOne Document Per Page Per Day¶\nConsider the following example schema for a solution that stores all\nstatistics for a single day and page in a single document:\n{\n    _id: \"20101010/site-1/apache_pb.gif\",\n    metadata: {\n        date: ISODate(\"2000-10-10T00:00:00Z\"),\n        site: \"site-1\",\n        page: \"/apache_pb.gif\" },\n    daily: 5468426,\n    hourly: {\n        \"0\": 227850,\n        \"1\": 210231,\n        ...\n        \"23\": 20457 },\n    minute: {\n        \"0\": 3612,\n        \"1\": 3241,\n        ...\n        \"1439\": 2819 }\n}\n\n\nThis approach has a couple of advantages:\n\nFor every request on the website, you only need to update one\ndocument.\nReports for time periods within the day, for a single page\nrequire fetching a single document.\n\nThere are, however, significant issues with this approach. The most\nsignificant issue is that, as you upsert data into the\nhourly and minute fields, the document grows. Although\nMongoDB will pad the space allocated to documents, it will\nneed to reallocate these documents multiple times throughout the day,\nwhich impacts performance.\n\n\nPre-allocate Documents¶\n\nSimple Pre-Allocation¶\nTo mitigate the impact of repeated document migrations throughout the\nday, you can tweak the “one document per page per day” approach by adding a\nprocess that “pre-allocates” documents with fields that hold 0\nvalues throughout the previous day. Thus, at midnight, new documents\nwill exist.\n\nNote\nTo avoid situations where your application must pre-allocate large\nnumbers of documents at midnight, it’s best to create documents\nthroughout the previous day by upserting randomly\nwhen you update a value in the current day’s data.\nThis requires some tuning, to balance two requirements:\n\nyour application should have pre-allocated all or nearly all of\ndocuments by the end of the day.\nyour application should infrequently pre-allocate a document\nthat already exists to save time and resources on extraneous\nupserts.\n\nAs a starting point, consider the average number of hits a day\n(h), and then upsert a blank document upon update with a\nprobability of 1/h.\n\nPre-allocating increases performance by initializing all documents\nwith 0 values in all fields. After create, documents will never\ngrow. This means that:\n\nthere will be no need to migrate documents within the data store,\nwhich is a problem in the “one document per page per day” approach.\nMongoDB will not add padding to the records, which leads to a more\ncompact data representation and better memory use of your memory.\n\n\n\nAdd Intra-Document Hierarchy¶\n\nNote\nMongoDB stores BSON documents as a\nsequence of fields and values, not as a hash table. As a result,\nwriting to the field stats.mn.0 is considerably faster than\nwriting to stats.mn.1439.\n\n\n\nIn order to update the value in minute #1349, MongoDB must skip over all 1349\nentries before it.\n\nTo optimize update and insert operations you can introduce\nintra-document hierarchy. In particular, you can split the minute\nfield up into 24 hourly fields:\n{\n    _id: \"20101010/site-1/apache_pb.gif\",\n    metadata: {\n        date: ISODate(\"2000-10-10T00:00:00Z\"),\n        site: \"site-1\",\n        page: \"/apache_pb.gif\" },\n    daily: 5468426,\n    hourly: {\n        \"0\": 227850,\n        \"1\": 210231,\n        ...\n        \"23\": 20457 },\n    minute: {\n        \"0\": {\n            \"0\": 3612,\n            \"1\": 3241,\n            ...\n            \"59\": 2130 },\n        \"1\": {\n        \"60\": ... ,\n        },\n        ...\n        \"23\": {\n            ...\n            \"1439\": 2819 }\n    }\n}\n\n\nThis allows MongoDB to “skip forward” throughout the day when updating\nthe minute data, which makes the update performance more uniform and\nfaster later in the day.\n\n\nTo update the value in minute #1349, MongoDB first skips the first\n23 hours and then skips 59 minutes for only 82 skips as opposed to\n1439 skips in the previous schema.\n\n\n\n\nSeparate Documents by Granularity Level¶\nPre-allocating documents is a\nreasonable design for storing intra-day data, but the model breaks\ndown when displaying data over longer multi-day periods like months or\nquarters. In these cases, consider storing daily statistics in a\nsingle document as above, and then aggregate monthly data into a\nseparate document.\nThis introduces a second set of upsert operations to the data\ncollection and aggregation portion of your application but the\nreduction in disk reads on the queries should be worth the\ncosts. Consider the following example schema:\n\nDaily Statistics\n{\n    _id: \"20101010/site-1/apache_pb.gif\",\n    metadata: {\n        date: ISODate(\"2000-10-10T00:00:00Z\"),\n        site: \"site-1\",\n        page: \"/apache_pb.gif\" },\n    hourly: {\n        \"0\": 227850,\n        \"1\": 210231,\n        ...\n        \"23\": 20457 },\n    minute: {\n        \"0\": {\n            \"0\": 3612,\n            \"1\": 3241,\n            ...\n            \"59\": 2130 },\n        \"1\": {\n            \"0\": ...,\n        },\n        ...\n        \"23\": {\n            \"59\": 2819 }\n    }\n}\n\n\n\nMonthly Statistics\n{\n    _id: \"201010/site-1/apache_pb.gif\",\n    metadata: {\n        date: ISODate(\"2000-10-00T00:00:00Z\"),\n        site: \"site-1\",\n        page: \"/apache_pb.gif\" },\n    daily: {\n        \"1\": 5445326,\n        \"2\": 5214121,\n        ... }\n}\n\n\n\n\n\n\n\nOperations¶\nThis section outlines a number of common operations for building and\ninteracting with real-time-analytics reporting systems. The major\nchallenge is in balancing performance and write (i.e. upsert)\nperformance. All examples in this document use the Python programming\nlanguage and the PyMongo driver for\nMongoDB, but you can implement this system using any language you\nchoose.\n\nLog an Event¶\nLogging an event such as a page request (i.e. “hit”) is the main\n“write” activity for your system. To maximize performance, you’ll be\ndoing in-place updates with the upsert operation. Consider the\nfollowing example:\nfrom datetime import datetime, time\n\ndef log_hit(db, dt_utc, site, page):\n\n    # Update daily stats doc\n    id_daily = dt_utc.strftime('%Y%m%d/') + site + page\n    hour = dt_utc.hour\n    minute = dt_utc.minute\n\n    # Get a datetime that only includes date info\n    d = datetime.combine(dt_utc.date(), time.min)\n    query = {\n        '_id': id_daily,\n        'metadata': { 'date': d, 'site': site, 'page': page } }\n    update = { '$inc': {\n            'hourly.%d' % (hour,): 1,\n            'minute.%d.%d' % (hour,minute): 1 } }\n    db.stats.daily.update(query, update, upsert=True)\n\n    # Update monthly stats document\n    id_monthly = dt_utc.strftime('%Y%m/') + site + page\n    day_of_month = dt_utc.day\n    query = {\n        '_id': id_monthly,\n        'metadata': {\n            'date': d.replace(day=1),\n            'site': site,\n            'page': page } }\n    update = { '$inc': {\n            'daily.%d' % day_of_month: 1} }\n    db.stats.monthly.update(query, update, upsert=True)\n\n\nThe upsert operation (i.e. upsert=True) performs an update if the\ndocument exists, and an insert if the document does not exist.\n\nNote\nThis application requires upserts, because the pre-allocation method only pre-allocates new\ndocuments with a high probability, not with complete certainty.\nWithout preallocation, you end up with a dynamically growing\ndocument, slowing upserts as MongoDB moves documents to accommodate\ngrowth.\n\n\n\nPre-allocate¶\nTo prevent document growth, you can preallocate new documents before\nthe system needs them. As you create new documents, set all values to\n0 for so that documents will not grow to accommodate\nupdates. Consider the following preallocate() function:\ndef preallocate(db, dt_utc, site, page):\n\n    # Get id values\n    id_daily = dt_utc.strftime('%Y%m%d/') + site + page\n    id_monthly = dt_utc.strftime('%Y%m/') + site + page\n\n    # Get daily metadata\n    daily_metadata = {\n        'date': datetime.combine(dt_utc.date(), time.min),\n        'site': site,\n        'page': page }\n    # Get monthly metadata\n    monthly_metadata = {\n        'date': daily_metadata['date'].replace(day=1),\n        'site': site,\n        'page': page }\n\n    # Initial zeros for statistics\n    hourly = dict((str(i), 0) for i in range(24))\n    minute = dict(\n        (str(i), dict((str(j), 0) for j in range(60)))\n        for i in range(24))\n    daily = dict((str(i), 0) for i in range(1, 32))\n\n    # Perform upserts, setting metadata\n    db.stats.daily.update(\n        {\n            '_id': id_daily,\n            'hourly': hourly,\n            'minute': minute},\n        { '$set': { 'metadata': daily_metadata }},\n        upsert=True)\n    db.stats.monthly.update(\n        {\n            '_id': id_monthly,\n            'daily': daily },\n        { '$set': { 'm': monthly_metadata }},\n        upsert=True)\n\n\nThe function pre-allocated both the monthly and daily documents at\nthe same time. The performance benefits from separating these\noperations are negligible, so it’s reasonable to keep both operations\nin the same function.\nIdeally, your application should pre-allocate documents before\nneeding to write data to maintain consistent update\nperformance. Additionally, its important to avoid causing a spike in\nactivity and latency by creating documents all at once.\nIn the following example, document updates (i.e. “log_hit()”) will\nalso pre-allocate a document probabilistically. However, by “tuning\nprobability,” you can limit redundant preallocate() calls.\nfrom random import random\nfrom datetime import datetime, timedelta, time\n\n# Example probability based on 500k hits per day per page\nprob_preallocate = 1.0 / 500000\n\ndef log_hit(db, dt_utc, site, page):\n    if random.random() < prob_preallocate:\n        preallocate(db, dt_utc + timedelta(days=1), site, page)\n    # Update daily stats doc\n    ...\n\n\nUsing this method, there will be a high probability that each document\nwill already exist before your application needs to issue update\noperations. You’ll also be able to prevent a regular spike in activity\nfor pre-allocation, and be able to eliminate document growth.\n\n\nRetrieving Data for a Real-Time Chart¶\nThis example describes fetching the data from the above MongoDB\nsystem, for use in generating a chart that displays the number of hits\nto a particular resource over the last hour.\n\nQuerying¶\nUse the following query in a find_one operation at the\nPython/PyMongo console to retrieve the number of hits to a specific\nresource (i.e. /index.html) with minute-level granularity:\n>>> db.stats.daily.find_one(\n...     {'metadata': {'date':dt, 'site':'site-1', 'page':'/index.html'}},\n...     { 'minute': 1 })\n\n\nUse the following query to retrieve the number of hits to a resource\nover the last day, with hour-level granularity:\n>>> db.stats.daily.find_one(\n...     {'metadata': {'date':dt, 'site':'site-1', 'page':'/foo.gif'}},\n...     { 'hourly': 1 })\n\n\nIf you want a few days of hourly data, you can use a query in the\nfollowing form:\n>>> db.stats.daily.find(\n...     {\n...         'metadata.date': { '$gte': dt1, '$lte': dt2 },\n...         'metadata.site': 'site-1',\n...         'metadata.page': '/index.html'},\n...     { 'metadata.date': 1, 'hourly': 1 } },\n...     sort=[('metadata.date', 1)])\n\n\n\n\nIndexing¶\nTo support these query operation, create a compound index on the\nfollowing daily statistics fields: metadata.site,\nmetadata.page, and metadata.date (in that order.) Use the\nfollowing operation at the Python/PyMongo console.\n>>> db.stats.daily.ensure_index([\n...     ('metadata.site', 1),\n...     ('metadata.page', 1),\n...     ('metadata.date', 1)])\n\n\nThis index makes it possible to efficiently run the query for multiple\ndays of hourly data. At the same time, any compound index on page and\ndate, will allow you to query efficiently for a single day’s\nstatistics.\n\n\n\nGet Data for a Historical Chart¶\n\nQuerying¶\nTo retrieve daily data for a single month, use the following query:\n>>> db.stats.monthly.find_one(\n...     {'metadata':\n...         {'date':dt,\n...         'site': 'site-1',\n...         'page':'/index.html'}},\n...     { 'daily': 1 })\n\n\nTo retrieve several months of daily data, use a variation on the above\nquery:\n>>> db.stats.monthly.find(\n...     {\n...         'metadata.date': { '$gte': dt1, '$lte': dt2 },\n...         'metadata.site': 'site-1',\n...         'metadata.page': '/index.html'},\n...     { 'metadata.date': 1, 'daily': 1 } },\n...     sort=[('metadata.date', 1)])\n\n\n\n\nIndexing¶\nCreate the following index to support these queries for monthly data\non the metadata.site, metadata.page, and\nmetadata.date fields:\n>>> db.stats.monthly.ensure_index([\n...     ('metadata.site', 1),\n...     ('metadata.page', 1),\n...     ('metadata.date', 1)])\n\n\nThis field order will efficiently support range queries for a single\npage over several months.\n\n\n\n\nSharding¶\nThe only potential limits on the performance of this system are the\nnumber of shards in your system, and the shard key that you use.\n\nAn ideal shard key will distribute upserts between\nthe shards while routing all queries to a single shard, or a small\nnumber of shards.\nWhile your choice of shard key may depend on the precise workload of\nyour deployment, consider using { metadata.site: 1, metadata.page:\n1 } as a shard key. The combination of site and page (or\nevent) will lead to a well balanced cluster for most deployments.\nEnable sharding for the daily statistics collection with the following\nshardCollection command in the Python/PyMongo console:\n>>> db.command('shardCollection', 'stats.daily', {\n...     key : { 'metadata.site': 1, 'metadata.page' : 1 } })\n\n\nUpon success, you will see the following response:\n{ \"collectionsharded\" : \"stats.daily\", \"ok\" : 1 }\n\n\nEnable sharding for the monthly statistics collection with the\nfollowing shardCollection command in the Python/PyMongo\nconsole:\n>>> db.command('shardCollection', 'stats.monthly', {\n...     key : { 'metadata.site': 1, 'metadata.page' : 1 } })\n\n\nUpon success, you will see the following response:\n{ \"collectionsharded\" : \"stats.monthly\", \"ok\" : 1 }\n\n\nOne downside of the { metadata.site: 1, metadata.page: 1 }\nshard key is: if one page dominates all your traffic, all\nupdates to that page will go to a single shard. This is basically\nunavoidable, since all updates for a single page are going to a single\ndocument.\nYou may wish to include the date in addition to the site and page\nfields. If you include the date, MongoDB can split the histories and serve\ndifferent historical ranges with different shards. Use the following\nshardCollection command to shard the daily statistics\ncollection in the Python/PyMongo console:\n>>> db.command('shardCollection', 'stats.daily', {\n...     'key':{'metadata.site':1,'metadata.page':1,'metadata.date':1}})\n{ \"collectionsharded\" : \"stats.daily\", \"ok\" : 1 }\n\n\nEnable sharding for the monthly statistics collection with the\nfollowing shardCollection command in the Python/PyMongo\nconsole:\n>>> db.command('shardCollection', 'stats.monthly', {\n...     'key':{'metadata.site':1,'metadata.page':1,'metadata.date':1}})\n{ \"collectionsharded\" : \"stats.monthly\", \"ok\" : 1 }\n\n\n\nNote\nDetermine your actual requirements and load before deciding to\nshard. In many situations a single MongoDB instance may be able to\nkeep track of all events and pages.\n\n\n\n\n                \n    \n      ←  \n      Storing Log Data\n      Hierarchical Aggregation\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Pre-Aggregated Reports (MMAPv1)","preview":"On this page Important For MMAPv1 storage engine only. This document outlines the basic patterns and principles for using\nMongoDB as an engine for collecting and processing events in real time\nfor use","tags":""},{"slug":"use-cases/product-catalog/","headings":[],"text":"Use Cases > \n          Product Catalog \n      \n    \n  \n                \n                  \nProduct Catalog\n\nOn this page\n\nOverview\nOperations\nScaling\nAdditional Resources\n\n\n\nOverview¶\nThis document describes the basic patterns and principles for\ndesigning an E-Commerce product catalog system using MongoDB as a\nstorage engine.\n\nProblem¶\nProduct catalogs must have the capacity to store many differed types\nof objects with different sets of attributes. These kinds of data\ncollections are quite compatible with MongoDB’s data model, but many\nimportant considerations and design decisions remain.\n\n\nSolution¶\nFor relational databases, there are several solutions that address\nthis problem, each with a different performance profile. This section\nexamines several of these options and then describes the preferred\nMongoDB solution.\n\n\nSQL and Relational Data Models¶\n\nConcrete Table Inheritance¶\nOne approach, in a relational model, is to create a table for each\nproduct category. Consider the following example SQL statement for\ncreating database tables:\nCREATE TABLE `product_audio_album` (\n    `sku` char(8) NOT NULL,\n    ...\n    `artist` varchar(255) DEFAULT NULL,\n    `genre_0` varchar(255) DEFAULT NULL,\n    `genre_1` varchar(255) DEFAULT NULL,\n    ...,\n    PRIMARY KEY(`sku`))\n...\nCREATE TABLE `product_film` (\n    `sku` char(8) NOT NULL,\n    ...\n    `title` varchar(255) DEFAULT NULL,\n    `rating` char(8) DEFAULT NULL,\n    ...,\n    PRIMARY KEY(`sku`))\n...\n\n\nThis approach has limited flexibility for two key reasons:\n\nYou must create a new table for every new category of products.\nYou must explicitly tailor all queries for the exact type of\nproduct.\n\n\n\nSingle Table Inheritance¶\nAnother relational data model uses a single table for all product\ncategories and adds new columns anytime you need to store data\nregarding a new type of product. Consider the following SQL statement:\nCREATE TABLE `product` (\n    `sku` char(8) NOT NULL,\n    ...\n    `artist` varchar(255) DEFAULT NULL,\n    `genre_0` varchar(255) DEFAULT NULL,\n    `genre_1` varchar(255) DEFAULT NULL,\n    ...\n    `title` varchar(255) DEFAULT NULL,\n    `rating` char(8) DEFAULT NULL,\n    ...,\n    PRIMARY KEY(`sku`))\n\n\nThis approach is more flexible than concrete table inheritance: it\nallows single queries to span different product types, but at the\nexpense of space.\n\n\nMultiple Table Inheritance¶\nAlso in the relational model, you may use a “multiple table\ninheritance” pattern to represent common attributes in a generic\n“product” table, with some variations in individual category product\ntables. Consider the following SQL statement:\nCREATE TABLE `product` (\n    `sku` char(8) NOT NULL,\n    `title` varchar(255) DEFAULT NULL,\n    `description` varchar(255) DEFAULT NULL,\n    `price`, ...\n    PRIMARY KEY(`sku`))\n\nCREATE TABLE `product_audio_album` (\n    `sku` char(8) NOT NULL,\n    ...\n    `artist` varchar(255) DEFAULT NULL,\n    `genre_0` varchar(255) DEFAULT NULL,\n    `genre_1` varchar(255) DEFAULT NULL,\n    ...,\n    PRIMARY KEY(`sku`),\n    FOREIGN KEY(`sku`) REFERENCES `product`(`sku`))\n...\nCREATE TABLE `product_film` (\n    `sku` char(8) NOT NULL,\n    ...\n    `title` varchar(255) DEFAULT NULL,\n    `rating` char(8) DEFAULT NULL,\n    ...,\n    PRIMARY KEY(`sku`),\n    FOREIGN KEY(`sku`) REFERENCES `product`(`sku`))\n...\n\n\nMultiple table inheritance is more space-efficient than single\ntable inheritance and\nsomewhat more flexible than concrete table inheritance.  However, this model\ndoes require an expensive JOIN operation to obtain all relevant\nattributes relevant to a product.\n\n\nEntity Attribute Values¶\nThe final substantive pattern from relational modeling is the\nentity-attribute-value schema where you would create a meta-model for\nproduct data. In this approach, you maintain a table with three\ncolumns, e.g. entity_id, attribute_id, value, and these\ntriples describe each product.\nConsider the description of an audio recording. You may have a series\nof rows representing the following relationships:\n\n\n\n\n\n\n\nEntity\nAttribute\nValue\n\n\n\nsku_00e8da9b\ntype\nAudio Album\n\nsku_00e8da9b\ntitle\nA Love Supreme\n\nsku_00e8da9b\n…\n…\n\nsku_00e8da9b\nartist\nJohn Coltrane\n\nsku_00e8da9b\ngenre\nJazz\n\nsku_00e8da9b\ngenre\nGeneral\n\n…\n…\n…\n\n\n\nThis schema is totally flexible:\n\nany entity can have any set of any attributes.\nNew product categories do not require any changes to the data model in the database.\n\nThe downside for these models, is that all nontrivial queries require\nlarge numbers of JOIN operations that results in large performance\npenalties.\n\n\nAvoid Modeling Product Data¶\nAdditionally some e-commerce solutions with relational database\nsystems avoid choosing one of the data models above, and serialize\nall of this data into a BLOB column. While simple, the details\nbecome difficult to access for search and sort.\n\n\nNon-Relational Data Model¶\nBecause MongoDB is a non-relational database, the data model for your\nproduct catalog can benefit from this additional flexibility. The best\nmodels use a single MongoDB collection to store all the product data,\nwhich is similar to the single table inheritance relational model. MongoDB’s dynamic schema\nmeans that each document need not conform to the same\nschema. As a result, the document for each product only needs to\ncontain attributes relevant to that product.\n\n\n\nSchema¶\nAt the beginning of the document, the schema must contain general\nproduct information, to facilitate searches of the entire\ncatalog. Then, a details sub-document that contains fields that\nvary between product types. Consider the following example document\nfor an album product.\n{\n  sku: \"00e8da9b\",\n  type: \"Audio Album\",\n  title: \"A Love Supreme\",\n  description: \"by John Coltrane\",\n  asin: \"B0000A118M\",\n\n  shipping: {\n    weight: 6,\n    dimensions: {\n      width: 10,\n      height: 10,\n      depth: 1\n    },\n  },\n\n  pricing: {\n    list: 1200,\n    retail: 1100,\n    savings: 100,\n    pct_savings: 8\n  },\n\n  details: {\n    title: \"A Love Supreme [Original Recording Reissued]\",\n    artist: \"John Coltrane\",\n    genre: [ \"Jazz\", \"General\" ],\n        ...\n    tracks: [\n      \"A Love Supreme Part I: Acknowledgement\",\n      \"A Love Supreme Part II - Resolution\",\n      \"A Love Supreme, Part III: Pursuance\",\n      \"A Love Supreme, Part IV-Psalm\"\n    ],\n  },\n}\n\n\nA movie item would have the same fields for general product\ninformation, shipping, and pricing, but have different details\nsub-document. Consider the following:\n{\n  sku: \"00e8da9d\",\n  type: \"Film\",\n  ...,\n  asin: \"B000P0J0AQ\",\n\n  shipping: { ... },\n\n  pricing: { ... },\n\n  details: {\n    title: \"The Matrix\",\n    director: [ \"Andy Wachowski\", \"Larry Wachowski\" ],\n    writer: [ \"Andy Wachowski\", \"Larry Wachowski\" ],\n    ...,\n    aspect_ratio: \"1.66:1\"\n  },\n}\n\n\n\nNote\nIn MongoDB, you can have fields that hold multiple values\n(i.e. arrays) without any restrictions on the number of fields or\nvalues (as with genre_0 and genre_1) and also without\nthe need for a JOIN operation.\n\n\n\n\nOperations¶\nFor most deployments the primary use of the product catalog is to\nperform search operations. This section provides an overview of\nvarious types of queries that may be useful for supporting an e-commerce\nsite. All examples in this document use the Python programming\nlanguage and the PyMongo driver for\nMongoDB, but you can implement this system using any language you\nchoose.\n\nFind Albums by Genre and Sort by Year Produced¶\n\nQuerying¶\nThis query returns the documents for the products of a specific genre,\nsorted in reverse chronological order:\nquery = db.products.find({'type':'Audio Album',\n                          'details.genre': 'Jazz'})\nquery = query.sort([('details.issue_date', -1)])\n\n\n\n\nIndexing¶\nTo support this query, create a compound index on all the properties\nused in the filter and in the sort:\ndb.products.ensure_index([\n    ('type', 1),\n    ('details.genre', 1),\n    ('details.issue_date', -1)])\n\n\n\nNote\nThe final component of the index is the sort field. This allows\nMongoDB to traverse the index in the sorted order to preclude a\nslow in-memory sort.\n\n\n\n\nFind Products Sorted by Percentage Discount Descending¶\nWhile most searches will be for a particular type of product (e.g\nalbum, movie, etc.,) in some situations you may want to return all\nproducts in a certain price range, or discount percentage.\n\nQuerying¶\nTo return this data use the pricing information that exists in all\nproducts to find the products with the highest percentage discount:\nquery = db.products.find( { 'pricing.pct_savings': {'$gt': 25 })\nquery = query.sort([('pricing.pct_savings', -1)])\n\n\n\n\nIndexing¶\nTo support this type of query, you will want to create an index on the\npricing.pct_savings field:\ndb.products.ensure_index('pricing.pct_savings')\n\n\nSince MongoDB can read indexes in ascending or descending order, the\norder of the index does not matter.\n\nNote\nIf you want to perform range queries (e.g. “return all products\nover $25”) and then sort by another property like pricing.retail,\nMongoDB cannot use the index as effectively in this situation.\nThe field that you want to select a range, or perform sort\noperations, must be the last field in a compound index in order\nto avoid scanning an entire collection. Using different properties\nwithin a single combined range query and sort operation requires\nsome scanning which will limit the speed of your query.\n\n\n\n\nFind Movies Based on Starring Actor¶\n\nQuerying¶\nUse the following query to select documents within the details of a\nspecified product type (i.e. Film) of product (a movie) to find\nproducts that contain a certain value (i.e. a specific actor in the\ndetails.actor field,) with the results sorted by date descending:\nquery = db.products.find({'type': 'Film',\n                          'details.actor': 'Keanu Reeves'})\nquery = query.sort([('details.issue_date', -1)])\n\n\n\n\nIndexing¶\nTo support this query, you may want to create the following index.\ndb.products.ensure_index([\n    ('type', 1),\n    ('details.actor', 1),\n    ('details.issue_date', -1)])\n\n\nThis index begins with the type field and then narrows by the\nother search field, where the final component of the index is the sort\nfield to maximize index efficiency.\n\n\n\nFind Movies with a Particular Word in the Title¶\nRegardless of database engine, in order to retrieve this information\nthe system will need to scan some number of documents or records to\nsatisfy this query.\n\nQuerying¶\nMongoDB supports regular expressions within queries. In Python, you\ncan use the “re” module to construct the query:\nimport re\nre_hacker = re.compile(r'.*hacker.*', re.IGNORECASE)\n\nquery = db.products.find({'type': 'Film', 'title': re_hacker})\nquery = query.sort([('details.issue_date', -1)])\n\n\nMongoDB provides a special syntax for regular expression queries\nwithout the need for the re module. Consider the following\nalternative which is equivalent to the above example:\nquery = db.products.find({\n    'type': 'Film',\n    'title': {'$regex': '.*hacker.*', '$options':'i'}})\nquery = query.sort([('details.issue_date', -1)])\n\n\nThe $options operator specifies a case\ninsensitive match.\n\n\nIndexing¶\nThe indexing strategy for these kinds of queries is different from\nprevious attempts. Here, create an index on { type: 1,\ndetails.issue_date: -1, title: 1 } using the following command at\nthe Python/PyMongo console:\ndb.products.ensure_index([\n    ('type', 1),\n    ('details.issue_date', -1),\n    ('title', 1)])\n\n\nThis index makes it possible to avoid scanning whole documents by\nusing the index for scanning the title rather than forcing MongoDB to\nscan whole documents for the title field. Additionally, to support the\nsort on the details.issue_date field, by placing this field\nbefore the title field, ensures that the result set is already\nordered before MongoDB filters title field.\n\n\n\n\nScaling¶\n\nSharding¶\nDatabase performance for these kinds of deployments are dependent on\nindexes. You may use sharding to enhance performance by\nallowing MongoDB to keep larger portions of those indexes in RAM. In\nsharded configurations, select a shard key that allows\nmongos to route queries directly to a single shard or small\ngroup of shards.\nSince most of the queries in this system include the type field,\ninclude this in the shard key. Beyond this, the remainder of the shard\nkey is difficult to predict without information about your database’s\nactual activity and distribution. Consider that:\n\ndetails.issue_date would be a poor addition to the shard key\nbecause, although it appears in a number of queries, no queries\nwere selective by this field.\nyou should include one or more fields in the detail document\nthat you query frequently, and a field that has quasi-random\nfeatures, to prevent large unsplittable chunks.\n\nIn the following example, assume that the details.genre field is\nthe second-most queried field after type. Enable sharding using\nthe following shardCollection operation at the\nPython/PyMongo console:\n>>> db.command('shardCollection', 'product', {\n...     key : { 'type': 1, 'details.genre' : 1, 'sku':1 } })\n{ \"collectionsharded\" : \"details.genre\", \"ok\" : 1 }\n\n\n\nNote\nEven if you choose a “poor” shard key that requires\nmongos to broadcast all to all shards, you will still\nsee  some benefits from sharding, because:\n\nSharding makes a larger amount of memory\navailable to store indexes, and\nMongoDB will parallelize queries across shards, reducing\nlatency.\n\n\n\n\nRead Preference¶\nWhile sharding is the best way to scale operations, some data\nsets make it impossible to partition data so that mongos\ncan route queries to specific shards. In these situations\nmongos sends the query to all shards and then combines the\nresults before returning to the client.\nIn these situations, you can add additional read performance by\nallowing mongos to read from the secondary\ninstances in a replica set by configuring read\npreference in your client. Read preference is configurable on a\nper-connection or per-operation basis. In PyMongo, set the read_preference argument.\nThe\nSECONDARY\nproperty in the following example, permits reads from a\nsecondary (as well as a primary) for the entire connection .\nconn = pymongo.MongoClient(read_preference=pymongo.SECONDARY)\n\n\nConversely, the\nSECONDARY_ONLY\nread preference means that the\nclient will only send read operation only to the secondary member\nconn = pymongo.MongoClient(read_preference=pymongo.SECONDARY_ONLY)\n\n\nYou can also specify\nread_preference\nfor specific queries, as follows:\nresults = db.product.find(..., read_preference=pymongo.SECONDARY)\n\n\nor\nresults = db.product.find(..., read_preference=pymongo.SECONDARY_ONLY)\n\n\n\nSee also\nReplica Set Read Preference\n\n\n\n\nAdditional Resources¶\nRetail Reference Architecture Part 1 (Blog Post)\n\n\n\n                \n    \n      ←  \n      Hierarchical Aggregation\n      Inventory Management\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Product Catalog","preview":"On this page This document describes the basic patterns and principles for\ndesigning an E-Commerce product catalog system using MongoDB as a\nstorage engine. Product catalogs must have the capacity to","tags":""},{"slug":"use-cases/storing-comments/","headings":[],"text":"Use Cases > \n          Storing Comments \n      \n    \n  \n                \n                  \nStoring Comments\n\nOn this page\n\nOverview\nOne Document per Comment\nEmbedding All Comments\nHybrid Schema Design\nSharding\n\n\nThis document outlines the basic patterns for storing user-submitted\ncomments in a content management system (CMS.)\n\nOverview¶\nMongoDB provides a number of different approaches for storing data like\nusers-comments on content from a CMS. There is no correct\nimplementation, but there are a number of common approaches and known\nconsiderations for each approach. This case study explores the\nimplementation details and trade offs of each option. The three basic\npatterns are:\n\nStore each comment in its own document.\nThis approach provides the greatest flexibility at the expense of\nsome additional application level complexity.\nThese implementations make it possible to display comments in\nchronological or threaded order, and place no restrictions on the\nnumber of comments attached to a specific object.\n\nEmbed all comments in the “parent” document.\nThis approach provides the greatest possible performance for\ndisplaying comments at the expense of flexibility: the structure of\nthe comments in the document controls the display format.\n\nNote\nBecause of the limit on document size,\ndocuments, including the original content and all comments,\ncannot grow beyond 16 megabytes.\n\n\nA hybrid design, stores comments separately from the “parent,” but\naggregates comments into a small number of documents, where each\ncontains many comments.\n\n\nAlso consider that comments can be threaded, where comments are\nalways replies to “parent” item or to another comment, which carries\ncertain architectural requirements discussed below.\n\n\nOne Document per Comment¶\n\nSchema¶\nIf you store each comment in its own document, the documents in your\ncomments collection, would have the following structure:\n{\n    _id: ObjectId(...),\n    discussion_id: ObjectId(...),\n    slug: '34db',\n    posted: ISODateTime(...),\n    author: {\n              id: ObjectId(...),\n              name: 'Rick'\n             },\n    text: 'This is so bogus ... '\n}\n\n\nThis form is only suitable for displaying comments in chronological\norder. Comments store:\n\nthe discussion_id field that references the discussion parent,\na URL-compatible slug identifier,\na posted timestamp,\nan author sub-document that contains a reference to a user’s\nprofile in the id field and their name in the name field,\nand\nthe full text of the comment.\n\nTo support threaded comments, you might use a slightly different\nstructure like the following:\n{\n    _id: ObjectId(...),\n    discussion_id: ObjectId(...),\n    parent_id: ObjectId(...),\n    slug: '34db/8bda'\n    full_slug: '2012.02.08.12.21.08:34db/2012.02.09.22.19.16:8bda',\n    posted: ISODateTime(...),\n    author: {\n              id: ObjectId(...),\n              name: 'Rick'\n             },\n    text: 'This is so bogus ... '\n}\n\n\nThis structure:\n\nadds a parent_id field that stores the contents of the _id\nfield of the parent comment,\nmodifies the slug field to hold a path composed of the parent or\nparent’s slug and this comment’s unique slug, and\nadds a full_slug field that combines the slugs and time information\nto make it easier to sort documents in a threaded discussion by date.\n\n\nWarning\nMongoDB can only index 1024 bytes. This\nincludes all field data, the field name, and the namespace\n(i.e. database name and collection name.) This may become an issue\nwhen you create an index of the full_slug field to support\nsorting.\n\n\n\nOperations¶\nThis section contains an overview of common operations for interacting\nwith comments represented using a schema where each comment is its own\ndocument.\nAll examples in this document use the Python programming language and\nthe PyMongo driver for MongoDB, but\nyou can implement this system using any language you choose. Issue the\nfollowing commands at the interactive Python shell to load the\nrequired libraries:\n>>> import bson\n>>> import pymongo\n\n\n\nPost a New Comment¶\nTo post a new comment in a chronologically ordered (i.e. without\nthreading) system, use the following insert()\noperation:\nslug = generate_pseudorandom_slug()\ndb.comments.insert({\n    'discussion_id': discussion_id,\n    'slug': slug,\n    'posted': datetime.utcnow(),\n    'author': author_info,\n    'text': comment_text })\n\n\nTo insert a comment for a system with threaded comments, you must\ngenerate the slug path and full_slug at insert. See the\nfollowing operation:\nposted = datetime.utcnow()\n\n# generate the unique portions of the slug and full_slug\nslug_part = generate_pseudorandom_slug()\nfull_slug_part = posted.strftime('%Y.%m.%d.%H.%M.%S') + ':' + slug_part\n# load the parent comment (if any)\nif parent_slug:\n    parent = db.comments.find_one(\n        {'discussion_id': discussion_id, 'slug': parent_slug })\n    slug = parent['slug'] + '/' + slug_part\n    full_slug = parent['full_slug'] + '/' + full_slug_part\nelse:\n    slug = slug_part\n    full_slug = full_slug_part\n\n# actually insert the comment\ndb.comments.insert({\n    'discussion_id': discussion_id,\n    'slug': slug,\n    'full_slug': full_slug,\n    'posted': posted,\n    'author': author_info,\n    'text': comment_text })\n\n\n\n\nView Paginated Comments¶\nTo view comments that are not threaded, select all comments\nparticipating in a discussion and sort by the posted field. For\nexample:\ncursor = db.comments.find({'discussion_id': discussion_id})\ncursor = cursor.sort('posted')\ncursor = cursor.skip(page_num * page_size)\ncursor = cursor.limit(page_size)\n\n\nBecause the full_slug field contains both hierarchical information\n(via the path) and chronological information, you can use a simple\nsort on the full_slug field to retrieve a threaded view:\ncursor = db.comments.find({'discussion_id': discussion_id})\ncursor = cursor.sort('full_slug')\ncursor = cursor.skip(page_num * page_size)\ncursor = cursor.limit(page_size)\n\n\n\nSee also\ncursor.limit, cursor.skip, and\ncursor.sort\n\n\n\nIndexing¶\nTo support the above queries  efficiently, maintain\ntwo compound indexes, on:\n\n(``discussion_id, posted)“ and\n(``discussion_id, full_slug)“\n\nIssue the following operation at the interactive Python shell.\n>>> db.comments.ensure_index([\n...    ('discussion_id', 1), ('posted', 1)])\n>>> db.comments.ensure_index([\n...    ('discussion_id', 1), ('full_slug', 1)])\n\n\n\nNote\nEnsure that you always sort by the final element in a compound\nindex to maximize the performance of these queries.\n\n\n\n\nRetrieve Comments via Direct Links¶\n\nQueries¶\nTo directly retrieve a comment, without needing to page through all\ncomments, you can select by the slug field:\ncomment = db.comments.find_one({\n    'discussion_id': discussion_id,\n    'slug': comment_slug})\n\n\nYou can retrieve a “sub-discussion,” or a comment and all of its\ndescendants recursively, by performing a regular expression prefix\nquery on the full_slug field:\nimport re\n\nsubdiscussion = db.comments.find_one({\n    'discussion_id': discussion_id,\n    'full_slug': re.compile('^' + re.escape(parent_slug)) })\nsubdiscussion = subdiscussion.sort('full_slug')\n\n\n\n\nIndexing¶\nSince you have already created indexes on { discussion_id: 1,\nfull_slug: } to support retrieving sub-discussions, you can add\nsupport for the above queries by adding an index on {\ndiscussion_id: 1 , slug: 1 }. Use the following operation in the\nPython shell:\n>>> db.comments.ensure_index([\n...    ('discussion_id', 1), ('slug', 1)])\n\n\n\n\n\n\nEmbedding All Comments¶\nThis design embeds the entire discussion of a comment thread inside of\nthe topic document. In this example, the “topic,” document\nholds the total content for whatever content you’re managing.\n\nSchema¶\nConsider the following prototype topic document:\n{\n    _id: ObjectId(...),\n    ... lots of topic data ...\n    comments: [\n        { posted: ISODateTime(...),\n          author: { id: ObjectId(...), name: 'Rick' },\n          text: 'This is so bogus ... ' },\n       ... ]\n}\n\n\nThis structure is only suitable for a chronological display of all\ncomments because it embeds comments in chronological order. Each\ndocument in the array in the comments contains the comment’s date,\nauthor, and text.\n\nNote\nSince you’re storing the comments in sorted order, there is no need\nto maintain per-comment slugs.\n\nTo support threading using this design, you would need to embed\ncomments within comments, using a structure that resembles the\nfollowing:\n{\n    _id: ObjectId(...),\n    ... lots of topic data ...\n    replies: [\n        { posted: ISODateTime(...),\n          author: { id: ObjectId(...), name: 'Rick' },\n          text: 'This is so bogus ... ',\n          replies: [\n              { author: { ... }, ... },\n       ... ]\n}\n\n\nHere, the replies field in each comment holds the\nsub-comments, which can in turn hold sub-comments.\n\nNote\nIn the embedded document design, you give up some flexibility\nregarding display format, because it is difficult to display\ncomments except as you store them in MongoDB.\nIf, in the future, you want to switch from chronological to\nthreaded or from threaded to chronological, this design would make\nthat migration quite expensive.\n\n\nWarning\nRemember that BSON documents have a 16 megabyte\nsize limit. If popular discussions grow\nlarger than 16 megabytes, additional document growth will fail.\nAdditionally, when MongoDB documents grow significantly after\ncreation you will experience greater storage fragmentation and\ndegraded update performance while MongoDB migrates documents\ninternally.\n\n\n\nOperations¶\nThis section contains an overview of common operations for interacting\nwith comments represented using a schema that embeds all comments the\ndocument of the “parent” or topic content.\n\nNote\nFor all operations below, there is no need for any new indexes\nsince all the operations are function within documents. Because you\nwould retrieve these documents by the _id field, you can rely\non the index that MongoDB creates automatically.\n\n\nPost a new comment¶\nTo post a new comment in a chronologically ordered (i.e unthreaded)\nsystem, you need the following update():\ndb.discussion.update(\n    { 'discussion_id': discussion_id },\n    { '$push': { 'comments': {\n        'posted': datetime.utcnow(),\n        'author': author_info,\n        'text': comment_text } } } )\n\n\nThe $push operator inserts comments into the comments\narray in correct chronological order. For threaded discussions, the\nupdate()\noperation is more complex. To reply to a comment, the following code\nassumes that it can retrieve the ‘path’ as a list of positions, for\nthe parent comment:\nif path != []:\n    str_path = '.'.join('replies.%d' % part for part in path)\n    str_path += '.replies'\nelse:\n    str_path = 'replies'\ndb.discussion.update(\n    { 'discussion_id': discussion_id },\n    { '$push': {\n        str_path: {\n            'posted': datetime.utcnow(),\n            'author': author_info,\n            'text': comment_text } } } )\n\n\nThis constructs a field name of the form replies.0.replies.2... as\nstr_path and then uses this value with the $push\noperator to insert the new comment into the parent comment’s\nreplies array.\n\n\nView Paginated Comments¶\nTo view the comments in a non-threaded design, you must use\nthe $slice operator:\ndiscussion = db.discussion.find_one(\n    {'discussion_id': discussion_id},\n    { ... some fields relevant to your page from the root discussion ...,\n      'comments': { '$slice': [ page_num * page_size, page_size ] }\n    })\n\n\nTo return paginated comments for the threaded design, you must\nretrieve the whole document and paginate the comments within the\napplication:\ndiscussion = db.discussion.find_one({'discussion_id': discussion_id})\n\ndef iter_comments(obj):\n    for reply in obj['replies']:\n        yield reply\n        for subreply in iter_comments(reply):\n            yield subreply\n\npaginated_comments = itertools.slice(\n    iter_comments(discussion),\n    page_size * page_num,\n    page_size * (page_num + 1))\n\n\n\n\nRetrieve a Comment via Direct Links¶\nInstead of retrieving comments via slugs as above, the following\nexample retrieves comments using their position in the comment list or\ntree.\nFor chronological (i.e. non-threaded) comments, just use the\n$slice operator to extract a comment, as follows:\ndiscussion = db.discussion.find_one(\n    {'discussion_id': discussion_id},\n    {'comments': { '$slice': [ position, position ] } })\ncomment = discussion['comments'][0]\n\n\nFor threaded comments, you must find the correct path through the tree\nin your application, as follows:\ndiscussion = db.discussion.find_one({'discussion_id': discussion_id})\ncurrent = discussion\nfor part in path:\n    current = current.replies[part]\ncomment = current\n\n\n\nNote\nSince parent comments embed child replies, this operation actually\nretrieves the entire sub-discussion for the comment you queried\nfor.\n\n\nSee\nfind_one().\n\n\n\n\n\nHybrid Schema Design¶\n\nSchema¶\nIn the “hybrid approach” you will store comments in “buckets” that\nhold about 100 comments. Consider the following example bucket document:\n{\n    _id: ObjectId(...),\n    discussion_id: ObjectId(...),\n    bucket: 1,\n    count: 42,\n    comments: [ {\n        slug: '34db',\n        posted: ISODateTime(...),\n        author: { id: ObjectId(...), name: 'Rick' },\n        text: 'This is so bogus ... ' },\n    ... ]\n}\n\n\nEach document maintains bucket and count data that contains meta\ndata regarding the bucket, the bucket number, and the comment count, in\naddition to the comments array that holds the comments\nthemselves.\n\nNote\nUsing a hybrid format makes storing threaded comments complex, and\nthis specific configuration is not covered in this document.\nAlso, 100 comments is a soft limit for the number of comments per\nbucket. This value is arbitrary: choose a value that will prevent the\nmaximum document size from growing beyond the 16MB BSON\ndocument size limit, but large enough\nto ensure that most comment threads will fit in a single\ndocument. In some situations the number of comments per document\ncan exceed 100, but this does not affect the correctness of the\npattern.\n\n\n\nOperations¶\nThis section contains a number of common operations that you may use\nwhen building a CMS using this hybrid storage model with documents\nthat hold 100 comment “buckets.”\nAll examples in this document use the Python programming language and\nthe PyMongo driver for MongoDB, but\nyou can implement this system using any language you choose.\n\nPost a New Comment¶\n\nUpdating¶\nIn order to post a new comment, you need to $push the\ncomment onto the last bucket and $inc that bucket’s comment\ncount. Consider the following example that queries on the basis of a\ndiscussion_id field:\nbucket = db.comment_buckets.find_and_modify(\n    { 'discussion_id': discussion['_id'],\n      'bucket': discussion['num_buckets'] },\n    { '$inc': { 'count': 1 },\n      '$push': {\n          'comments': { 'slug': slug, ... } } },\n    fields={'count':1},\n    upsert=True,\n    new=True )\n\n\nThe find_and_modify()\noperation is an upsert,: if MongoDB cannot find a document\nwith the correct bucket number, the find_and_modify()\nwill create it and initialize the new document with appropriate values\nfor count and comments.\nTo limit the number of comments per bucket to roughly 100, you will\nneed to create new pages as they become necessary. Add the following\nlogic to support this:\nif bucket['count'] > 100:\n    db.discussion.update(\n        { 'discussion_id: discussion['_id'],\n          'num_buckets': discussion['num_buckets'] },\n        { '$inc': { 'num_buckets': 1 } } )\n\n\nThis update() operation\nincludes the last known number of pages in the query to prevent a race\ncondition where the number of pages increments twice, that would\nresult in a nearly or totally empty document. If another\nprocess increments the number of pages, then update above does\nnothing.\n\n\nIndexing¶\nTo support the find_and_modify()\nand update()\noperations, maintain a compound index on (discussion_id, bucket)\nin the comment_buckets collection, by issuing the following\noperation at the Python/PyMongo console:\n>>> db.comment_buckets.ensure_index([\n...    ('discussion_id', 1), ('bucket', 1)])\n\n\n\n\n\nView Paginated Comments¶\nThe following function presents an example of paginating comments into pages\nof fixed size. This works by iterating through all comments in a discussion,\nand keeping a counter of both how many comments it has skipped, and how many it\nhas returned.\ndef find_comments(discussion_id, skip, limit):\n    result = []\n\n    # Find this discussion's comment buckets\n    buckets = db.comment_buckets.find(\n        { 'discussion_id': discussion_id },\n        { 'bucket': 1 })\n    buckets = buckets.sort('bucket')\n\n    # Iterate through those buckets, making a query obtaining comments for each\n    for bucket in buckets:\n        page_query = db.comment_buckets.find_one(\n            { 'discussion_id': discussion_id, 'bucket': bucket['bucket'] },\n            { 'count': 1, 'comments': { '$slice': [ skip, limit ] }})\n        result.append((bucket['bucket'], page_query['comments']))\n        skip = max(0, skip - page_query['count'])\n        limit -= len(page_query['comments'])\n        if limit == 0: break\n\n    return result\n\n\nHere, the $slice operator pulls out comments from each\nbucket, but only when this satisfies the skip requirement. For\nexample: if you have 4 buckets with 100, 102, 101, and 22 comments on\neach bucket, respectively, and you wish to retrieve comments where skip=300 and\nlimit=50. Use the following algorithm:\n\n\n\n\n\n\n\nSkip\nLimit\nDiscussion\n\n\n\n300\n50\n{$slice: [ 300, 50 ] } matches nothing in bucket\n#1; subtract bucket #1’s count from skip and\ncontinue.\n\n200\n50\n{$slice: [ 200, 50 ] } matches nothing in bucket\n#2; subtract bucket #2’s count from skip and\ncontinue.\n\n98\n50\n{$slice: [ 98, 50 ] } matches 3 comments in bucket\n#3; subtract bucket #3’s count from skip\n(saturating at 0), subtract 3 from limit, and\ncontinue.\n\n0\n48\n{$slice: [ 0, 48 ] } matches all 22 comments in\nbucket #4; subtract 22 from limit and continue.\n\n0\n26\nThere are no more buckets; terminate loop.\n\n\n\n\nNote\nSince you already have an index on (discussion_id, bucket) in\nyour comment_buckets collection, MongoDB can satisfy these\nqueries efficiently.\n\n\n\nRetrieve a Comment via Direct Links¶\n\nQuery¶\nTo retrieve a comment directly without paging through all preceding\npages of commentary, use the slug to find the correct bucket, and then\nuse application logic to find the correct comment:\nbucket = db.comment_buckets.find_one(\n    { 'discussion_id': discussion_id,\n      'comments.slug': comment_slug},\n    { 'comments': 1 })\nfor comment in bucket['comments']:\n    if comment['slug'] = comment_slug:\n        break\n\n\n\n\nIndexing¶\nTo perform this query efficiently you’ll need a new index on the\ndiscussion_id and comments.slug fields (i.e. {\ndiscussion_id: 1 comments.slug: 1 }.) Create this index using the\nfollowing operation in the Python/PyMongo console:\n>>> db.comment_buckets.ensure_index([\n...    ('discussion_id', 1), ('comments.slug', 1)])\n\n\n\n\n\n\n\nSharding¶\nFor all of the architectures discussed above, you will want to the\ndiscussion_id field to participate in the shard key, if you need\nto shard your application.\nFor applications that use the “one document per comment” approach,\nconsider using slug (or full_slug, in the case of threaded\ncomments) fields in the shard key to allow the mongos\ninstances to route requests by slug. Issue the following operation\nat the Python/PyMongo console:\n>>> db.command('shardCollection', 'comments', {\n...     'key' : { 'discussion_id' : 1, 'full_slug': 1 } })\n\n\nThis will return the following response:\n{ \"collectionsharded\" : \"comments\", \"ok\" : 1 }\n\n\nIn the case of comments that fully-embedded in parent content\ndocuments the determination of the shard key is\noutside of the scope of this document.\nFor hybrid documents, use the bucket number in the\nshard key along with the discussion_id to allow MongoDB to split\npopular discussions between pages while grouping discussions on the same\nshard. Issue the following operation\nat the Python/PyMongo console:\n>>> db.command('shardCollection', 'comment_buckets', {\n...     key : { 'discussion_id' : 1, 'bucket': 1 } })\n{ \"collectionsharded\" : \"comment_buckets\", \"ok\" : 1 }\n\n\n\n\n\n                \n    \n      ←  \n      Metadata and Asset Management\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Storing Comments","preview":"On this page This document outlines the basic patterns for storing user-submitted\ncomments in a content management system (CMS.) MongoDB provides a number of different approaches for storing data like","tags":""},{"slug":"use-cases/storing-log-data/","headings":[],"text":"Use Cases > \n          Storing Log Data \n      \n    \n  \n                \n                  \nStoring Log Data\n\nOn this page\n\nOverview\nOperations\nSharding\nManaging Event Data Growth\n\n\n\nOverview¶\nThis document outlines the basic patterns and principles for using\nMongoDB as a persistent storage engine for log data from servers and\nother machine data.\n\nProblem¶\nServers generate a large number of events (i.e. logging,) that contain\nuseful information about their operation including errors, warnings,\nand users behavior. By default, most servers, store these data in\nplain text log files on their local file systems.\nWhile plain-text logs are accessible and human-readable, they are\ndifficult to use, reference, and analyze without holistic systems for\naggregating and storing these data.\n\n\nSolution¶\nThe solution described below assumes that each server\ngenerates events also consumes event data and that each server can\naccess the MongoDB instance. Furthermore, this design assumes that the\nquery rate for this logging data is substantially lower than\ncommon for logging applications with a high-bandwidth event stream.\n\nNote\nThis case assumes that you’re using a standard uncapped collection\nfor this event data, unless otherwise noted. See the section on\ncapped collections\n\n\n\nSchema Design¶\nThe schema for storing log data in MongoDB depends on the format of\nthe event data that you’re storing. For a simple example, consider\nstandard request logs in the combined format from the Apache HTTP\nServer.  A line from these logs may resemble the following:\n127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] \"GET /apache_pb.gif HTTP/1.0\" 200 2326 \"[http://www.example.com/start.html](http://www.example.com/start.html)\" \"Mozilla/4.08 [en] (Win98; I ;Nav)\"\n\n\nThe simplest approach to storing the log data would be putting the exact\ntext of the log record into a document:\n{\n  _id: ObjectId('4f442120eb03305789000000'),\n line: '127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] \"GET /apache_pb.gif HTTP/1.0\" 200 2326 \"[http://www.example.com/start.html](http://www.example.com/start.html)\" \"Mozilla/4.08 [en] (Win98; I ;Nav)\"'\n}\n\n\nWhile this solution does capture all data in a format that MongoDB can\nuse, the data is not particularly useful, or it’s not terribly\nefficient: if you need to find events that the same page, you would\nneed to use a regular expression query, which would require a full\nscan of the collection. The preferred approach is to extract the\nrelevant information from the log data into individual fields in a\nMongoDB document.\nWhen you extract data from the log into fields, pay attention to the\ndata types you use to render the log data into MongoDB.\nAs you design this schema, be mindful that the data types you use to\nencode the data can have a significant impact on the performance and\ncapability of the logging system. Consider the date field: In the\nabove example, [10/Oct/2000:13:55:36 -0700] is 28 bytes long. If\nyou store this with the UTC timestamp type, you can convey the same\ninformation in only 8 bytes.\nAdditionally, using proper types for your data also increases query\nflexibility: if you store date as a timestamp you can make date range\nqueries, whereas it’s very difficult to compare two strings that\nrepresent dates. The same issue holds for numeric fields; storing\nnumbers as strings requires more space and is difficult to query.\nConsider the following document that captures all data from the above\nlog entry:\n{\n     _id: ObjectId('4f442120eb03305789000000'),\n     host: \"127.0.0.1\",\n     logname: null,\n     user: 'frank',\n     time: ISODate(\"2000-10-10T20:55:36Z\"),\n     path: \"/apache_pb.gif\",\n     request: \"GET /apache_pb.gif HTTP/1.0\",\n     status: 200,\n     response_size: 2326,\n     referrer: \"[http://www.example.com/start.html](http://www.example.com/start.html)\",\n     user_agent: \"Mozilla/4.08 [en] (Win98; I ;Nav)\"\n}\n\n\nWhen extracting data from logs and designing a schema, also consider\nwhat information you can omit from your log tracking system. In most\ncases there’s no need to track all data from an event log, and you\ncan omit other fields. To continue the above example, here the most\ncrucial information may be the host, time, path, user agent, and\nreferrer, as in the following example document:\n{\n     _id: ObjectId('4f442120eb03305789000000'),\n     host: \"127.0.0.1\",\n     time:  ISODate(\"2000-10-10T20:55:36Z\"),\n     path: \"/apache_pb.gif\",\n     referer: \"[http://www.example.com/start.html](http://www.example.com/start.html)\",\n     user_agent: \"Mozilla/4.08 [en] (Win98; I ;Nav)\"\n}\n\n\nYou may also consider omitting explicit time fields, because the\nObjectId embeds creation time:\n{\n     _id: ObjectId('4f442120eb03305789000000'),\n     host: \"127.0.0.1\",\n     path: \"/apache_pb.gif\",\n     referer: \"[http://www.example.com/start.html](http://www.example.com/start.html)\",\n     user_agent: \"Mozilla/4.08 [en] (Win98; I ;Nav)\"\n}\n\n\n\n\nSystem Architecture¶\nThe primary performance concern for event logging systems are:\n\nhow many inserts per second can it support, which limits the event\nthroughput, and\n\nhow will the system manage the growth of event data, particularly\nconcerning a growth in insert activity.\nIn most cases the best way to increase the capacity of the system\nis to use an architecture with some sort of partitioning\nor sharding that distributes writes among a\ncluster of systems.\n\n\n\n\n\nOperations¶\nInsertion speed is the primary performance concern for an event\nlogging system. At the same time, the system must be able to support\nflexible queries so that you can return data from the system\nefficiently. This section describes procedures for both document\ninsertion and basic analytics queries.\nThe examples that follow use the Python programming language and the\nPyMongo driver for MongoDB, but you\ncan implement this system using any language you choose.\n\nInserting a Log Record¶\n\nWrite Concern¶\nMongoDB has a configurable write concern. This capability\nallows you to balance the importance of guaranteeing that all writes\nare fully recorded in the database with the speed of the insert.\nFor example, if you issue writes to MongoDB and do not require that\nthe database issue any response, the write operations will return\nvery fast (i.e. asynchronously,) but you cannot be certain that all\nwrites succeeded. Conversely, if you require that MongoDB acknowledge\nevery write operation, the database will not return as quickly but you\ncan be certain that every item will be present in the database.\nThe proper write concern is often an application specific decision,\nand depends on the reporting requirements and uses of your analytics\napplication.\n\n\nInsert Performance¶\nThe following example contains the setup for a Python console session\nusing PyMongo, with an event from the Apache Log:\n>>> import bson\n>>> import pymongo\n>>> from datetime import datetime\n>>> conn = pymongo.MongoClient()\n>>> db = conn.event_db\n>>> event = {\n...     _id: bson.ObjectId(),\n...     host: \"127.0.0.1\",\n...     time:  datetime(2000,10,10,20,55,36),\n...     path: \"/apache_pb.gif\",\n...     referer: \"[http://www.example.com/start.html](http://www.example.com/start.html)\",\n...     user_agent: \"Mozilla/4.08 [en] (Win98; I ;Nav)\"\n...}\n\n\nThe following command will insert the event object into the\nevents collection.\n>>> db.events.insert(event, w=0)\n\n\nBy setting w=0, you do not require that MongoDB\nacknowledges receipt of the insert.  Although very fast, this is risky\nbecause the application cannot detect network and server failures. See\nWrite Concern for more information.\nIf you want to ensure that MongoDB acknowledges inserts, you can pass\nw=1 argument as follows:\n>>> db.events.insert(event, w=1)\n\n\nMongoDB also supports a more stringent level of write concern, if you\nhave a lower tolerance for data loss:\nYou can ensure that MongoDB not only acknowledge receipt of the\nmessage but also commit the write operation to the on-disk journal\nbefore returning successfully to the application. For this, you can use the\nfollowing insert() operation:\n>>> db.events.insert(event, j=True)\n\n\n\nNote\nj=True implies w=1.\n\nFinally, if you have extremely low tolerance for event data loss,\nyou can require that MongoDB replicate the data to multiple\nsecondary replica set members before returning:\n>>> db.events.insert(event, w='majority')\n\n\nThis will force your application to acknowledge that the data has\nreplicated to a majority of configured members of the replica set. You can combine\noptions as well:\n>>> db.events.insert(event, j=True, w='majority')\n\n\nIn this case, your application will wait for a successful journal\ncommit on the primary and a replication acknowledgment from\na majority of configured secondaries. This is the safest option\npresented in this section, but it is the slowest. There is always a\ntrade-off between safety and speed.\n\nNote\nIf possible, consider using bulk inserts to insert event data.\nAll write concern options apply to bulk inserts, but you can pass\nmultiple events to the insert() method at\nonce. Batch inserts allow MongoDB to distribute the performance\npenalty incurred by more stringent write concern across a group of\ninserts.\n\n\nSee also\nWrite Concern for Replica Sets\nand getLastError.\n\n\n\n\nFinding All Events for a Particular Page¶\nThe value in maintaining a collection of event data derives from being\nable to query that data to answer specific questions. You may have a\nnumber of simple queries that you may use to analyze these data.\nAs an example, you may want to return all of the events associated\nwith specific value of a field. Extending the Apache access log\nexample from above, a common case would be to query for all events\nwith a specific value in the path field: This section contains a\npattern for returning data and optimizing this operation.\n\nQuery¶\nUse a query that resembles the following to return all documents with\nthe /apache_pb.gif value in the path field:\n>>> q_events = db.events.find({'path': '/apache_pb.gif'})\n\n\n\nNote\nIf you choose to shard the collection that stores this\ndata, the shard key you choose can impact the performance\nof this query. See the sharding\nsection of the sharding document.\n\n\n\nIndex Support¶\nAdding an index on the path field would significantly enhance the\nperformance of this operation.\n>>> db.events.ensure_index('path')\n\n\nBecause the values of the path likely have a random distribution,\nin order to operate efficiently, the entire index should be resident\nin RAM. In this case, the number of distinct paths is typically small\nin relation to the number of documents, which will limit the space\nthat the index requires.\nIf your system has a limited amount of RAM, or your data set has a\nwider distribution in values, you may need to re investigate your\nindexing support. In most cases, however, this index is entirely\nsufficient.\n\nSee also\nThe ensureIndex() JavaScript method\nand the db.events.ensure_index()\nmethod in PyMongo.\n\n\n\n\nFinding All the Events for a Particular Date¶\nThe next example describes the process for returning all the events\nfor a particular date.\n\nQuery¶\nTo retrieve this data, use the following query:\n>>> q_events = db.events.find('time':\n...     { '$gte':datetime(2000,10,10),'$lt':datetime(2000,10,11)})\n\n\n\n\nIndex Support¶\nIn this case, an index on the time field would optimize\nperformance:\n>>> db.events.ensure_index('time')\n\n\nBecause your application is inserting events in order, the parts of\nthe index that capture recent events will always be in active RAM. As\na result, if you query primarily on recent data, MongoDB will be able\nto maintain a large index, quickly fulfill queries, and avoid using\nmuch system memory.\n\nSee also\nThe ~db.events.ensureIndex() JavaScript method\nand the db.events.ensure_index()\nmethod in PyMongo.\n\n\n\n\nFinding All Events for a Particular Host/Date¶\nThe following example describes a more complex query for returning\nall events in the collection for a particular host on a particular\ndate. This kinds analysis may be useful for investigating suspicious\nbehavior by a specific user.\n\nQuery¶\nUse a query that resembles the following:\n>>> q_events = db.events.find({\n...     'host': '127.0.0.1',\n...     'time': {'$gte':datetime(2000,10,10),'$lt':datetime(2000,10,11)}\n... })\n\n\nThis query selects documents from the events\ncollection where the host field is 127.0.0.1 (i.e. local\nhost), and the value of the time field represents a date that is\non or after (i.e. $gte) 2000-10-10 but before\n(i.e. $lt) 2000-10-11.\n\n\nIndex Support¶\nThe indexes you use may have significant implications for the\nperformance of these kinds of queries. For instance, you can create\na compound index on the time and host field, using the\nfollowing command:\n>>> db.events.ensure_index([('time', 1), ('host', 1)])\n\n\nTo analyze the performance for the above query using this index, issue\nthe\nq_events.explain()\nmethod in a Python console. This will return something that resembles:\n{ ...\n  u'cursor': u'BtreeCursor time_1_host_1',\n  u'indexBounds': {u'host': [[u'127.0.0.1', u'127.0.0.1']],\n  u'time': [\n      [ datetime.datetime(2000, 10, 10, 0, 0),\n        datetime.datetime(2000, 10, 11, 0, 0)]]\n  },\n  ...\n  u'millis': 4,\n  u'n': 11,\n  u'nscanned': 1296,\n  u'nscannedObjects': 11,\n  ... }\n\n\nThis query had to scan 1296 items from the index to return 11 objects in 4\nmilliseconds. Conversely, you can test a different compound index with\nthe host field first, followed by the time field. Create this\nindex using the following operation:\n>>> db.events.ensure_index([('host', 1), ('time', 1)])\n\n\nUse the\nq_events.explain()\noperation to test the performance:\n{ ...\n  u'cursor': u'BtreeCursor host_1_time_1',\n  u'indexBounds': {u'host': [[u'127.0.0.1', u'127.0.0.1']],\n  u'time': [[datetime.datetime(2000, 10, 10, 0, 0),\n      datetime.datetime(2000, 10, 11, 0, 0)]]},\n  ...\n  u'millis': 0,\n  u'n': 11,\n  ...\n  u'nscanned': 11,\n  u'nscannedObjects': 11,\n  ...\n}\n\n\nHere, the query had to scan 11 items from the index before returning\n11 objects in less than a millisecond. By placing the more selective\nelement of your query first in a compound index you may be able to\nbuild more useful queries.\n\nNote\nAlthough the index order has an impact query performance, remember\nthat index scans are much faster than collection scans, and\ndepending on your other queries, it may make more sense to use the\n{ time: 1, host: 1 } index depending on usage profile.\n\n\nSee also\nThe ~db.events.ensureIndex() JavaScript method and the\ndb.events.ensure_index()\nmethod in PyMongo.\n\n\n\n\nCounting Requests by Day and Page¶\nThe following example describes the process for using the collection\nof Apache access events to determine the number of request per\nresource (i.e. page) per day in the last month.\n\nAggregation¶\n\nNew in version 2.1.\n\nThe aggregation framework provides the capacity for queries\nthat select, process, and aggregate results from large numbers of\ndocuments. The aggregate() offers greater\nflexibility, capacity with less complexity than the existing\nmapReduce and group aggregation commands.\nConsider the following aggregation pipeline:\n[1]\n>>> result = db.command('aggregate', 'events', pipeline=[\n...         {  '$match': {\n...               'time': {\n...                   '$gte': datetime(2000,10,1),\n...                   '$lt':  datetime(2000,11,1) } } },\n...         {  '$project': {\n...                 'path': 1,\n...                 'date': {\n...                     'y': { '$year': '$time' },\n...                     'm': { '$month': '$time' },\n...                     'd': { '$dayOfMonth': '$time' } } } },\n...         { '$group': {\n...                 '_id': {\n...                     'p':'$path',\n...                     'y': '$date.y',\n...                     'm': '$date.m',\n...                     'd': '$date.d' },\n...                 'hits': { '$sum': 1 } } },\n...         ])\n\n\nThis command aggregates documents from the events collection with\na pipeline that:\n\nUses the $match to limit the documents that the\naggregation framework must process. $match is\nsimilar to a find() query.\nThis operation selects all documents where the value of the\ntime field represents a date that is on or after\n(i.e. $gte) 2000-10-10 but before\n(i.e. $lt) 2000-10-11.\n\nUses the $project to limit the data that continues\nthrough the pipeline. This operator:\n\nSelects the path field.\nCreates a y field to hold the year, computed from the\ntime field in the original documents.\nCreates a m field to hold the month, computed from the\ntime field in the original documents\nCreates a d field to hold the day, computed from the\ntime field in the original documents.\n\n\nUses the $group to create new computed\ndocuments. This step will create a single new document for each\nunique path/date combination. The documents take the following\nform:\n\nthe _id field holds a sub-document with the contents path\nfield from the original documents in the p field, with the\ndate fields from the $project  as the\nremaining fields.\nthe hits field use the $sum statement to\nincrement a counter for every document in the group. In the\naggregation output, this field holds the total number of\ndocuments at the beginning of the aggregation pipeline with this\nunique date and path.\n\n\n\n\nNote\nIn sharded environments, the performance of aggregation operations\ndepends on the shard key. Ideally, all the items in a\nparticular $group operation will reside on the same\nserver.\nWhile this distribution of documents would occur if you chose the\ntime field as the shard key, a field like path also has\nthis property and is a typical choice for sharding. Also see the\n“sharding considerations.”\nof this document for additional recommendations for using sharding.\n\n\nSee also\nData Aggregation\n\n\n\n\n[1]To translate statements from the\naggregation pipeline to SQL,\nyou can consider the $match equivalent to\nWHERE, $project to SELECT, and\n$group to GROUP BY.\n\n\n\n\nIndex Support¶\nTo optimize the aggregation operation, ensure that the initial\n$match query has an index. Use the following command\nto create an index on the time field in the events collection:\n>>> db.events.ensure_index('time')\n\n\n\nNote\nIf you have already created a compound index on the time and\nhost (i.e. { time: 1, host, 1 },) MongoDB will use this\nindex for range queries on just the time field. Do not create\nan additional index, in these situations.\n\n\n\n\n\nSharding¶\nEventually your system’s events will exceed the capacity of a single\nevent logging database instance. In these situations you will want to\nuse a sharded cluster, which takes advantage of MongoDB’s\nsharding functionality. This section introduces the unique\nsharding concerns for this event logging case.\n\nSee also\nSharding and FAQ for\nSharded Clusters in the MongoDB Manual.\n\n\nLimitations¶\nIn a sharded environment the limitations on the maximum insertion rate\nare:\n\nthe number of shards in the cluster.\nthe shard key you chose.\n\nBecause MongoDB distributed data in using “ranges” (i.e. chunks) of keys, the choice of shard key can\ncontrol how MongoDB distributes data and the resulting systems’\ncapacity for writes and queries.\nIdeally, your shard key should allow insertions balance evenly among\nthe shards [2] and for most queries to only need to\naccess a single shard. [3] Continue reading for an analysis of\na collection of shard key choices.\n\n\n\n[2]For this reason, avoid shard keys based on the\ntimestamp or the insertion time (i.e. the ObjectId) because all\nwrites will end up on a single node.\n\n\n\n\n\n[3]For this reason, avoid randomized shard keys (e.g. hash\nbased shard keys) because any query will have to access all shards\nin the cluster.\n\n\n\n\nShard by Time¶\nWhile using the timestamp, or the ObjectId in the _id\nfield, [4] would distribute your data evenly among shards, these\nkeys lead to two problems:\n\nAll inserts always flow to the same shard, which means that your\nsharded cluster will have the same write throughput as a\nstandalone instance.\nMost reads will tend to cluster on the same shard, as analytics\nqueries.\n\n\n\n\n[4]The ObjectId derives from the creation time, and is\neffectively a timestamp in this case.\n\n\n\n\nShard by a Semi-Random Key¶\nTo distribute data more evenly among the shards, you may consider\nusing a more “random” piece of data, such as a hash of the _id\nfield (i.e. the ObjectId as a shard key.\nWhile this introduces some additional complexity into your application,\nto generate the key, it will distribute writes among the shards. In\nthese deployments having 5 shards will provide 5 times the write\ncapacity as a single instance.\nUsing this shard key, or any hashed value as a key presents the\nfollowing downsides:\n\nthe shard key, and the index on the key will consume additional\nspace in the database.\nqueries, unless they include the shard key itself, [5]\nmust run in parallel on all shards, which may lead to degraded\nperformance.\n\nThis might be an acceptable trade-off in some situations. The workload\nof event logging systems tends to be heavily skewed toward writing,\nread performance may not be as critical as more robust write\nperformance.\n\n\n\n[5]Typically, it is difficult to use these kinds of shard\nkeys in queries.\n\n\n\n\nShard by an Evenly-Distributed Key in the Data Set¶\nIf a field in your documents has values that are evenly distributed\namong the documents, you may consider using this key as a shard\nkey.\nContinuing the example from above, you may consider using the path\nfield. Which may have a couple of advantages:\n\nwrites will tend to balance evenly among shards.\nreads will tend to be selective and local to a single shard if the\nquery selects on the path field.\n\nThere are a few potential problems with these kinds of shard keys:\n\nIf a large number of documents will have the same shard key, you\nrun the risk of having a portion of your data collection MongoDB\ncannot distribute throughout the cluster.\nIf there are a small number of possible values, there may be a\nlimit to how much MongoDB will be able to distribute the data among\nthe shard.\n\n\nNote\nTest using your existing data to ensure that the distribution is\ntruly even, and that there is a sufficient quantity of distinct\nvalues for the shard key.\n\n\n\nShard by Combine a Natural and Synthetic Key¶\nMongoDB supports compound shard keys that combine\nthe best aspects of sharding by a evenly distributed key in the\nset and sharding by a\nrandom key. In these\nsituations, the shard key would resemble { path: 1 , ssk: 1 }\nwhere, path is an often used “natural key”, or value from your data\nand ssk is a hash of the _id field, calculated in your\napplication using a standard hash function.\nUsing this type of shard key, data is largely distributed by the\nnatural key, or path, which makes most queries that access the\npath field local to a single shard or group of shards. At the same\ntime, if there is not sufficient distribution for specific values of\npath, the ssk makes it possible for MongoDB to create\nchunks and data across the cluster.\nIn most situations, these kinds of keys provide the ideal balance\nbetween distributing writes across the cluster and ensuring that most\nqueries will only need to access a select number of shards.\n\n\nTest with Your Own Data¶\nSelecting shard keys is difficult because: there are no definitive\n“best-practices,” the decision has a large impact on performance, and\nit is difficult or impossible to change the shard key after making the\nselection.\nThe sharding options provides a\ngood starting point for thinking about shard key\nselection. Nevertheless, the best way to select a shard key is to\nanalyze the actual insertions and queries from your own application.\n\n\n\nManaging Event Data Growth¶\nWithout some strategy for managing the size of your database, most\nevent logging systems can grow infinitely. This is particularly\nimportant in the context of MongoDB may not relinquish data to the\nfile system in the way you might expect. Consider the following\nstrategies for managing data growth:\n\nCapped Collections¶\nDepending on your data retention requirements as well as your\nreporting and analytics needs, you may consider using a capped\ncollection to store your events. Capped collections have a fixed\nsize, and drop old data when inserting new data after reaching cap.\n\nNote\nIn the current version, it is not possible to shard capped\ncollections.\n\n\n\nMultiple Collections, Single Database¶\nStrategy: Periodically rename your event collection so that your\ndata collection rotates in much the same way that you might rotate log\nfiles. When needed, you can drop the oldest collection from the\ndatabase.\nThis approach has several advantages over the single collection\napproach:\n\nCollection renames are fast and atomic.\nMongoDB does not bring any document into memory to drop a\ncollection.\nMongoDB can effectively reuse space freed by removing entire\ncollections without leading to data fragmentation.\n\nNevertheless, this operation may increase some complexity for queries,\nif any of your analyses depend on events that may reside in the\ncurrent and previous collection. For most real time data collection\nsystems, this approach is the most ideal.\n\n\nMultiple Databases¶\nStrategy: Rotate databases rather than collections, as in the\n“Multiple Collections, Single Database\nexample.\nWhile this significantly increases application complexity for\ninsertions and queries, when you drop old databases, MongoDB will\nreturn disk space to the file system. This approach makes the most\nsense in scenarios where your event insertion rates and/or your data\nretention rates were extremely variable.\nFor example, if you are performing a large backfill of event data and\nwant to make sure that the entire set of event data for 90 days is\navailable during the backfill, during normal operations you only need\n30 days of event data, you might consider using multiple databases.\n\n\n\n\n                \n    \n      ←  \n      Use Cases\n      Pre-Aggregated Reports (MMAPv1)\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Storing Log Data","preview":"On this page This document outlines the basic patterns and principles for using\nMongoDB as a persistent storage engine for log data from servers and\nother machine data. Servers generate a large number","tags":""},{"slug":"use-cases/","headings":[],"text":"Use Cases\nThe use case documents introduce the patterns, designs,\nand operations used in application development with MongoDB. Each\ndocument provides concrete examples and implementation details to\nsupport core MongoDB use cases. These documents highlight\napplication design, and data modeling strategies (i.e. schema\ndesign) for MongoDB with special attention to pragmatic\nconsiderations including indexing, performance, sharding, and\nscaling. Each document is distinct and can stand alone; however, each\nsection builds on a set of common examples and general use cases.\nThe operational intelligence case studies describe applications that\ncollect machine generated data from logging systems, application\noutput, and other systems. The product data management case studies\naddress aspects of applications required for building product\ncatalogs, and managing inventory in e-commerce systems. Finally, the\ncontent management case studies introduce basic patterns and\ntechniques for building content management systems using MongoDB.\n\nOperational Intelligence¶\nAs an introduction to the use of MongoDB for operational intelligence\nand real time analytics use, the document “Storing Log Data”\ndescribes several ways and approaches to modeling and storing\nmachine generated data with MongoDB. Then,\n“Pre-Aggregated Reports (MMAPv1)” describes methods and\nstrategies for processing data to generate aggregated reports from raw\nevent-data. Finally “Hierarchical Aggregation”\npresents a method for using MongoDB to process and store hierarchical\nreports (i.e. per-minute, per-hour, and per-day) from raw event data.\n\n\nStoring Log Data\nPre-Aggregated Reports (MMAPv1)\nHierarchical Aggregation\n\n\n\n\nProduct Data Management¶\nMongoDB’s flexible schema makes it particularly well suited to storing\ninformation for product data management and e-commerce websites and\nsolutions. The “Product Catalog” document describes\nmethods and practices for modeling and managing a product catalog\nusing MongoDB, while the “Inventory Management”\ndocument introduces a pattern for handling interactions between\ninventory and users’ shopping carts. Finally the\n“Category Hierarchy” document describes methods for\ninteracting with category hierarchies in MongoDB.\n\n\nProduct Catalog\nInventory Management\nCategory Hierarchy\n\n\n\n\nContent Management Systems¶\nThe content management use cases introduce fundamental MongoDB\npractices and approaches, using familiar problems and simple\nexamples. The “Metadata and Asset Management”\ndocument introduces a model that you may use when designing a web site\ncontent management system, while “Storing Comments”\nintroduces the method for modeling user comments on content, like blog\nposts, and media, in MongoDB.\n\n\nMetadata and Asset Management\nStoring Comments\n\n\n\n\n\n                \n    \n      ←  \n      Windows Quick Links and Reference Center\n      Storing Log Data\n       →\n    \n                  \n                    \n                      © MongoDB, Inc 2008-2017. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.","title":"Use Cases","preview":"The use case documents introduce the patterns, designs,\nand operations used in application development with MongoDB. Each\ndocument provides concrete examples and implementation details to\nsupport core","tags":""}]}
